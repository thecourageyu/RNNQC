{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- performance evaluation\n",
    "- date: 2020-08-07\n",
    "- maintainer: YZK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to script mbuilder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import deque, Counter\n",
    "from fbprophet import Prophet\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Layer, Dense, Input, LSTM\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "\n",
    "\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Layer, Dense, Input, LSTM\n",
    "# from keras.optimizers import SGD\n",
    "# from keras import initializers, regularizers, constraints\n",
    "# from keras.callbacks import CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10)                560       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 571\n",
      "Trainable params: 571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def lstmbuilder(units, input_shape, loss, optimizer):\n",
    "    '''\n",
    "        input_shape: a tuple (timesteps, nfeatures)\n",
    "    '''\n",
    "    \n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units, input_shape=input_shape))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.compile(loss=loss, optimizer=optimizer)\n",
    "             \n",
    "    return lstm\n",
    "\n",
    "\n",
    "lstmbuilder(10, (10, 3), 'mae', SGD()).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking. \n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \n",
    "        tensorflow & keras reference:\n",
    "            https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "            https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "            https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
    "            \n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)  # inherit Layer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3\n",
    "        '''\n",
    "        \n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "#         self.step_dim = input_shape[-2]\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "        '''\n",
    "        \n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBuilder():\n",
    "    def __init__(self, modeld=\"model\", ckptd=\"ckpt\"):\n",
    "        \n",
    "        if not os.path.exists(modeld):\n",
    "            os.makedirs(modeld)\n",
    "            \n",
    "        if not os.path.exists(ckptd):\n",
    "            os.makedirs(ckptd)\n",
    "        \n",
    "        self.modeld = modeld\n",
    "        self.ckptd = ckptd\n",
    "        self.callbacks = self._callbacks(modeld, ckptd)\n",
    "        self.optimizer = self._optimizer()\n",
    "        \n",
    "    def TPALSTM(self):\n",
    "        embedding_layer = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "        sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        merged = concatenate([x1, y1])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        ########################################\n",
    "        ## add class weight\n",
    "        ########################################\n",
    "        if re_weight:\n",
    "            class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "        else:\n",
    "            class_weight = None\n",
    "\n",
    "        ########################################\n",
    "        ## train the model\n",
    "        ########################################\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "                outputs=preds)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer='nadam',\n",
    "                metrics=['acc'])\n",
    "        #model.summary()\n",
    "        print(STAMP)\n",
    "\n",
    "        early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "        bst_model_path = STAMP + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "        hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "                validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "                epochs=200, batch_size=2048, shuffle=True, \\\n",
    "                class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "        model.load_weights(bst_model_path)\n",
    "        bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "        ########################################\n",
    "        ## make the submission\n",
    "        ########################################\n",
    "        print('Start making the submission before fine-tuning')\n",
    "\n",
    "        preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "        preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "        preds /= 2\n",
    "\n",
    "        submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "        submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n",
    "    \n",
    "\n",
    "    def stackedLSTM(self, shape, cells, target=\"regression\"):    \n",
    "        \n",
    "        timesteps = shape[0]\n",
    "        nfeatures = shape[1]\n",
    "                \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "   \n",
    "        model = Sequential()\n",
    "        \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    model.add(LSTM(units[idx], input_shape=(timesteps, nfeatures), return_sequences=True, name=\"lstm_{}\".format(idx))) \n",
    "                elif idx == nlayer - 1:  # the last hidden layer\n",
    "                    model.add(LSTM(units[idx], name=\"lstm_{}\".format(idx))) \n",
    "                else:\n",
    "                    model.add(LSTM(units[idx], return_sequences=True, name=\"lstm_{}\".format(idx))) \n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm_0\")) \n",
    "                \n",
    "        if target == \"regression\":\n",
    "            model.add(Dense(nfeatures, activation='sigmoid', name=\"dense\"))  # for regression\n",
    "        else:\n",
    "            model.add(Dense(nfeatures, activation='softmax', name=\"dense\"))  # for classification\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def LSTMbasicAttention(self, shape, cells):\n",
    "        '''\n",
    "            shape = (timestep, feature)\n",
    "            return [model, optimizer, callbacks]\n",
    "        '''\n",
    "        \n",
    "        nfeatures = shape[1]\n",
    "        \n",
    "        inputs = Input(shape, name=\"input\")  # return a tensor\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "        else:\n",
    "            units = [cells]\n",
    "                \n",
    "        for idx, unit in enumerate(units):\n",
    "            if idx == 0:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(inputs)\n",
    "            else:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(x)\n",
    "            x = Attention(shape[0])(x)\n",
    "            \n",
    "        outputs = Dense(nfeatures)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _callbacks(modeld, ckptd, mmonitor=\"val_loss\", emonitor=\"loss\", lmonitor=\"val_loss\", name=\"ckpt\"):\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "        \n",
    "        name_ = \"{epoch:04d}_{loss:.3f}_{val_loss:.3f}\"\n",
    "        checkpointer = ModelCheckpoint(filepath=os.path.join(modeld, \"{0}_{1}_{2}.hdf5\".format(name, name_, timestamp)),\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True, \n",
    "                                       monitor=mmonitor)\n",
    "        \n",
    "        earlystopper = EarlyStopping(monitor=emonitor, patience=10)\n",
    "\n",
    "        reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.5, patience=20, min_lr=0.00001)\n",
    "        \n",
    "        tb = TensorBoard(log_dir=ckptd)\n",
    "\n",
    "        csvlogger = CSVLogger(os.path.join(ckptd, \"{}_{}.log\".format(name, timestamp)), append=False, separator=\",\")\n",
    "\n",
    "        # Learning rate schedule.\n",
    "    #     lr_schedule = LearningRateScheduler(fixed_schedule, verbose=0)\n",
    "\n",
    "        return [checkpointer, earlystopper, reduceLR, tb, csvlogger]\n",
    "\n",
    "#         return [checkpointer, earlystopper, reduceLR, csvlogger]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _optimizer(lr=1e-2):\n",
    "        optimizer = SGD(lr=lr, momentum=0.9, nesterov=True)\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, y_train, epochs, batch_size):\n",
    "    \n",
    "    timesteps = X_train.shape[1]\n",
    "    nfeatures = X_train.shape[2]\n",
    "    stackedLSTM, callbacks_, optimizer_ = NNBuilder(modeld=\"model\", ckptd=\"ckpt\").stackedLSTM(shape=(timesteps, nfeatures), cells=60)\n",
    "    stackedLSTM.compile(loss=\"mae\", optimizer=optimizer_)\n",
    "\n",
    "    history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "    \n",
    "#     history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history['loss'], label='train')\n",
    "    ax.plot(history.history['val_loss'], label='test')\n",
    "    ax.legend(fontsize=14)\n",
    "    plt.savefig(\"./ckpt/trainingHistory.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_0 (LSTM)                (None, 60)                15600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 244       \n",
      "=================================================================\n",
      "Total params: 15,844\n",
      "Trainable params: 15,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2000\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0026s vs `on_train_batch_end` time: 0.0625s). Check your callbacks.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24996, saving model to model/ckpt_0001_0.250_0.250_202008071719.hdf5\n",
      "18000/18000 - 29s - loss: 0.2501 - val_loss: 0.2500\n",
      "Epoch 2/2000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24996 to 0.24988, saving model to model/ckpt_0002_0.250_0.250_202008071719.hdf5\n",
      "18000/18000 - 28s - loss: 0.2501 - val_loss: 0.2499\n",
      "Epoch 3/2000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.24988\n",
      "18000/18000 - 29s - loss: 0.2501 - val_loss: 0.2499\n",
      "Epoch 4/2000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.24988 to 0.24987, saving model to model/ckpt_0004_0.250_0.250_202008071719.hdf5\n",
      "18000/18000 - 29s - loss: 0.2501 - val_loss: 0.2499\n",
      "Epoch 5/2000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.24987\n",
      "18000/18000 - 28s - loss: 0.2501 - val_loss: 0.2499\n",
      "Epoch 6/2000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.24987\n",
      "18000/18000 - 27s - loss: 0.2501 - val_loss: 0.2500\n",
      "Epoch 7/2000\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24987 to 0.24986, saving model to model/ckpt_0007_0.250_0.250_202008071719.hdf5\n",
      "18000/18000 - 26s - loss: 0.2501 - val_loss: 0.2499\n",
      "Epoch 8/2000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.24986\n",
      "18000/18000 - 27s - loss: 0.2501 - val_loss: 0.2499\n",
      "Epoch 9/2000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.24986\n",
      "18000/18000 - 27s - loss: 0.2501 - val_loss: 0.2499\n",
      "Epoch 10/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f6e5b370bd7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-07e478a004e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstackedLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstackedLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, verbose=2, shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    stackedLSTM, callbacks_, optimizer_ = NNBuilder().stackedLSTM([6, 4], 60)\n",
    "    stackedLSTM.compile(loss=\"mae\", optimizer=optimizer_)\n",
    "    stackedLSTM.summary()\n",
    "#     history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "\n",
    "    X_train = np.random.random_sample((1000, 6, 4))\n",
    "    y_train = np.random.random_sample((1000, 4))\n",
    "\n",
    "    main(X_train, y_train, 3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
