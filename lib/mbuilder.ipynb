{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- performance evaluation\n",
    "- date: 2020-08-07\n",
    "- maintainer: YZK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to script mbuilder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import deque, Counter\n",
    "from fbprophet import Prophet\n",
    "from functools import partial\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, losses\n",
    "from tensorflow.keras.layers import Layer, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Conv1D, Conv2D, Dense, Dropout, Flatten, Input, LSTM, MaxPool1D, MaxPool2D\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "# from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# from tensorflow.keras import losses\n",
    "\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "# @keras_export('keras.losses.CosineSimilarityCB')\n",
    "\n",
    "\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Layer, Dense, Input, LSTM\n",
    "# from keras.optimizers import SGD\n",
    "# from keras import initializers, regularizers, constraints\n",
    "# from keras.callbacks import CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmbuilder(units, input_shape, loss, optimizer):\n",
    "    '''\n",
    "        input_shape: a tuple (timesteps, nfeatures)\n",
    "    '''\n",
    "    \n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units, input_shape=input_shape))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.compile(loss=loss, optimizer=optimizer)\n",
    "             \n",
    "    return lstm\n",
    "\n",
    "\n",
    "# lstmbuilder(10, (10, 3), 'mae', SGD()).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking. \n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \n",
    "        tensorflow & keras reference:\n",
    "            https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "            https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "            https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
    "            \n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)  # inherit Layer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3\n",
    "        '''\n",
    "        \n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "#         self.step_dim = input_shape[-2]\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "        '''\n",
    "        \n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4 * 10 + 10) + (10 * 20 + 20) + (20 * 30 + 30) + 4 * (30 * 4 + 4 + 4 * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        l2 = np.sum(x**2)\n",
    "    else:\n",
    "        l2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=x)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def cosine_similarity(y_true, y_pred, axis=1):\n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "#     l2norm1 = tf.reduce_sum(tf.square(y_true), axis=axis)\n",
    "#     l2norm2 = tf.reduce_sum(tf.square(y_pred), axis=axis)\n",
    "    l2norm1 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_true)\n",
    "    l2norm2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_pred)\n",
    "    yy = tf.reduce_sum(tf.multiply(y_true, y_pred), axis=axis)\n",
    "        \n",
    "    return -tf.divide(yy, tf.multiply(tf.sqrt(l2norm1), tf.sqrt(l2norm2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YZKError(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None, \n",
    "                 penalized=None):\n",
    "    \n",
    "        \"\"\" Initializes `YZKError` instance.\n",
    "            Args:\n",
    "              reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "                loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "                option will be determined by the usage context. For almost all cases\n",
    "                this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
    "                `tf.distribute.Strategy`, outside of built-in training loops such as\n",
    "                `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n",
    "                will raise an error. Please see this custom training [tutorial](\n",
    "                  https://www.tensorflow.org/tutorials/distribute/custom_training)\n",
    "                for more details.\n",
    "              name: Optional name for the op. Defaults to 'mean_squared_error'.\n",
    "        \"\"\"\n",
    "\n",
    "        super(YZKError, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "        self.penalized = penalized\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        element_weight = self.element_weight\n",
    "\n",
    "#         logging.info(self.reduction, element_weight)\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        batchsize = y_pred.shape[0] \n",
    "        assert batchsize == y_true.shape[0]\n",
    "        \n",
    "#         logging.info(\"YZKError, shape of y_true = {}, y_pred = {}\".format(y_true.shape, y_pred.shape))\n",
    "        \n",
    "        PLoss = 0.    \n",
    "        penalized = self.penalized\n",
    "        if penalized is not None:\n",
    "            \n",
    "            y_pred_ = tf.convert_to_tensor(y_pred[:, penalized])\n",
    "            y_true_ = tf.convert_to_tensor(y_true[:, penalized])\n",
    "            \n",
    "            y_pred_ = tf.reshape(y_pred_, [-1, 1])\n",
    "            y_true_ = tf.reshape(y_true_, [-1, 1])\n",
    "#                 PLoss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true[:, penalized], y_pred[:, penalized])\n",
    "            PLoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(y_true_, y_pred_)\n",
    "        \n",
    "#         HuberLoss = 0\n",
    "#         MAELoss = 0\n",
    "#         if element_weight is not None:\n",
    "#             element_weight = tf.convert_to_tensor(element_weight)\n",
    "#             if element_weight.shape != []:  # is not a scale\n",
    "#                 nelement = y_pred.shape[1]\n",
    "#                 assert nelement == y_true.shape[1] \n",
    "#                 element_weight = tf.broadcast_to(element_weight, [batchsize, nelement])\n",
    "                \n",
    "#             y_pred_ = tf.math.multiply(y_pred, element_weight)\n",
    "#             y_true_ = tf.math.multiply(y_true, element_weight)\n",
    "        \n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true_, y_pred_)\n",
    "#         else:\n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true, y_pred)\n",
    "        \n",
    "        MAELoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "\n",
    "    #         CosSimLoss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "        CosSimLoss = tf.losses.CosineSimilarity(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "#         logging.info(\"YZKError, PLoss: {}\\n, HuberLoss: {}\\n, MAELoss: {}\\n, CosSimLoss: {}\\n\".format(PLoss, HuberLoss, MAELoss, CosSimLoss))\n",
    "        \n",
    "        if penalized is not None:\n",
    "            return tf.math.add(tf.math.add(tf.math.scalar_mul(3, PLoss), tf.math.scalar_mul(2, MAELoss)), tf.math.scalar_mul(1, CosSimLoss))\n",
    "        else:\n",
    "            return tf.math.add(tf.math.scalar_mul(2, MAELoss), tf.math.scalar_mul(1, CosSimLoss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inverse transform sigmoid(x)\n",
    "<center>\n",
    "<font size=4>\n",
    "$\n",
    "\\begin{align}\n",
    "\\sigma &=\\frac{1}{1+e^{-x}} \\\\\n",
    "\\frac{1}{\\sigma} &= 1+e^{-x} \\\\\n",
    "\\frac{1}{\\sigma}-1 &= e^{-x} \\\\\n",
    "\\log{\\frac{1-\\sigma}{\\sigma}} &= \\log{e^{-x}} \\\\\n",
    "x &= -\\log{\\frac{1-\\sigma}{\\sigma}} \\\\\n",
    "x &= \\log{\\frac{\\sigma}{1-\\sigma}}\n",
    "\\end{align}\n",
    "$\n",
    "</font size>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Crossentropy\n",
    "\n",
    "<center><font size=4>$BCE=y_{true}\\cdot\\log{y_{pred}}+\\left(1-y_{true}\\right)\\cdot\\log{\\left(1-y_{pred}\\right)}$</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBinaryCrossntropy(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None):\n",
    "    \n",
    "        \"\"\" Initializes `YZKError` instance.\n",
    "            Args:\n",
    "              reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "                loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "                option will be determined by the usage context. For almost all cases\n",
    "                this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
    "                `tf.distribute.Strategy`, outside of built-in training loops such as\n",
    "                `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n",
    "                will raise an error. Please see this custom training [tutorial](\n",
    "                  https://www.tensorflow.org/tutorials/distribute/custom_training)\n",
    "                for more details.\n",
    "               element_weight: [w for y_true=1, w for y_true=0]\n",
    "            Ex.\n",
    "                WeightedBinaryCrossntropy(reduction=tf.losses.Reduction.NONE, element_weight=[5, 1])(y_true, y_pred)\n",
    "        \"\"\"\n",
    "\n",
    "        super(WeightedBinaryCrossntropy, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "    \n",
    "    def call(self, y_true, y_pred, from_logits=False):\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        \n",
    "        element_weight = self.element_weight\n",
    "        \n",
    "#         print(element_weight)\n",
    "        \n",
    "        if not from_logits:\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "            y_pred = tf.math.log(y_pred / (1 - y_pred))\n",
    "            y_pred = tf.math.sigmoid(y_pred)\n",
    "            \n",
    "        if element_weight is None:\n",
    "            bc = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        else:\n",
    "            bc = -(y_true * tf.math.log(y_pred) * element_weight[0] + (1 - y_true) * tf.math.log(1 - y_pred) * element_weight[1])\n",
    "\n",
    "        return tf.math.reduce_mean(bc, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(X_train, y_train, epochs, batch_size, mconf, loss=\"mae\", modeld=\"model\", ckptd=\"ckpt\", name=\"NN\", earlystopper=True):\n",
    "    '''\n",
    "        X: [nsize, nstn, timestep, feature]\n",
    "        y: [nsize, nstn, features]\n",
    "        mconf: {name, units, inshape, outshape, outactfn, batchNormalization, dropouts, activations}\n",
    "    '''\n",
    "\n",
    "    timesteps = X_train.shape[1]\n",
    "    nfeatures = X_train.shape[2]\n",
    "    \n",
    "#     timesteps = 6\n",
    "#     nfeatures = 4\n",
    "\n",
    "    NN = NNBuilder(modeld=modeld, ckptd=ckptd, name=name)\n",
    "    if mconf[\"name\"] == \"DNNLSTM\":\n",
    "        LSTM, callbacks_, optimizer_ = NN.DNNLSTM(mconf[\"units\"], inshape=(timesteps, nfeatures), outshape=nfeatures, dropouts=mconf[\"dropouts\"], activations=mconf[\"activations\"])\n",
    "    elif mconf[\"name\"] == \"stackedLSTM\":\n",
    "        LSTM, callbacks_, optimizer_ = NN.stackedLSTM(shape=(timesteps, nfeatures), cells=60)\n",
    "    else:\n",
    "        logging.warning(\"model name undefined.\")\n",
    "        \n",
    "    LSTM.summary()\n",
    "    LSTM.compile(loss=loss, optimizer=optimizer_)\n",
    "    \n",
    "#     callbacks_ = checkpointer, earlystopper, reduceLR, tb, csvlogger\n",
    "    \n",
    "    if not earlystopper:\n",
    "        callbacks_.pop(1)\n",
    "\n",
    "    history = LSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "#     history = LSTM.fit(x=dg, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, verbose=2, shuffle=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history['loss'], label='train')\n",
    "    ax.plot(history.history['val_loss'], label='test')\n",
    "    ax.legend(fontsize=14)\n",
    "    plt.savefig(\"{}/{}_trainingHistory.png\".format(ckptd, name))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBuilder():\n",
    "    def __init__(self, modeld=\"model\", ckptd=\"ckpt\", name=\"NNBuilder\", optimizer=\"SGD\"):\n",
    "        \n",
    "        if not os.path.exists(modeld):\n",
    "            os.makedirs(modeld)\n",
    "            \n",
    "        if not os.path.exists(ckptd):\n",
    "            os.makedirs(ckptd)\n",
    "        \n",
    "        self.name = name\n",
    "        self.modeld = modeld\n",
    "        self.ckptd = ckptd\n",
    "        self.callbacks = self._callbacks(modeld, ckptd, name=name)\n",
    "        self.optimizer = self._optimizer(name=optimizer)\n",
    "        \n",
    "            \n",
    "        \n",
    "    def CNN1D(self, filters, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None):\n",
    "        \n",
    "        _args       = NNBuilder._argreset(filters, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "                \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "\n",
    "        x = MaxPool1D(pool_size=2)(x)\n",
    "        x = Flatten()(x)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(x)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(x)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"CNN1D\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"CNN1D\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    \n",
    "    def TPALSTM(self):\n",
    "        embedding_layer = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "        sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        merged = concatenate([x1, y1])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        ########################################\n",
    "        ## add class weight\n",
    "        ########################################\n",
    "        if re_weight:\n",
    "            class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "        else:\n",
    "            class_weight = None\n",
    "\n",
    "        ########################################\n",
    "        ## train the model\n",
    "        ########################################\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "                outputs=preds)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer='nadam',\n",
    "                metrics=['acc'])\n",
    "        #model.summary()\n",
    "        print(STAMP)\n",
    "\n",
    "        early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "        bst_model_path = STAMP + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "        hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "                validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "                epochs=200, batch_size=2048, shuffle=True, \\\n",
    "                class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "        model.load_weights(bst_model_path)\n",
    "        bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "        ########################################\n",
    "        ## make the submission\n",
    "        ########################################\n",
    "        print('Start making the submission before fine-tuning')\n",
    "\n",
    "        preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "        preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "        preds /= 2\n",
    "\n",
    "        submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "        submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n",
    "    \n",
    "    \n",
    "    def _stackedLSTM(self, cells, inshape, outshape, target=\"regression\"):    \n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "   \n",
    "        model = Sequential()\n",
    "        \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    model.add(LSTM(units[idx], input_shape=(timesteps, nfeatures), return_sequences=True, name=\"lstm_{}\".format(idx))) \n",
    "                elif idx == nlayer - 1:  # the last hidden layer\n",
    "                    model.add(LSTM(units[idx], name=\"lstm_{}\".format(idx))) \n",
    "                else:\n",
    "                    model.add(LSTM(units[idx], return_sequences=True, name=\"lstm_{}\".format(idx))) \n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm_0\")) \n",
    "                \n",
    "        if target == \"regression\":\n",
    "            model.add(Dense(nfeatures, activation='sigmoid', name=\"dense\"))  # for regression\n",
    "        else:\n",
    "            model.add(Dense(nfeatures, activation='softmax', name=\"dense\"))  # for classification\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    def stackedLSTM(self, cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0):    \n",
    "        \n",
    "        '''\n",
    "            - dropout, applied to the first operation on the inputs\n",
    "            - recurrent_dropout, applied to the other operation on the recurrent inputs (previous output and/or states)\n",
    "        '''\n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "           \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    x = LSTM(units[idx], return_sequences=True, dropout=dropout, name=\"LSTM_{}\".format(idx + 1))(inputs) \n",
    "                elif idx == nlayer - 1:  # the last hidden layer\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, name=\"LSTM_{}\".format(idx + 1))(x) \n",
    "                else:\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, return_sequences=True, name=\"LSTM_{}\".format(idx + 1))(x)\n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            x = LSTM(units[0], dropout=dropout, name=\"LSTM_1\")(inputs)\n",
    "                \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"stackedLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"stackedLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def LSTMbasicAttention(self, shape, cells):\n",
    "        '''\n",
    "            shape = (timestep, feature)\n",
    "            return [model, optimizer, callbacks]\n",
    "        '''\n",
    "        \n",
    "        nfeatures = shape[1]\n",
    "        \n",
    "        inputs = Input(shape, name=\"input\")  # return a tensor\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "        else:\n",
    "            units = [cells]\n",
    "                \n",
    "        for idx, unit in enumerate(units):\n",
    "            if idx == 0:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(inputs)\n",
    "            else:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(x)\n",
    "            x = Attention(shape[0])(x)\n",
    "            \n",
    "        outputs = Dense(nfeatures)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    \n",
    "    def DNNLSTM(self, units, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None):\n",
    "        \n",
    "        '''\n",
    "            units: [units for Dense_1, ..., units for Dense_n, cells for LSTM_1], i.e. units[-1]: cells for LSTM_1\n",
    "            inshape: (timesteps, # of features)\n",
    "            outshape: an integer number for output layer (Dense), ex. 4\n",
    "            \n",
    "            NNB = NNBuilder()\n",
    "            model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 4], inshape=(10, 4), outshape=4, batchNormalization=None)\n",
    "            model.summary()\n",
    "            _________________________________________________________________\n",
    "            Model: \"DNNLSTM\"\n",
    "            _________________________________________________________________\n",
    "            Layer (type)                 Output Shape              Param #   \n",
    "            =================================================================\n",
    "            input (InputLayer)           [(None, 10, 4)]           0         \n",
    "            _________________________________________________________________\n",
    "            TDense_1 (TimeDistributed)   (None, 10, 10)            50         (4 * 10 + 10)\n",
    "            _________________________________________________________________\n",
    "            Activation_1 (TimeDistribute (None, 10, 10)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_2 (TimeDistributed)   (None, 10, 20)            220        (10 * 20 + 20)  \n",
    "            _________________________________________________________________\n",
    "            Activation_2 (TimeDistribute (None, 10, 20)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_3 (TimeDistributed)   (None, 10, 30)            630        (20 * 30 + 30)    \n",
    "            _________________________________________________________________\n",
    "            Activation_3 (TimeDistribute (None, 10, 30)            0         \n",
    "            _________________________________________________________________  ↓ (input gate, forget gate, output gate and neuron) \n",
    "            LSTM (LSTM)                  (None, 4)                 560        (4 * (30 * 4 + 4 + 4 * 4))\n",
    "            _________________________________________________________________                    ↑ (cell state pass to the other cells) \n",
    "            output (Dense)               (None, 4)                 20         (4 * 4 + 4)       \n",
    "            =================================================================\n",
    "            Total params: 1,480\n",
    "            Trainable params: 1,480\n",
    "            Non-trainable params: 0\n",
    "            \n",
    "            (4 * 10 + 10) + (10 * 20 + 20) + (20 * 30 + 30) + 4 * (30 * 4 + 4 + 4 * 4) + (4 * 4 + 4)   \n",
    "        '''\n",
    "        \n",
    "        assert len(units) >= 2\n",
    "        \n",
    "        inputs = Input(inshape, name=\"input\")  \n",
    "\n",
    "        nlayer = len(units) - 1\n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "        else:\n",
    "            activations = [\"relu\" for _ in range(nlayer)]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = TimeDistributed(BatchNormalization(), name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = TimeDistributed(Activation(activations[i]), name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = TimeDistributed(Dropout(dropouts[i]), name=\"Dropout_{}\".format(i + 1))(x)\n",
    "        x = LSTM(units[-1], name=\"LSTM\")(x)\n",
    "        \n",
    "        \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"DNNLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"DNNLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def DenseBuilder(units, inputs, batchNormalization=True, dropouts=None, activations=None):\n",
    "        \n",
    "        _args       = NNBuilder._argreset(units, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "    \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _argreset(units, dropouts=None, activations=None):\n",
    "        \n",
    "        _args = dict()\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(units, list):\n",
    "            nlayer = len(units)\n",
    "        else:\n",
    "            units = [units]\n",
    "        \n",
    "        _args[\"units\"] = units\n",
    "        _args[\"nlayer\"] = nlayer\n",
    "        \n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "            _args[\"dropouts\"] = dropouts\n",
    "        else:\n",
    "            _args[\"dropouts\"] = None\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "            _args[\"activations\"] = activations\n",
    "        else:\n",
    "            _args[\"activations\"] = None\n",
    "#             activations = [\"relu\" for _ in range(nlayer)]\n",
    "            \n",
    "        return _args\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _callbacks(modeld, ckptd, mmonitor=\"val_loss\", emonitor=\"loss\", lmonitor=\"val_loss\", name=\"ckpt\"):\n",
    "        \n",
    "        '''\n",
    "            mmonitor: monitor for model \n",
    "            emonitor: monitor for earlystopping\n",
    "            lmonitor: monitor for learning rate\n",
    "        '''\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "        \n",
    "        name_ = \"{epoch:04d}_{loss:.3f}_{val_loss:.3f}\"\n",
    "#         checkpointer = ModelCheckpoint(filepath=os.path.join(modeld, \"{0}_{1}_{2}.hdf5\".format(name, name_, timestamp)),\n",
    "        checkpointer = ModelCheckpoint(filepath=os.path.join(modeld, \"{0}.hdf5\".format(name)),\n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       monitor=mmonitor)\n",
    "        \n",
    "        earlystopper = EarlyStopping(monitor=emonitor, patience=10)\n",
    "\n",
    "        reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.5, patience=10, min_lr=0.0001)\n",
    "\n",
    "#         reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.9, patience=10, min_lr=0.0001)\n",
    "        \n",
    "        tb = TensorBoard(log_dir=ckptd)\n",
    "\n",
    "        csvlogger = CSVLogger(os.path.join(ckptd, \"{}_{}.log\".format(name, timestamp)), append=False, separator=\",\")\n",
    "\n",
    "        # Learning rate schedule.\n",
    "    #     lr_schedule = LearningRateScheduler(fixed_schedule, verbose=0)\n",
    "\n",
    "        return [checkpointer, earlystopper, reduceLR, tb, csvlogger]\n",
    "\n",
    "#         return [checkpointer, earlystopper, reduceLR, csvlogger]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _optimizer(lr=1e-2, name=\"Adam\"):\n",
    "        if name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "        else:\n",
    "            optimizer = Adam(learning_rate=lr)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def mloader(filepath, custom_objects=None):\n",
    "        if custom_objects is not None:\n",
    "            return load_model(filepath, custom_objects=custom_objects)\n",
    "        else:\n",
    "            return load_model(filepath)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import tensorflow as tf\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    nMB  = 1024 * 8  # for tf.config.experimental.VirtualDeviceConfiguration\n",
    "    \n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], enable=True)\n",
    "#             tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=nMB)])\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    # 0. reset arguments\n",
    "    _args = NNBuilder._argreset([10, 20, 30], dropouts=0.5, activations=\"relu\")\n",
    "    print(_args)\n",
    "    \n",
    "    \n",
    "    # 1. DNNbuilder\n",
    "    inputs = Input(shape=(10), name=\"Input\")\n",
    "    x = NNBuilder.DenseBuilder([10, 30, 20, 40], inputs, dropouts=0.25, activations=\"relu\")\n",
    "    model = Model(inputs=inputs, outputs=x, name=\"DNN\")\n",
    "    model.summary()\n",
    "\n",
    "    # >>>>>> 2. DNNLSTM\n",
    "    NNB = NNBuilder()\n",
    "    model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 10], inshape=(6, 4), outshape=[4, 1], outactfn=[\"tanh\", \"sigmoid\"], batchNormalization=None)\n",
    "    model.summary()\n",
    "    plot_model(model, to_file=\"DNNLSTM.png\", show_shapes=True)\n",
    "\n",
    "    model.compile(loss={\"regression_output\": \"mae\", \"classification_output\": \"binary_crossentropy\"},\n",
    "                  metrics={\"regression_output\": \"mae\", \"classification_output\": \"accuracy\"},\n",
    "                  optimizer=optimizer_)\n",
    "\n",
    "    epochs = 1000\n",
    "    batch_size = 10\n",
    "    n = 10000\n",
    "#     X_train = np.random.random_sample((n, 6, 4))\n",
    "#     y_train = [np.random.random_sample((n, 4)), np.random.randint(low=0, high=2, size=(n, 1))]\n",
    "#     history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "\n",
    "    # >>>>>> 3. stackedLSTM\n",
    "    stackedLSTM, callbacks_, optimizer_ = NNBuilder().stackedLSTM(cells=[10, 20], inshape=[6, 4], outshape=[4, 4], outactfn=[\"sigmoid\", \"softmax\"])\n",
    "    stackedLSTM.compile(loss=\"mae\", optimizer=optimizer_)\n",
    "    stackedLSTM.summary()\n",
    "    \n",
    "    epochs = 3\n",
    "    batch_size = 200\n",
    "    X_train = np.random.random_sample((1000, 6, 4))\n",
    "    y_train = [np.random.random_sample((1000, 4)), np.random.randint(low=0, high=2, size=1000)]\n",
    "\n",
    "    \n",
    "#     history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "   \n",
    "#     saved_model = \"/home/yuzhe/DataScience/QC/model/lstm1_0154_0.009_0.008_202008071814.hdf5\"\n",
    "#     model = NNBuilder.mloader(saved_model)\n",
    "\n",
    "\n",
    "    # >>>>>> 4. CNN1D\n",
    "    CNN1D, callbacks_, optimizer_ = NNBuilder().CNN1D(filters=[10, 20], inshape=[6, 4], outshape=[2], outactfn=[\"sigmoid\"], activations=\"relu\")\n",
    "    CNN1D.summary()\n",
    "\n",
    "#     train(X_train, y_train, 30, 5000, loss=YZKError(element_weight=[1 / 6., 1 / 6., 1 / 6., 1 / 2.]), name=\"NNBuilderTest1\")\n",
    "#     train(X_train, y_train, 30, 5000, loss=YZKError(), name=\"NNBuilderTest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logcosh(a, t):\n",
    "    return (1 / a) * np.log(np.cosh(a * t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "#### check losses     \n",
    "    \n",
    "    from dgenerator import dgenerator\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    tperiod = [2016010101, 2016123124]\n",
    "    n_in = 6\n",
    "    n_out = 1\n",
    "    mode = \"test\"\n",
    "    vstack = True\n",
    "    fnpy = True\n",
    "    npyd = \"/home/yuzhe/DataScience/dataset\"\n",
    "    gif = \"/home/yuzhe/CODE/ProgramT1/GRDTools/SRC/RES/GI/1500_decode_stationlist_without_space.txt\"\n",
    "\n",
    "    dg = dgenerator(gif=gif, npyd=npyd)\n",
    "    vinfo = pd.DataFrame(dg.vrange)  \n",
    "    vinfo = pd.DataFrame(vinfo)\n",
    "    print(vinfo)\n",
    "#     vinfo = {\"Temp\": [-20.0, 50.0],\n",
    "#              \"RH\": [0.0, 100.0], \n",
    "#              \"Pres\": [600.0, 1100.0], \n",
    "#              \"Precp\": [0.0, 220.0]}\n",
    "    dataset = dg.hrfgenerator(tperiod, n_in=n_in, n_out=n_out, mode=mode, rescale=True, reformat=True, vstack=vstack, fnpy=fnpy, generator=False)\n",
    "    \n",
    "    datetimes = dataset[1]\n",
    "    nsize = len(datetimes)\n",
    "    print(dataset[0].shape)\n",
    "\n",
    "    \n",
    "    saved_model = \"../QC/model/lstm1_0055_0.008_0.011_202008111819_2.hdf5\"\n",
    "    model = NNBuilder.mloader(saved_model)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(vinfo.values)\n",
    "    \n",
    "    print(scaler.inverse_transform([[0.7, 0.6, 0.7, 0.2]]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    n = nsize\n",
    "    \n",
    "#     x = dataset[0][:, -4:]\n",
    "#     x = x[~np.isnan(x).any(axis=1)]\n",
    "#     idx = np.random.choice(np.arange(x.shape[0]), n, replace=False)\n",
    "#     x = x[idx, 0:]\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "\n",
    "    scaled = dataset[0]\n",
    "    scaled = scaled[~np.isnan(scaled).any(axis=1)]\n",
    "    X_test = np.reshape(scaled[:, :-4], (-1, 6, 4))\n",
    "    y_true = scaled[:, -4:]\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "#     xynorm = tf.norm(tf.subtract(y_pred, y_true), axis=1) \n",
    "\n",
    "    y_true = tf.reshape(y_true[:, -1], [-1, 1])\n",
    "    y_pred = tf.reshape(y_pred[:, -1], [-1, 1])\n",
    "    xynorm = tf.subtract(y_pred, y_true)\n",
    "\n",
    "    print(y_true.shape, y_true)\n",
    "    print(y_pred.shape, y_pred)\n",
    "    print(xynorm.shape)\n",
    "    \n",
    "    sample_weight = 1\n",
    "#     sample_weight = tf.broadcast_to(sample_weight, y_pred.shape)\n",
    "    \n",
    "    loss = YZKError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"YZK\")\n",
    "    print('yzk-loss: ', loss)\n",
    "    \n",
    "    loss = tf.losses.LogCosh(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "\n",
    "    print(\"shape of loss = {}, xynorm = {}\".format(loss, xynorm.shape))\n",
    "    ax.scatter(xynorm, loss, label=\"LogCosh\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MAE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Cos\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "\n",
    "#     loss = tf.keras.losses.KLDivergence(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "#     ax.scatter(xynorm, loss, label=\"KL\")\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"MSE\")\n",
    "\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MSLE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "    \n",
    "    loss = tf.keras.losses.Huber(reduction=tf.losses.Reduction.NONE, delta=0.25)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Huber\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_weights(training_data, class_weight_dictionary): \n",
    "    sample_weights = [class_weight_dictionary[np.where(one_hot_row==1)[0][0]] for one_hot_row in training_data]\n",
    "    return np.asarray(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectionalLSTM(cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0):    \n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "           \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    x = Bidirectional(LSTM(units[idx], return_sequences=True), name=\"BLSTM_{}\".format(idx + 1))(inputs)\n",
    "#                 elif idx == nlayer - 1:  # the last hidden layer\n",
    "#                     x = LSTM(units[idx], name=\"LSTM_{}\".format(idx + 1))(x) \n",
    "                else:\n",
    "                    x = Bidirectional(LSTM(units[idx], return_sequences=True), name=\"BLSTM_{}\".format(idx + 1))(x)\n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            x = LSTM(units[0], name=\"LSTM_1\")(inputs)\n",
    "           \n",
    "#         x = tf.strided_slice(x, [0, 2, 0], [-1, 2, 4])\n",
    "\n",
    "\n",
    "#         print(x[:, 3, :].shape)\n",
    "\n",
    "#         x = x[:, 2, :]\n",
    "        \n",
    "        print(x.shape)\n",
    "        \n",
    "        \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"stackedLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"stackedLSTM\")\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "#         return [model, self.callbacks, self.optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 7, 60)\n",
      "Model: \"stackedLSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 7, 4)]            0         \n",
      "_________________________________________________________________\n",
      "BLSTM_1 (Bidirectional)      (None, 7, 20)             1200      \n",
      "_________________________________________________________________\n",
      "BLSTM_2 (Bidirectional)      (None, 7, 40)             6560      \n",
      "_________________________________________________________________\n",
      "BLSTM_3 (Bidirectional)      (None, 7, 60)             17040     \n",
      "_________________________________________________________________\n",
      "LDense1_1 (Dense)            (None, 7, 30)             1830      \n",
      "_________________________________________________________________\n",
      "LDense1_2 (Dense)            (None, 7, 9)              279       \n",
      "_________________________________________________________________\n",
      "Loss1 (Dense)                (None, 7, 3)              30        \n",
      "=================================================================\n",
      "Total params: 26,939\n",
      "Trainable params: 26,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fc4fc2a3190>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidirectionalLSTM([10, 20, 30], [7, 4], [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 6, 30)\n",
      "Model: \"stackedLSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 6, 4)]            0         \n",
      "_________________________________________________________________\n",
      "LSTM_1 (LSTM)                (None, 6, 10)             600       \n",
      "_________________________________________________________________\n",
      "LSTM_2 (LSTM)                (None, 6, 20)             2480      \n",
      "_________________________________________________________________\n",
      "LSTM_3 (LSTM)                (None, 6, 30)             6120      \n",
      "_________________________________________________________________\n",
      "LDense1_1 (Dense)            (None, 6, 30)             930       \n",
      "_________________________________________________________________\n",
      "LDense1_2 (Dense)            (None, 6, 9)              279       \n",
      "_________________________________________________________________\n",
      "Loss1 (Dense)                (None, 6, 3)              30        \n",
      "=================================================================\n",
      "Total params: 10,439\n",
      "Trainable params: 10,439\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fc6e007fd50>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bidirectionalLSTM([10, 20, 30], [6, 4], [3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
