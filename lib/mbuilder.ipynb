{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model builder\n",
    "- date: 2020-08-07\n",
    "- maintainer: YZK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:56:21.098461Z",
     "iopub.status.busy": "2021-03-10T05:56:21.098018Z",
     "iopub.status.idle": "2021-03-10T05:56:21.102179Z",
     "shell.execute_reply": "2021-03-10T05:56:21.101417Z",
     "shell.execute_reply.started": "2021-03-10T05:56:21.098405Z"
    }
   },
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to script mbuilder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-30T08:06:50.241780Z",
     "iopub.status.busy": "2021-03-30T08:06:50.241565Z",
     "iopub.status.idle": "2021-03-30T08:07:00.277347Z",
     "shell.execute_reply": "2021-03-30T08:07:00.276892Z",
     "shell.execute_reply.started": "2021-03-30T08:06:50.241760Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import deque, Counter\n",
    "from fbprophet import Prophet\n",
    "from functools import partial\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential, Model, losses\n",
    "from tensorflow.keras import constraints, initializers, regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Lambda, Layer, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Conv1D, Conv2D, Dense, Dropout, Flatten, Input, LSTM, MaxPool1D, MaxPool2D\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import msetup\n",
    "    msetup.setLogging(loglv=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:00.823231Z",
     "iopub.status.busy": "2021-03-10T05:57:00.822964Z",
     "iopub.status.idle": "2021-03-10T05:57:00.826584Z",
     "shell.execute_reply": "2021-03-10T05:57:00.825733Z",
     "shell.execute_reply.started": "2021-03-10T05:57:00.823199Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-30T08:07:00.278274Z",
     "iopub.status.busy": "2021-03-30T08:07:00.278125Z",
     "iopub.status.idle": "2021-03-30T08:07:01.620230Z",
     "shell.execute_reply": "2021-03-30T08:07:01.619530Z",
     "shell.execute_reply.started": "2021-03-30T08:07:00.278254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 2, 2, 3)           39        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 52\n",
      "Trainable params: 52\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn1 = Sequential()\n",
    "cnn1.add(Conv2D(filters=3, kernel_size=(2, 2), input_shape=(3, 3, 3)))\n",
    "cnn1.add(Flatten())\n",
    "cnn1.add(Dense(1))\n",
    "cnn1.compile(loss='mae', optimizer='Adam')\n",
    "cnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:00.828548Z",
     "iopub.status.busy": "2021-03-10T05:57:00.828254Z",
     "iopub.status.idle": "2021-03-10T05:57:00.907951Z",
     "shell.execute_reply": "2021-03-10T05:57:00.906579Z",
     "shell.execute_reply.started": "2021-03-10T05:57:00.828515Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstmbuilder(units, input_shape, loss, optimizer):\n",
    "    '''\n",
    "        input_shape: a tuple (timesteps, nfeatures)\n",
    "    '''\n",
    "    \n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units, input_shape=input_shape))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.compile(loss=loss, optimizer=optimizer)\n",
    "             \n",
    "    return lstm\n",
    "\n",
    "\n",
    "# lstmbuilder(10, (10, 3), 'mae', SGD()).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:00.909720Z",
     "iopub.status.busy": "2021-03-10T05:57:00.909414Z",
     "iopub.status.idle": "2021-03-10T05:57:00.953021Z",
     "shell.execute_reply": "2021-03-10T05:57:00.952051Z",
     "shell.execute_reply.started": "2021-03-10T05:57:00.909689Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking. \n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \n",
    "        tensorflow & keras reference:\n",
    "            https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "            https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "            https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
    "            \n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)  # inherit Layer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3, [batchsize, timestep, # of feature]\n",
    "        '''\n",
    "        \n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "#         self.step_dim = input_shape[-2]\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "        '''\n",
    "        \n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:00.954611Z",
     "iopub.status.busy": "2021-03-10T05:57:00.954342Z",
     "iopub.status.idle": "2021-03-10T05:57:00.978979Z",
     "shell.execute_reply": "2021-03-10T05:57:00.978084Z",
     "shell.execute_reply.started": "2021-03-10T05:57:00.954579Z"
    }
   },
   "outputs": [],
   "source": [
    "def l2norm(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        l2 = np.sum(x**2)\n",
    "    else:\n",
    "        l2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=x)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def cosine_similarity(y_true, y_pred, axis=1):\n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "#     l2norm1 = tf.reduce_sum(tf.square(y_true), axis=axis)\n",
    "#     l2norm2 = tf.reduce_sum(tf.square(y_pred), axis=axis)\n",
    "    l2norm1 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_true)\n",
    "    l2norm2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_pred)\n",
    "    yy = tf.reduce_sum(tf.multiply(y_true, y_pred), axis=axis)\n",
    "        \n",
    "    return -tf.divide(yy, tf.multiply(tf.sqrt(l2norm1), tf.sqrt(l2norm2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Losses\n",
    "- Inherits From [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss)\n",
    "- Reference of Implement\n",
    "1. [BaseLossClass](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/losses.py#L47)\n",
    "2. [LossFunctionWrapper](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/losses.py#L213)\n",
    "3. [CategoricalCrossentropy](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/losses.py#L587-L662)\n",
    "- When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras`, `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom training [tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) for more details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:00.980488Z",
     "iopub.status.busy": "2021-03-10T05:57:00.980240Z",
     "iopub.status.idle": "2021-03-10T05:57:00.995838Z",
     "shell.execute_reply": "2021-03-10T05:57:00.994858Z",
     "shell.execute_reply.started": "2021-03-10T05:57:00.980457Z"
    }
   },
   "outputs": [],
   "source": [
    "class YZKError(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None, \n",
    "                 penalized=None):\n",
    "    \n",
    "        \"\"\" \n",
    "            Initializes `YZKError` instance. get_config() need   \n",
    "\n",
    "            Args:\n",
    "              reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "                loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "                option will be determined by the usage context. For almost all cases\n",
    "                this defaults to `SUM_OVER_BATCH_SIZE`. \n",
    "              name: Optional name for the op. \n",
    "        \"\"\"\n",
    "\n",
    "        super(YZKError, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "        self.penalized = penalized\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        element_weight = self.element_weight\n",
    "\n",
    "#         logging.info(self.reduction, element_weight)\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        batchsize = y_pred.shape[0] \n",
    "        assert batchsize == y_true.shape[0]\n",
    "        \n",
    "#         logging.info(\"YZKError, shape of y_true = {}, y_pred = {}\".format(y_true.shape, y_pred.shape))\n",
    "        \n",
    "        PLoss = 0.    \n",
    "        penalized = self.penalized\n",
    "        if penalized is not None:\n",
    "            \n",
    "            _y_pred = tf.convert_to_tensor(y_pred[:, penalized])\n",
    "            _y_true = tf.convert_to_tensor(y_true[:, penalized])\n",
    "            \n",
    "            _y_pred = tf.reshape(_y_pred, [-1, 1])\n",
    "            _y_true = tf.reshape(_y_true, [-1, 1])\n",
    "#                 PLoss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true[:, penalized], y_pred[:, penalized])\n",
    "            PLoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(_y_true, _y_pred)\n",
    "        \n",
    "#         HuberLoss = 0\n",
    "#         MAELoss = 0\n",
    "#         if element_weight is not None:\n",
    "#             element_weight = tf.convert_to_tensor(element_weight)\n",
    "#             if element_weight.shape != []:  # is not a scale\n",
    "#                 nelement = y_pred.shape[1]\n",
    "#                 assert nelement == y_true.shape[1] \n",
    "#                 element_weight = tf.broadcast_to(element_weight, [batchsize, nelement])\n",
    "                \n",
    "#             y_pred_ = tf.math.multiply(y_pred, element_weight)\n",
    "#             y_true_ = tf.math.multiply(y_true, element_weight)\n",
    "        \n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true_, y_pred_)\n",
    "#         else:\n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true, y_pred)\n",
    "        \n",
    "        MAELoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "\n",
    "    #         CosSimLoss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "        CosSimLoss = tf.losses.CosineSimilarity(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "#         logging.info(\"YZKError, PLoss: {}\\n, HuberLoss: {}\\n, MAELoss: {}\\n, CosSimLoss: {}\\n\".format(PLoss, HuberLoss, MAELoss, CosSimLoss))\n",
    "        \n",
    "        if penalized is not None:\n",
    "            return tf.math.add(tf.math.add(tf.math.scalar_mul(3, PLoss), tf.math.scalar_mul(2, MAELoss)), tf.math.scalar_mul(1, CosSimLoss))\n",
    "        else:\n",
    "            return tf.math.add(tf.math.scalar_mul(2, MAELoss), tf.math.scalar_mul(1, CosSimLoss))\n",
    "        \n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the config dictionary for a `Loss` instance.\"\"\"\n",
    "        return {'reduction': self.reduction, 'name': self.name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inverse transform sigmoid(x)\n",
    "<center>\n",
    "<font size=4>\n",
    "$\n",
    "\\begin{align}\n",
    "\\sigma &=\\frac{1}{1+e^{-x}} \\\\\n",
    "\\frac{1}{\\sigma} &= 1+e^{-x} \\\\\n",
    "\\frac{1}{\\sigma}-1 &= e^{-x} \\\\\n",
    "\\log{\\frac{1-\\sigma}{\\sigma}} &= \\log{e^{-x}} \\\\\n",
    "x &= -\\log{\\frac{1-\\sigma}{\\sigma}} \\\\\n",
    "x &= \\log{\\frac{\\sigma}{1-\\sigma}}\n",
    "\\end{align}\n",
    "$\n",
    "</font size>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Crossentropy\n",
    "\n",
    "<center><font size=4>$BCE=y_{true}\\cdot\\log{y_{pred}}+\\left(1-y_{true}\\right)\\cdot\\log{\\left(1-y_{pred}\\right)}$</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:00.997620Z",
     "iopub.status.busy": "2021-03-10T05:57:00.997126Z",
     "iopub.status.idle": "2021-03-10T05:57:01.019821Z",
     "shell.execute_reply": "2021-03-10T05:57:01.018808Z",
     "shell.execute_reply.started": "2021-03-10T05:57:00.997584Z"
    }
   },
   "outputs": [],
   "source": [
    "class WeightedBinaryCrossntropy(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None):\n",
    "    \n",
    "        \"\"\" \n",
    "            Args:\n",
    "                element_weight: a weight list [weight for y_true=1, weight for y_true=0]\n",
    "            Ex.\n",
    "                WeightedBinaryCrossntropy(reduction=tf.losses.Reduction.NONE, element_weight=[5, 1])(y_true, y_pred)\n",
    "        \"\"\"\n",
    "\n",
    "        super(WeightedBinaryCrossntropy, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "    \n",
    "    def call(self, y_true, y_pred, from_logits=False):\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        \n",
    "        element_weight = self.element_weight\n",
    "        \n",
    "#         print(element_weight)\n",
    "        \n",
    "        if not from_logits:  # after activation\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "            y_pred = tf.math.log(y_pred / (1 - y_pred))\n",
    "            y_pred = tf.math.sigmoid(y_pred)\n",
    "            \n",
    "        if element_weight is None:\n",
    "            bc = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        else:\n",
    "            bc = -(y_true * tf.math.log(y_pred) * element_weight[0] + (1 - y_true) * tf.math.log(1 - y_pred) * element_weight[1])\n",
    "\n",
    "        return tf.math.reduce_mean(bc, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T08:22:32.495039Z",
     "iopub.status.busy": "2021-02-08T08:22:32.494808Z",
     "iopub.status.idle": "2021-02-08T08:22:32.497438Z",
     "shell.execute_reply": "2021-02-08T08:22:32.496949Z",
     "shell.execute_reply.started": "2021-02-08T08:22:32.495013Z"
    }
   },
   "source": [
    "# Customized Layers\n",
    "- [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
    "- [masking_and_padding](https://www.tensorflow.org/guide/keras/masking_and_padding)\n",
    "- [Masking](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking)\n",
    "- Layers are recursively composable</br>\n",
    "If you assign a Layer instance as an attribute of another Layer, <font color=\"red\">the outer layer will start tracking the weights of the inner layer.</font></br>\n",
    "We recommend creating such sublayers in the <font color=\"gray\">\\_\\_init\\_\\_()</font> method (since the sublayers will typically have a build method, they will be built when the outer layer gets built).\n",
    "- You can optionally enable serialization on your layers  \n",
    "If you need your custom layers to be serializable as part of a Functional model, you can optionally implement a <font color=\"gray\">get_config()</font> method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T08:32:30.494670Z",
     "iopub.status.busy": "2021-02-25T08:32:30.494432Z",
     "iopub.status.idle": "2021-02-25T08:32:30.498284Z",
     "shell.execute_reply": "2021-02-25T08:32:30.497666Z",
     "shell.execute_reply.started": "2021-02-25T08:32:30.494645Z"
    }
   },
   "source": [
    "# Reference\n",
    "- [Weight initialization & Batch Normalization](https://reurl.cc/9ZRela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:01.022182Z",
     "iopub.status.busy": "2021-03-10T05:57:01.021892Z",
     "iopub.status.idle": "2021-03-10T05:57:01.052643Z",
     "shell.execute_reply": "2021-03-10T05:57:01.051561Z",
     "shell.execute_reply.started": "2021-03-10T05:57:01.022145Z"
    }
   },
   "outputs": [],
   "source": [
    "class TPCNN1D(Layer):\n",
    "\n",
    "    '''\n",
    "        temporal pattern 1d-CNN\n",
    "        input shape = [None, timesteps, # of cells], channel is timesteps\n",
    "        output shape = [None, features, filters] or [None, m, k]\n",
    "        \n",
    "        Layers are recursively composable\n",
    "            If you assign a Layer instance as an attribute of another Layer, \n",
    "            the outer layer will start tracking the weights of the inner layer.\n",
    "            We recommend creating such sublayers in the __init__() method (since the sublayers will typically have a build method, t\n",
    "            hey will be built when the outer layer gets built).\n",
    "    '''\n",
    "    def __init__(self, filters, name=\"TPCNN1D\", **kwargs):\n",
    "        super(TPCNN1D, self).__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        self.cnn1d = Conv1D(kernel_size=1, filters=self.filters, data_format='channels_first', name=self.name)\n",
    "\n",
    "        \n",
    "#     def build(self, input_shape):\n",
    "#         self.cnn1d = Conv1D(kernel_size=1, filters=self.filters, data_format='channels_first', name=self.name)\n",
    "#         self.cnn1d.build(input_shape)\n",
    "#         self._trainable_weights = self.cnn1d.trainable_weights\n",
    "        \n",
    "#         super(TPCNN1D, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "#         print(self.cnn1d(x))\n",
    "#         return self.cnn1d(x)\n",
    "        # if channel first, then output shape = [None, filters, features] that need to transpose to [None, features, filters]\n",
    "        return tf.transpose(self.cnn1d(x), perm=[0, 2, 1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"filters\": self.filters}\n",
    "        base_config = super(TPCNN1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:01.054253Z",
     "iopub.status.busy": "2021-03-10T05:57:01.054012Z",
     "iopub.status.idle": "2021-03-10T05:57:01.105934Z",
     "shell.execute_reply": "2021-03-10T05:57:01.104816Z",
     "shell.execute_reply.started": "2021-03-10T05:57:01.054224Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1], units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.W)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:01.107500Z",
     "iopub.status.busy": "2021-03-10T05:57:01.107244Z",
     "iopub.status.idle": "2021-03-10T05:57:01.132090Z",
     "shell.execute_reply": "2021-03-10T05:57:01.131074Z",
     "shell.execute_reply.started": "2021-03-10T05:57:01.107471Z"
    }
   },
   "outputs": [],
   "source": [
    "class TPAttention(Layer):\n",
    "    def __init__(self, timesteps=None, features=None,\n",
    "                 W_regularizer=None, W_constraint=None, name=\"TPAttention\", **kwargs):\n",
    "        '''     \n",
    "            Temporal Pattern Attention [https://arxiv.org/abs/1809.04206]             \n",
    "                \n",
    "                - k: filters or timesteps - 1 \n",
    "                - m: features\n",
    "            \n",
    "            Keras Layer that implements an Attention mechanism for temporal data.\n",
    "            Supports Masking. \n",
    "            \n",
    "            Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "            # Input shape\n",
    "                3D tensor with shape: `(samples, steps, features)`.\n",
    "            # Output shape\n",
    "                2D tensor with shape: `(samples, features)`.\n",
    "                \n",
    "            :param kwargs:\n",
    "            Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "            The dimensions are inferred based on the output shape of the RNN.\n",
    "            Example:\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(Attention())\n",
    "            \n",
    "        '''\n",
    "        super(TPAttention, self).__init__(name=name, **kwargs)  # inherits from Layer\n",
    "\n",
    "#         self.Linear()\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3, [batchsize, timestep, # of feature]\n",
    "        '''\n",
    "\n",
    "        if self.timesteps is None:  # k + 1\n",
    "            self.timesteps = input_shape[-1] + 1\n",
    "            \n",
    "        if self.features is None:  # m\n",
    "            self.features = input_shape[-2]   \n",
    "\n",
    "        # shape of W = [k, m]\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.features,),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "        # shape of W_h = [m, m]        \n",
    "        self.W_h = self.add_weight(shape=(self.features, self.features,),\n",
    "                                   initializer=self.init,\n",
    "                                   name='{}_Wh'.format(self.name))\n",
    "        \n",
    "        # shape of W_v = [m, k]\n",
    "        self.W_v = self.add_weight(shape=(self.features, input_shape[-1],),\n",
    "                                   initializer=self.init,\n",
    "                                   name='{}_Wv'.format(self.name))\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, h_t, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "            \n",
    "            Notations:\n",
    "                - H^C (= x): Convlutional operations on return sequence of LSTM, [None, m, k]\n",
    "                - h_t: hidden state of current time, [None, m]\n",
    "                - k: filters or timesteps - 1 \n",
    "                - m: features\n",
    "                \n",
    "            x.shape = [None, k, m]\n",
    "            h_t.shape = [None, m] \n",
    "        '''        \n",
    "\n",
    "        features = self.features\n",
    "        timesteps = self.timesteps\n",
    "        \n",
    "        logging.info(\"features = {}\".format(features))\n",
    "        logging.info(\"timesteps = {}\".format(timesteps))\n",
    "        logging.info(\"x.shape (batchsize, m, k) = {}\".format(x.shape))\n",
    "        logging.info(\"W.shape (k, m) = {}\".format(self.W.shape))\n",
    "        logging.info(\"h_t.shape (batchsize, m) (need to expand dim) = {}\".format(h_t.shape))\n",
    "        \n",
    "        scored = tf.matmul(x, self.W)\n",
    "        scored = tf.matmul(scored, tf.expand_dims(h_t, axis=-1))  # let shape of h_t = [None, m, 1], then shape of scored = [None, m, 1] \n",
    "        alpha_i = tf.math.sigmoid(scored)  # [None, m, 1]        \n",
    "        context_vector = tf.matmul(tf.ones((1, features)), tf.multiply(alpha_i, x))  # row summation\n",
    "        context_vector = tf.transpose(context_vector, perm=[0, 2, 1])  # [None, k, 1] \n",
    "        \n",
    "        logging.info(\"scored.shape (batchsize, m, 1) = {}\".format(scored.shape))\n",
    "        logging.info(\"alpha_i.shape (batchsize, m, 1) = {}\".format(alpha_i.shape))        \n",
    "        logging.info(\"shape of W_h (m, m) = {}, h_t (batchsize, m, 1) = {}\".format(self.W_h.shape, tf.reshape(h_t, (-1, features, 1)).shape))\n",
    "        logging.info(\"shape of W_v (m, k) = {}, context_vector (batchsize, k, 1) = {}\".format(self.W_v.shape, context_vector.shape))\n",
    "        return tf.reshape(tf.matmul(self.W_h, tf.reshape(h_t, (-1, features, 1))) + tf.matmul(self.W_v, context_vector), (-1, features))\n",
    "      \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "#         if mask is not None:\n",
    "#             # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "#             a *= tf.cast(mask, K.floatx())\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         #return input_shape[0], input_shape[-1]\n",
    "#         return input_shape[0], self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\"timesteps\": self.timesteps, \n",
    "                  \"features\": self.features, \n",
    "                  \"W_regularizer\": self.W_regularizer, \n",
    "                  \"W_constraint\": self.W_constraint}\n",
    "        base_config = super(TPAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:01.133503Z",
     "iopub.status.busy": "2021-03-10T05:57:01.133266Z",
     "iopub.status.idle": "2021-03-10T05:57:01.154956Z",
     "shell.execute_reply": "2021-03-10T05:57:01.153852Z",
     "shell.execute_reply.started": "2021-03-10T05:57:01.133474Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChangeableLossw(Callback):\n",
    "    def __init__(self, lossw, wmultiplier):\n",
    "    \n",
    "        self.nlossw = 2\n",
    "        self.lossw = lossw\n",
    "        self.wmultiplier = wmultiplier\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch <= 10:\n",
    "#             logf = \"epoch {}, \".format(epoch)\n",
    "            for idx in range(self.nlossw):\n",
    "                K.set_value(self.lossw[idx], K.get_value(self.lossw[idx]) * self.wmultiplier[idx])\n",
    "#                 logf += \"lossw_{} = {}, \".format(idx, K.get_value(self.lossw[idx]))\n",
    "#             logf += \"\\n\"\n",
    "        \n",
    "#             logging.info(logf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T07:20:58.949923Z",
     "iopub.status.busy": "2021-02-08T07:20:58.949700Z",
     "iopub.status.idle": "2021-02-08T07:20:58.952436Z",
     "shell.execute_reply": "2021-02-08T07:20:58.951909Z",
     "shell.execute_reply.started": "2021-02-08T07:20:58.949896Z"
    }
   },
   "source": [
    "# Neuron Network Builder\n",
    "- [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda)\n",
    "- [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) \n",
    "- [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)\n",
    "- [Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T05:48:24.781447Z",
     "iopub.status.busy": "2021-02-26T05:48:24.780895Z",
     "iopub.status.idle": "2021-02-26T05:48:24.786005Z",
     "shell.execute_reply": "2021-02-26T05:48:24.785189Z",
     "shell.execute_reply.started": "2021-02-26T05:48:24.781357Z"
    }
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T05:49:00.922309Z",
     "iopub.status.busy": "2021-02-26T05:49:00.922038Z",
     "iopub.status.idle": "2021-02-26T05:49:00.927097Z",
     "shell.execute_reply": "2021-02-26T05:49:00.926344Z",
     "shell.execute_reply.started": "2021-02-26T05:49:00.922282Z"
    }
   },
   "source": [
    "- [How LSTM networks solve the problem of vanishing gradients](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577)\n",
    "- bidirectional LSTM，兩個方向的LSTM是獨立的，參數不共享（架構可不同，只要timestep能處理成相同），但output（同時間的hidden states）會堆疊在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:01.157091Z",
     "iopub.status.busy": "2021-03-10T05:57:01.156763Z",
     "iopub.status.idle": "2021-03-10T05:57:01.258517Z",
     "shell.execute_reply": "2021-03-10T05:57:01.257579Z",
     "shell.execute_reply.started": "2021-03-10T05:57:01.157057Z"
    }
   },
   "outputs": [],
   "source": [
    "class NNBuilder():\n",
    "    def __init__(self, modeld=\"model\", ckptd=\"ckpt\", name=\"NNBuilder\", optimizer=\"SGD\"):\n",
    "        \n",
    "        if not os.path.exists(modeld):\n",
    "            os.makedirs(modeld)\n",
    "            \n",
    "        if not os.path.exists(ckptd):\n",
    "            os.makedirs(ckptd)\n",
    "        \n",
    "        self.name = name\n",
    "        self.modeld = modeld\n",
    "        self.ckptd = ckptd\n",
    "        \n",
    "#         self.callbacks = self._callbacks(modeld, ckptd, name=name)\n",
    "#         self.optimizer = self._optimizer(name=optimizer)\n",
    "        \n",
    "    def setObjV(self, optimizer=None, callbacks=None):\n",
    "        if optimizer is None:\n",
    "            self.optimizer = self._optimizer()\n",
    "        else:\n",
    "            self.optimizer = self._optimizer(name=optimizer[\"name\"], lr=optimizer[\"lr\"])\n",
    "\n",
    "        if callbacks is None:\n",
    "            self.callbacks = self._callbacks()\n",
    "        else:\n",
    "            self.callbacks = self._callbacks(mmonitor=callbacks[\"mmonitor\"], \n",
    "                                             emonitor=callbacks[\"emonitor\"], \n",
    "                                             lmonitor=callbacks[\"lmonitor\"])    \n",
    "        \n",
    "    def CNN1D(self, filters, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None, optimizer=None, callbacks=None):\n",
    "        \n",
    "        self.name = \"CNN1D\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        _args       = NNBuilder._argreset(filters, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "                \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "\n",
    "        x = MaxPool1D(pool_size=2)(x)\n",
    "        x = Flatten()(x)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(x)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(x)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"CNN1D\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"CNN1D\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def bidirectionalLSTM(self, cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0, merge_mode='concat', optimizer=None, callbacks=None):    \n",
    "        \n",
    "        '''\n",
    "            input: [batchsize, timesteps, nfeatures]\n",
    "            merge_mode: one of {'sum', 'mul', 'concat', 'ave', None}.\n",
    "            if merge_mode='concat', then shape of LSTM output is [timesteps, # of cells * 2 (directions)] \n",
    "        '''\n",
    "        \n",
    "        self.name = \"bidirectionalLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]          \n",
    "       \n",
    "        for idx in range(nlayer):\n",
    "            if idx == 0:  # the first hidden layer\n",
    "                x = Bidirectional(layer=LSTM(units[idx], return_sequences=True), \n",
    "                                  backward_layer=LSTM(units[idx], return_sequences=True, go_backwards=True), \n",
    "                                  merge_mode=merge_mode,\n",
    "                                  name=\"BLSTM_{}\".format(idx + 1))(inputs)\n",
    "            else:\n",
    "                x = Bidirectional(layer=LSTM(units[idx], return_sequences=True), \n",
    "                                  backward_layer=LSTM(units[idx], return_sequences=True, go_backwards=True), \n",
    "                                  merge_mode=merge_mode,\n",
    "                                  name=\"BLSTM_{}\".format(idx + 1))(x)\n",
    "\n",
    "            logging.info(\"{0:02d}, x.shape = {1}\".format(idx + 1, x.shape))\n",
    "\n",
    "#         x = tf.strided_slice(x, [0, 2, 0], [-1, 2, 4])\n",
    "\n",
    "        assert x.shape[1] == timesteps\n",
    "\n",
    "        H = Lambda(lambda x: x[:, 0:-1, :], name=\"H\")(x)\n",
    "        h_t = Lambda(lambda x: x[:, -1, :], name=\"h_t\")(x)\n",
    "        x = TPCNN1D(filters=units[-1], name=\"TPCNN1D\")(H)\n",
    "        x = TPAttention(name=\"TPAttention\")(x, h_t)\n",
    "\n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1\")(x)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2\")(x)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"bidirectionalLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"bidirectionalLSTM\")\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    def stackedLSTM(self, cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0, optimizer=None, callbacks=None):    \n",
    "        \n",
    "        '''\n",
    "            - dropout, applied to the first operation on the inputs\n",
    "            - recurrent_dropout, applied to the other operation on the recurrent inputs (previous output and/or states)\n",
    "        '''\n",
    "        \n",
    "        self.name = \"stackedLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "                \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "           \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    x = LSTM(units[idx], return_sequences=True, dropout=dropout, name=\"LSTM_{}\".format(idx + 1))(inputs) \n",
    "                elif idx == nlayer - 1:  # the last hidden layer\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, name=\"LSTM_{}\".format(idx + 1))(x) \n",
    "                else:\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, return_sequences=True, name=\"LSTM_{}\".format(idx + 1))(x)\n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            x = LSTM(units[0], dropout=dropout, name=\"LSTM_1\")(inputs)\n",
    "                \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"stackedLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"stackedLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def LSTMbasicAttention(self, shape, cells, optimizer=None, callbacks=None):\n",
    "        '''\n",
    "            shape = (timestep, feature)\n",
    "            return [model, optimizer, callbacks]\n",
    "        '''\n",
    "        \n",
    "        self.name = \"stackedLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        nfeatures = shape[1]\n",
    "        \n",
    "        inputs = Input(shape, name=\"input\")  # return a tensor\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "        else:\n",
    "            units = [cells]\n",
    "                \n",
    "        for idx, unit in enumerate(units):\n",
    "            if idx == 0:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(inputs)\n",
    "            else:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(x)\n",
    "            x = Attention(shape[0])(x)\n",
    "            \n",
    "        outputs = Dense(nfeatures)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    \n",
    "    def DNNLSTM(self, units, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None, optimizer=None, callbacks=None):\n",
    "        \n",
    "        '''\n",
    "            units: [units for Dense_1, ..., units for Dense_n, cells for LSTM_1], i.e. units[-1]: cells for LSTM_1\n",
    "            inshape: (timesteps, # of features)\n",
    "            outshape: an integer number for output layer (Dense), ex. 4\n",
    "            \n",
    "            NNB = NNBuilder()\n",
    "            model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 4], inshape=(10, 4), outshape=4, batchNormalization=None)\n",
    "            model.summary()\n",
    "            _________________________________________________________________\n",
    "            Model: \"DNNLSTM\"\n",
    "            _________________________________________________________________\n",
    "            Layer (type)                 Output Shape              Param #   \n",
    "            =================================================================\n",
    "            input (InputLayer)           [(None, 10, 4)]           0         \n",
    "            _________________________________________________________________\n",
    "            TDense_1 (TimeDistributed)   (None, 10, 10)            50         (4 * 10 + 10)\n",
    "            _________________________________________________________________\n",
    "            Activation_1 (TimeDistribute (None, 10, 10)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_2 (TimeDistributed)   (None, 10, 20)            220        (10 * 20 + 20)  \n",
    "            _________________________________________________________________\n",
    "            Activation_2 (TimeDistribute (None, 10, 20)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_3 (TimeDistributed)   (None, 10, 30)            630        (20 * 30 + 30)    \n",
    "            _________________________________________________________________\n",
    "            Activation_3 (TimeDistribute (None, 10, 30)            0         \n",
    "            _________________________________________________________________  ↓ (input gate, forget gate, output gate and neuron) \n",
    "            LSTM (LSTM)                  (None, 4)                 560        (4 * (30 * 4 + 4 + 4 * 4))\n",
    "            _________________________________________________________________                    ↑ (cell state pass to the other cells) \n",
    "            output (Dense)               (None, 4)                 20         (4 * 4 + 4)       \n",
    "            =================================================================\n",
    "            Total params: 1,480\n",
    "            Trainable params: 1,480\n",
    "            Non-trainable params: 0\n",
    "            \n",
    "            (4 * 10 + 10) + (10 * 20 + 20) + (20 * 30 + 30) + 4 * (30 * 4 + 4 + 4 * 4) + (4 * 4 + 4)   \n",
    "        '''\n",
    "        \n",
    "        self.name = \"DNNLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        assert len(units) >= 2\n",
    "        \n",
    "        inputs = Input(inshape, name=\"input\")  \n",
    "\n",
    "        nlayer = len(units) - 1\n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "        else:\n",
    "            activations = [\"relu\" for _ in range(nlayer)]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = TimeDistributed(BatchNormalization(), name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = TimeDistributed(Activation(activations[i]), name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = TimeDistributed(Dropout(dropouts[i]), name=\"Dropout_{}\".format(i + 1))(x)\n",
    "        x = LSTM(units[-1], name=\"LSTM\")(x)\n",
    "        \n",
    "        \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"DNNLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"DNNLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    \n",
    "    def TPALSTM(self, optimizer=None, callbacks=None):\n",
    "        \n",
    "        self.name = \"TPALSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        embedding_layer = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "        sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        merged = concatenate([x1, y1])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        ########################################\n",
    "        ## add class weight\n",
    "        ########################################\n",
    "        if re_weight:\n",
    "            class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "        else:\n",
    "            class_weight = None\n",
    "\n",
    "        ########################################\n",
    "        ## train the model\n",
    "        ########################################\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "                outputs=preds)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer='nadam',\n",
    "                metrics=['acc'])\n",
    "        #model.summary()\n",
    "        print(STAMP)\n",
    "\n",
    "        early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "        bst_model_path = STAMP + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "        hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "                validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "                epochs=200, batch_size=2048, shuffle=True, \\\n",
    "                class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "        model.load_weights(bst_model_path)\n",
    "        bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "        ########################################\n",
    "        ## make the submission\n",
    "        ########################################\n",
    "        print('Start making the submission before fine-tuning')\n",
    "\n",
    "        preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "        preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "        preds /= 2\n",
    "\n",
    "        submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "        submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def DenseBuilder(units, inputs, batchNormalization=True, dropouts=None, activations=None, optimizer=None, callbacks=None):\n",
    "        \n",
    "        _args       = NNBuilder._argreset(units, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "    \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _argreset(units, dropouts=None, activations=None):\n",
    "        \n",
    "        _args = dict()\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(units, list):\n",
    "            nlayer = len(units)\n",
    "        else:\n",
    "            units = [units]\n",
    "        \n",
    "        _args[\"units\"] = units\n",
    "        _args[\"nlayer\"] = nlayer\n",
    "        \n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "            _args[\"dropouts\"] = dropouts\n",
    "        else:\n",
    "            _args[\"dropouts\"] = None\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "            _args[\"activations\"] = activations\n",
    "        else:\n",
    "            _args[\"activations\"] = None\n",
    "#             activations = [\"relu\" for _ in range(nlayer)]\n",
    "            \n",
    "        return _args\n",
    "\n",
    "    \n",
    "    def _callbacks(self, mmonitor=\"val_loss\", emonitor=\"loss\", lmonitor=\"val_loss\"):\n",
    "\n",
    "#     def _callbacks(modeld, ckptd, mmonitor=\"val_loss\", emonitor=\"loss\", lmonitor=\"val_loss\", name=\"ckpt\"):\n",
    "        \n",
    "        '''\n",
    "            mmonitor: monitor for model \n",
    "            emonitor: monitor for earlystopping\n",
    "            lmonitor: monitor for learning rate\n",
    "        '''\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "        \n",
    "        _name = \"{epoch:04d}_{loss:.3f}_{val_loss:.3f}\"\n",
    "#         checkpointer = ModelCheckpoint(filepath=os.path.join(modeld, \"{0}_{1}_{2}.hdf5\".format(name, _name, timestamp)),\n",
    "        checkpointer = ModelCheckpoint(filepath=os.path.join(self.modeld, \"{0}.hdf5\".format(self.name)),\n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       monitor=mmonitor)\n",
    "        \n",
    "        earlystopper = EarlyStopping(monitor=emonitor, patience=10)\n",
    "\n",
    "        reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.5, patience=10, min_lr=0.0001)\n",
    "\n",
    "#         reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.9, patience=10, min_lr=0.0001)\n",
    "        \n",
    "        tb = TensorBoard(log_dir=self.ckptd)\n",
    "\n",
    "        csvlogger = CSVLogger(os.path.join(self.ckptd, \"{}_{}.log\".format(self.name, timestamp)), append=False, separator=\",\")\n",
    "\n",
    "        # Learning rate schedule.\n",
    "    #     lr_schedule = LearningRateScheduler(fixed_schedule, verbose=0)\n",
    "\n",
    "        return [checkpointer, earlystopper, reduceLR, tb, csvlogger]\n",
    "\n",
    "#         return [checkpointer, earlystopper, reduceLR, csvlogger]\n",
    "    \n",
    "    def _optimizer(self, lr=1e-3, name=\"Adam\"):\n",
    "#     def _optimizer(lr=1e-2, name=\"Adam\"):\n",
    "        if name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "        else:\n",
    "            optimizer = Adam(learning_rate=lr)\n",
    "        return optimizer\n",
    "    \n",
    "    @staticmethod\n",
    "    def mloader(filepath, custom_objects=None):\n",
    "        if custom_objects is not None:\n",
    "            return load_model(filepath, custom_objects=custom_objects)\n",
    "        else:\n",
    "            return load_model(filepath)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T05:57:01.259841Z",
     "iopub.status.busy": "2021-03-10T05:57:01.259623Z",
     "iopub.status.idle": "2021-03-10T05:57:01.287817Z",
     "shell.execute_reply": "2021-03-10T05:57:01.286822Z",
     "shell.execute_reply.started": "2021-03-10T05:57:01.259814Z"
    }
   },
   "outputs": [],
   "source": [
    "class MArgs(object):\n",
    "    def __init__(self, mname, testf=None, train=None):\n",
    "        self.mname = mname\n",
    "        self.testf = testf\n",
    "        self.train = train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T06:21:20.423796Z",
     "iopub.status.busy": "2021-03-26T06:21:20.334661Z",
     "iopub.status.idle": "2021-03-26T06:22:46.038601Z",
     "shell.execute_reply": "2021-03-26T06:22:46.037974Z",
     "shell.execute_reply.started": "2021-03-26T06:21:20.398475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "****** epochs = 100, batch_size = 100, nsize = 10000, nstep = 6, nfeature = 5, ntarget = 4, nclass = 5\n",
      "****** X_train.shape = (10000, 6, 5), y_train[0].shape = (10000, 4), y_train[1].shape = (10000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:21:22,926, bidirectionalLSTM-103-INFO: 01, x.shape = (None, 6, 20)\n",
      "2021-03-26 14:21:23,353, bidirectionalLSTM-103-INFO: 02, x.shape = (None, 6, 40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f24f1275980>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:21:25,995, converted_call-603-INFO: features = 40\n",
      "2021-03-26 14:21:27,227, warn-146-WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f24f1275980>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f24f1275980>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:21:27,807, converted_call-603-INFO: timesteps = 21\n",
      "2021-03-26 14:21:27,819, converted_call-603-INFO: x.shape (batchsize, m, k) = (None, 40, 20)\n",
      "2021-03-26 14:21:27,832, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-03-26 14:21:27,844, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (None, 40)\n",
      "2021-03-26 14:21:27,863, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (None, 40, 1)\n",
      "2021-03-26 14:21:27,875, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (None, 40, 1)\n",
      "2021-03-26 14:21:27,889, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (None, 40, 1)\n",
      "2021-03-26 14:21:27,901, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (None, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bidirectionalLSTM\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 6, 5)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BLSTM_1 (Bidirectional)         (None, 6, 20)        1280        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BLSTM_2 (Bidirectional)         (None, 6, 40)        6560        BLSTM_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "H (Lambda)                      (None, 5, 40)        0           BLSTM_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "TPCNN1D (TPCNN1D)               (None, 40, 20)       120         H[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "h_t (Lambda)                    (None, 40)           0           BLSTM_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "TPAttention (TPAttention)       (None, 40)           3200        TPCNN1D[0][0]                    \n",
      "                                                                 h_t[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "LDense1 (Dense)                 (None, 20)           820         TPAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LDense2 (Dense)                 (None, 20)           820         TPAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Loss1 (Dense)                   (None, 4)            84          LDense1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Loss2 (Dense)                   (None, 5)            105         LDense2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,989\n",
      "Trainable params: 12,989\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "      0  1  2  3  4\n",
      "0     0  0  0  0  1\n",
      "1     0  0  0  0  1\n",
      "2     0  0  0  1  0\n",
      "3     0  0  0  1  0\n",
      "4     1  0  0  0  0\n",
      "...  .. .. .. .. ..\n",
      "9995  1  0  0  0  0\n",
      "9996  0  0  0  0  1\n",
      "9997  0  0  1  0  0\n",
      "9998  0  0  0  0  1\n",
      "9999  0  0  0  0  1\n",
      "\n",
      "[10000 rows x 5 columns]\n",
      "(10000, 6, 5) (10000, 4) (10000, 5)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:21:29,660, converted_call-603-INFO: features = 40\n",
      "2021-03-26 14:21:29,675, converted_call-603-INFO: timesteps = 21\n",
      "2021-03-26 14:21:29,689, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2021-03-26 14:21:29,703, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-03-26 14:21:29,717, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2021-03-26 14:21:29,739, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:29,754, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:29,770, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:29,784, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n",
      "2021-03-26 14:21:31,729, converted_call-603-INFO: features = 40\n",
      "2021-03-26 14:21:31,743, converted_call-603-INFO: timesteps = 21\n",
      "2021-03-26 14:21:31,758, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2021-03-26 14:21:31,773, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-03-26 14:21:31,787, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2021-03-26 14:21:31,808, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:31,821, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:31,836, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:31,850, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/70 [..............................] - ETA: 0s - loss: 1.8541 - Loss1_loss: 0.2436 - Loss2_loss: 1.6105WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:21:39,198, new_func-323-WARNING: From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/70 [..............................] - ETA: 3s - loss: 1.8634 - Loss1_loss: 0.2527 - Loss2_loss: 1.6106WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_end` time: 0.0903s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:21:39,286, _call_batch_end_hook-328-WARNING: Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0104s vs `on_train_batch_end` time: 0.0903s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/70 [==========================>...] - ETA: 0s - loss: 1.8610 - Loss1_loss: 0.2499 - Loss2_loss: 1.6112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-26 14:21:41,116, converted_call-603-INFO: features = 40\n",
      "2021-03-26 14:21:41,130, converted_call-603-INFO: timesteps = 21\n",
      "2021-03-26 14:21:41,145, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2021-03-26 14:21:41,159, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-03-26 14:21:41,173, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2021-03-26 14:21:41,193, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:41,208, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:41,224, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-03-26 14:21:41,239, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 3s 41ms/step - loss: 1.8610 - Loss1_loss: 0.2500 - Loss2_loss: 1.6110 - val_loss: 1.8569 - val_Loss1_loss: 0.2473 - val_Loss2_loss: 1.6096\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 1.8596 - Loss1_loss: 0.2498 - Loss2_loss: 1.6098 - val_loss: 1.8566 - val_Loss1_loss: 0.2473 - val_Loss2_loss: 1.6093\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8592 - Loss1_loss: 0.2498 - Loss2_loss: 1.6094 - val_loss: 1.8568 - val_Loss1_loss: 0.2475 - val_Loss2_loss: 1.6094\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8591 - Loss1_loss: 0.2498 - Loss2_loss: 1.6094 - val_loss: 1.8568 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6094\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8591 - Loss1_loss: 0.2498 - Loss2_loss: 1.6093 - val_loss: 1.8569 - val_Loss1_loss: 0.2473 - val_Loss2_loss: 1.6096\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8591 - Loss1_loss: 0.2498 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8591 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8591 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8591 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8602 - Loss1_loss: 0.2497 - Loss2_loss: 1.6105 - val_loss: 1.8573 - val_Loss1_loss: 0.2475 - val_Loss2_loss: 1.6098\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8605 - Loss1_loss: 0.2499 - Loss2_loss: 1.6106 - val_loss: 1.8570 - val_Loss1_loss: 0.2475 - val_Loss2_loss: 1.6095\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8591 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8569 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6095\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6095\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8591 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6095\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8570 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6096\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8590 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2497 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8572 - val_Loss1_loss: 0.2475 - val_Loss2_loss: 1.6097\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8572 - val_Loss1_loss: 0.2475 - val_Loss2_loss: 1.6097\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2475 - val_Loss2_loss: 1.6097\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2496 - Loss2_loss: 1.6093 - val_loss: 1.8571 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6097\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2496 - Loss2_loss: 1.6092 - val_loss: 1.8572 - val_Loss1_loss: 0.2474 - val_Loss2_loss: 1.6098\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    gpuId = 1\n",
    "    mname = \"bidirectionalLSTM\"\n",
    "#     mname = \"stackedLSTM\"\n",
    "\n",
    "    # epochs = {}, batch_size = {}, nsize = {}, nstep = {}, nfeature = {}, ntarget = {}, nclass = {}\"\n",
    "    args = MArgs(mname, train=[100, 100, 10000, 6, 5, 4, 5])\n",
    "    \n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(gpuId)  # only device you want to use can visible\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(gpus)\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], enable=True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    if args.testf == \"_argreset\":\n",
    "        _args = NNBuilder._argreset([10, 20, 30], dropouts=0.5, activations=\"relu\")\n",
    "        print(_args)\n",
    "    \n",
    "    \n",
    "    if args.train is not None:\n",
    "        epochs, batch_size, nsize, nstep, nfeature, ntarget, nclass = args.train\n",
    "        X_train = np.random.random_sample((nsize, nstep, nfeature))\n",
    "        y_train = [np.random.random_sample((nsize, ntarget)), pd.get_dummies(pd.Series(np.random.randint(low=0, high=nclass, size=nsize)))]\n",
    "        print(\"****** epochs = {}, batch_size = {}, nsize = {}, nstep = {}, nfeature = {}, ntarget = {}, nclass = {}\".format(epochs, batch_size, nsize, nstep, nfeature, ntarget, nclass))\n",
    "        print(\"****** X_train.shape = {}, y_train[0].shape = {}, y_train[1].shape = {}\".format(X_train.shape, y_train[0].shape, y_train[1].shape))\n",
    "        \n",
    "    \n",
    "    if args.mname == \"DNNbuilder\":\n",
    "        inputs = Input(shape=(10), name=\"Input\")\n",
    "        x = NNBuilder.DenseBuilder([10, 30, 20, 40], inputs, dropouts=0.25, activations=\"relu\")\n",
    "        model = Model(inputs=inputs, outputs=x, name=\"DNN\")\n",
    "        model.summary()\n",
    "\n",
    "    elif args.mname == \"DNNLSTM\":\n",
    "        NNB = NNBuilder()\n",
    "        model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 10], inshape=(6, 4), outshape=[4, 1], outactfn=[\"tanh\", \"sigmoid\"], batchNormalization=None)\n",
    "        model.summary()\n",
    "        plot_model(model, to_file=\"DNNLSTM.png\", show_shapes=True)\n",
    "\n",
    "        model.compile(loss={\"regression_output\": \"mae\", \"classification_output\": \"binary_crossentropy\"},\n",
    "                      metrics={\"regression_output\": \"mae\", \"classification_output\": \"accuracy\"},\n",
    "                      optimizer=optimizer_)\n",
    "\n",
    "        epochs = 1000\n",
    "        batch_size = 10\n",
    "        n = 10000\n",
    "        X_train = np.random.random_sample((n, 6, 4))\n",
    "        y_train = [np.random.random_sample((n, 4)), np.random.randint(low=0, high=2, size=(n, 1))]\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "    elif args.mname == \"stackedLSTM\":\n",
    "        stackedLSTM, callbacks_, optimizer_ = NNBuilder().stackedLSTM(cells=[10, 20], inshape=[6, 4], outshape=[4, 4], outactfn=[\"sigmoid\", \"softmax\"])\n",
    "        stackedLSTM.compile(loss=\"mae\", optimizer=optimizer_)\n",
    "        stackedLSTM.summary()\n",
    "\n",
    "        epochs = 3\n",
    "        batch_size = 200\n",
    "        X_train = np.random.random_sample((1000, 6, 4))\n",
    "        y_train = [np.random.random_sample((1000, 4)), np.random.randint(low=0, high=2, size=1000)]\n",
    "\n",
    "\n",
    "    #     history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "    #     saved_model = \"/home/yuzhe/DataScience/QC/model/lstm1_0154_0.009_0.008_202008071814.hdf5\"\n",
    "    #     model = NNBuilder.mloader(saved_model)\n",
    "\n",
    "    elif args.mname == \"CNN1D\":\n",
    "        CNN1D, callbacks_, optimizer_ = NNBuilder().CNN1D(filters=[10, 20], inshape=[6, 4], outshape=[2], outactfn=[\"sigmoid\"], activations=\"relu\")\n",
    "        CNN1D.summary()\n",
    "\n",
    "    #     train(X_train, y_train, 30, 5000, loss=YZKError(element_weight=[1 / 6., 1 / 6., 1 / 6., 1 / 2.]), name=\"NNBuilderTest1\")\n",
    "    #     train(X_train, y_train, 30, 5000, loss=YZKError(), name=\"NNBuilderTest\")\n",
    "\n",
    "    elif args.mname == \"bidirectionalLSTM\":\n",
    "        bLSTM, callbacks_, optimizer_ = NNBuilder().bidirectionalLSTM(merge_mode=\"concat\", cells=[10, 20], inshape=[nstep, nfeature], outshape=[ntarget, nclass], outactfn=[\"sigmoid\", \"softmax\"])\n",
    "\n",
    "        bLSTM.compile(loss={\"Loss1\": \"mae\", \"Loss2\": \"categorical_crossentropy\"}, optimizer=optimizer_)\n",
    "        print(y_train[1])\n",
    "        print(X_train.shape, y_train[0].shape, y_train[1].shape)\n",
    "        history = bLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.3, verbose=1, shuffle=True)\n",
    "\n",
    "        \n",
    "#         (None, 4, 20) mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:29:31.870226Z",
     "iopub.status.busy": "2021-02-25T07:29:31.869997Z",
     "iopub.status.idle": "2021-02-25T07:29:31.873967Z",
     "shell.execute_reply": "2021-02-25T07:29:31.873393Z",
     "shell.execute_reply.started": "2021-02-25T07:29:31.870198Z"
    }
   },
   "outputs": [],
   "source": [
    "def logcosh(a, t):\n",
    "    return (1 / a) * np.log(np.cosh(a * t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:29:31.875019Z",
     "iopub.status.busy": "2021-02-25T07:29:31.874830Z",
     "iopub.status.idle": "2021-02-25T07:29:32.510968Z",
     "shell.execute_reply": "2021-02-25T07:29:32.510167Z",
     "shell.execute_reply.started": "2021-02-25T07:29:31.874996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:29:32,216, hrfgenerator-367-INFO: hrfgenerator-vnames-199: ['Temp', 'RH', 'Pres', 'Precp']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temp     RH    Pres  Precp\n",
      "0 -20.0    0.0   600.0    0.0\n",
      "1  50.0  100.0  1100.0  220.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yuzhe/DataScience/dataset/hrf_2016010101_2016123124_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b712c943eefe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#              \"Pres\": [600.0, 1100.0],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#              \"Precp\": [0.0, 220.0]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhrfgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtperiod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfnpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdatetimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DataScience/lib/dgenerator.py\u001b[0m in \u001b[0;36mhrfgenerator\u001b[0;34m(self, tperiod, n_in, n_out, t2last, mode, fnpy, rescale, reformat, vstack, dropnan, classify, generator, batchsize)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mdarray\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/hrf_{}_{}_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpyd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mqcdtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/qcdtime_{}_{}_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpyd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mstnids_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/stnid_{}_{}_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpyd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yuzhe/DataScience/dataset/hrf_2016010101_2016123124_test.npy'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "#### check losses     \n",
    "    \n",
    "    from dgenerator import dgenerator\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    tperiod = [2016010101, 2016123124]\n",
    "    n_in = 6\n",
    "    n_out = 1\n",
    "    mode = \"test\"\n",
    "    vstack = True\n",
    "    fnpy = True\n",
    "    npyd = \"/home/yuzhe/DataScience/dataset\"\n",
    "    gif = \"/home/yuzhe/CODE/ProgramT1/GRDTools/SRC/RES/GI/1500_decode_stationlist_without_space.txt\"\n",
    "\n",
    "    dg = dgenerator(gif=gif, npyd=npyd)\n",
    "    vinfo = pd.DataFrame(dg.vrange)  \n",
    "    vinfo = pd.DataFrame(vinfo)\n",
    "    print(vinfo)\n",
    "#     vinfo = {\"Temp\": [-20.0, 50.0],\n",
    "#              \"RH\": [0.0, 100.0], \n",
    "#              \"Pres\": [600.0, 1100.0], \n",
    "#              \"Precp\": [0.0, 220.0]}\n",
    "    dataset = dg.hrfgenerator(tperiod, n_in=n_in, n_out=n_out, mode=mode, rescale=True, reformat=True, vstack=vstack, fnpy=fnpy, generator=False)\n",
    "    \n",
    "    datetimes = dataset[1]\n",
    "    nsize = len(datetimes)\n",
    "    print(dataset[0].shape)\n",
    "\n",
    "    \n",
    "    saved_model = \"../QC/model/lstm1_0055_0.008_0.011_202008111819_2.hdf5\"\n",
    "    model = NNBuilder.mloader(saved_model)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(vinfo.values)\n",
    "    \n",
    "    print(scaler.inverse_transform([[0.7, 0.6, 0.7, 0.2]]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    n = nsize\n",
    "    \n",
    "#     x = dataset[0][:, -4:]\n",
    "#     x = x[~np.isnan(x).any(axis=1)]\n",
    "#     idx = np.random.choice(np.arange(x.shape[0]), n, replace=False)\n",
    "#     x = x[idx, 0:]\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "\n",
    "    scaled = dataset[0]\n",
    "    scaled = scaled[~np.isnan(scaled).any(axis=1)]\n",
    "    X_test = np.reshape(scaled[:, :-4], (-1, 6, 4))\n",
    "    y_true = scaled[:, -4:]\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "#     xynorm = tf.norm(tf.subtract(y_pred, y_true), axis=1) \n",
    "\n",
    "    y_true = tf.reshape(y_true[:, -1], [-1, 1])\n",
    "    y_pred = tf.reshape(y_pred[:, -1], [-1, 1])\n",
    "    xynorm = tf.subtract(y_pred, y_true)\n",
    "\n",
    "    print(y_true.shape, y_true)\n",
    "    print(y_pred.shape, y_pred)\n",
    "    print(xynorm.shape)\n",
    "    \n",
    "    sample_weight = 1\n",
    "#     sample_weight = tf.broadcast_to(sample_weight, y_pred.shape)\n",
    "    \n",
    "    loss = YZKError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"YZK\")\n",
    "    print('yzk-loss: ', loss)\n",
    "    \n",
    "    loss = tf.losses.LogCosh(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "\n",
    "    print(\"shape of loss = {}, xynorm = {}\".format(loss, xynorm.shape))\n",
    "    ax.scatter(xynorm, loss, label=\"LogCosh\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MAE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Cos\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "\n",
    "#     loss = tf.keras.losses.KLDivergence(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "#     ax.scatter(xynorm, loss, label=\"KL\")\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"MSE\")\n",
    "\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MSLE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "    \n",
    "    loss = tf.keras.losses.Huber(reduction=tf.losses.Reduction.NONE, delta=0.25)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Huber\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-02-25T07:29:32.511697Z",
     "iopub.status.idle": "2021-02-25T07:29:32.511970Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample_weights(training_data, class_weight_dictionary): \n",
    "    sample_weights = [class_weight_dictionary[np.where(one_hot_row==1)[0][0]] for one_hot_row in training_data]\n",
    "    return np.asarray(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = Sequential()\n",
    "left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(99, 13)))\n",
    "right = Sequential()\n",
    "right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', input_shape=(99, 13), go_backwards=True))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='sum'))\n",
    "\n",
    "model.add(TimeDistributedDense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "print(\"Train...\")\n",
    "model.fit([X_train, X_train], Y_train, batch_size=1, nb_epoch=nb_epoches, validation_data=([X_test, X_test], Y_test), verbose=1, show_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T07:27:01.849928Z",
     "iopub.status.busy": "2021-02-26T07:27:01.849565Z",
     "iopub.status.idle": "2021-02-26T07:27:01.853180Z",
     "shell.execute_reply": "2021-02-26T07:27:01.852424Z",
     "shell.execute_reply.started": "2021-02-26T07:27:01.849891Z"
    }
   },
   "source": [
    "# number of parameters for LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T07:30:09.767055Z",
     "iopub.status.busy": "2021-02-26T07:30:09.766765Z",
     "iopub.status.idle": "2021-02-26T07:30:09.770535Z",
     "shell.execute_reply": "2021-02-26T07:30:09.769734Z",
     "shell.execute_reply.started": "2021-02-26T07:30:09.767026Z"
    }
   },
   "outputs": [],
   "source": [
    "#                                                                    ↓ 每個gate及current cell state的bias\n",
    "# 4 * (輸入資料的個數 * 這層cell的個數 + 這層cell的個數 * 這層cell的個數 + 這層cell的個數)\n",
    "# ↑     ↑ input或前層的輸出             ↑ 因為hidden state會傳給同層的其他cell\n",
    "# input gate, output gate and forget gate and weights of current cell state, 共4個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T06:34:30.030838Z",
     "iopub.status.busy": "2021-02-26T06:34:30.030468Z",
     "iopub.status.idle": "2021-02-26T06:34:30.035992Z",
     "shell.execute_reply": "2021-02-26T06:34:30.035222Z",
     "shell.execute_reply.started": "2021-02-26T06:34:30.030796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 * (10 * 4 + 4 * 4 + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T06:34:51.671811Z",
     "iopub.status.busy": "2021-02-26T06:34:51.671443Z",
     "iopub.status.idle": "2021-02-26T06:34:51.979308Z",
     "shell.execute_reply": "2021-02-26T06:34:51.978514Z",
     "shell.execute_reply.started": "2021-02-26T06:34:51.671776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 5, 10)]           0         \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 5, 4)              240       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 5, 4)              0         \n",
      "=================================================================\n",
      "Total params: 240\n",
      "Trainable params: 240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input((5, 10))\n",
    "# x = Bidirectional(LSTM(4, return_sequences=True))(inputs)\n",
    "x = LSTM(4, return_sequences=True)(inputs)\n",
    "\n",
    "outputs = Activation('softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T06:23:15.786726Z",
     "iopub.status.busy": "2021-02-26T06:23:15.786474Z",
     "iopub.status.idle": "2021-02-26T06:23:16.147741Z",
     "shell.execute_reply": "2021-02-26T06:23:16.147117Z",
     "shell.execute_reply.started": "2021-02-26T06:23:15.786688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 5, 10)]           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 5, 20)             1680      \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 20)                2480      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 105       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 4,265\n",
      "Trainable params: 4,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "x = Bidirectional(LSTM(10))(x)\n",
    "x = Dense(5)(x)\n",
    "outputs = Activation('softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T06:19:11.249167Z",
     "iopub.status.busy": "2021-02-26T06:19:11.248906Z",
     "iopub.status.idle": "2021-02-26T06:19:11.252688Z",
     "shell.execute_reply": "2021-02-26T06:19:11.252103Z",
     "shell.execute_reply.started": "2021-02-26T06:19:11.249139Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.random_sample((1, 5, 10))\n",
    "y = np.random.random_sample((1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T06:19:31.207687Z",
     "iopub.status.busy": "2021-02-26T06:19:31.207265Z",
     "iopub.status.idle": "2021-02-26T06:19:36.750057Z",
     "shell.execute_reply": "2021-02-26T06:19:36.749410Z",
     "shell.execute_reply.started": "2021-02-26T06:19:31.207640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fec3028e050>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T06:42:46.993240Z",
     "iopub.status.busy": "2021-03-26T06:42:46.992923Z",
     "iopub.status.idle": "2021-03-26T06:42:46.997152Z",
     "shell.execute_reply": "2021-03-26T06:42:46.996417Z",
     "shell.execute_reply.started": "2021-03-26T06:42:46.993209Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ConvLSTM2D, Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T06:48:42.129187Z",
     "iopub.status.busy": "2021-03-26T06:48:42.128758Z",
     "iopub.status.idle": "2021-03-26T06:48:43.084649Z",
     "shell.execute_reply": "2021-03-26T06:48:43.083525Z",
     "shell.execute_reply.started": "2021-03-26T06:48:42.129128Z"
    }
   },
   "outputs": [],
   "source": [
    "seq = Sequential(\n",
    "    [\n",
    "        Input(\n",
    "            shape=(None, 40, 40, 1), name='input',\n",
    "        ),  # Variable-length sequence of 40x40x1 frames\n",
    "        ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"ConvLSTM2D_1\", data_format=\"channels_last\",\n",
    "        ),\n",
    "        BatchNormalization(name=\"BN_1\"),\n",
    "        ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"ConvLSTM2D_2\",\n",
    "        ),\n",
    "        BatchNormalization(name=\"BN_2\"),\n",
    "        ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"ConvLSTM2D_3\",\n",
    "        ),\n",
    "        BatchNormalization(name=\"BN_3\"),\n",
    "        ConvLSTM2D(\n",
    "            filters=40, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"ConvLSTM2D_4\",\n",
    "        ),\n",
    "        BatchNormalization(name=\"BN_4\"),\n",
    "        Conv3D(\n",
    "            filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "seq.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-26T06:48:43.086945Z",
     "iopub.status.busy": "2021-03-26T06:48:43.086587Z",
     "iopub.status.idle": "2021-03-26T06:48:43.098750Z",
     "shell.execute_reply": "2021-03-26T06:48:43.097730Z",
     "shell.execute_reply.started": "2021-03-26T06:48:43.086908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "ConvLSTM2D_1 (ConvLSTM2D)    (None, None, 40, 40, 40)  59200     \n",
      "_________________________________________________________________\n",
      "BN_1 (BatchNormalization)    (None, None, 40, 40, 40)  160       \n",
      "_________________________________________________________________\n",
      "ConvLSTM2D_2 (ConvLSTM2D)    (None, None, 40, 40, 40)  115360    \n",
      "_________________________________________________________________\n",
      "BN_2 (BatchNormalization)    (None, None, 40, 40, 40)  160       \n",
      "_________________________________________________________________\n",
      "ConvLSTM2D_3 (ConvLSTM2D)    (None, None, 40, 40, 40)  115360    \n",
      "_________________________________________________________________\n",
      "BN_3 (BatchNormalization)    (None, None, 40, 40, 40)  160       \n",
      "_________________________________________________________________\n",
      "ConvLSTM2D_4 (ConvLSTM2D)    (None, None, 40, 40, 40)  115360    \n",
      "_________________________________________________________________\n",
      "BN_4 (BatchNormalization)    (None, None, 40, 40, 40)  160       \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, None, 40, 40, 1)   1081      \n",
      "=================================================================\n",
      "Total params: 407,001\n",
      "Trainable params: 406,681\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
