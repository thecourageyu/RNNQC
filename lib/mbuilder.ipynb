{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- performance evaluation\n",
    "- date: 2020-08-07\n",
    "- maintainer: YZK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to script mbuilder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import deque, Counter\n",
    "from fbprophet import Prophet\n",
    "from functools import partial\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential, Model, losses\n",
    "from tensorflow.keras import constraints, initializers, regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Lambda, Layer, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Conv1D, Conv2D, Dense, Dropout, Flatten, Input, LSTM, MaxPool1D, MaxPool2D\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "# from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# from tensorflow.keras import losses\n",
    "\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import msetup\n",
    "    msetup.setLogging(loglv=logging.DEBUG)\n",
    "\n",
    "# @keras_export('keras.losses.CosineSimilarityCB')\n",
    "\n",
    "\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Layer, Dense, Input, LSTM\n",
    "# from keras.optimizers import SGD\n",
    "# from keras import initializers, regularizers, constraints\n",
    "# from keras.callbacks import CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmbuilder(units, input_shape, loss, optimizer):\n",
    "    '''\n",
    "        input_shape: a tuple (timesteps, nfeatures)\n",
    "    '''\n",
    "    \n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units, input_shape=input_shape))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.compile(loss=loss, optimizer=optimizer)\n",
    "             \n",
    "    return lstm\n",
    "\n",
    "\n",
    "# lstmbuilder(10, (10, 3), 'mae', SGD()).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking. \n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \n",
    "        tensorflow & keras reference:\n",
    "            https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "            https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "            https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
    "            \n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)  # inherit Layer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3, [batchsize, timestep, # of feature]\n",
    "        '''\n",
    "        \n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "#         self.step_dim = input_shape[-2]\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "        '''\n",
    "        \n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        l2 = np.sum(x**2)\n",
    "    else:\n",
    "        l2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=x)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def cosine_similarity(y_true, y_pred, axis=1):\n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "#     l2norm1 = tf.reduce_sum(tf.square(y_true), axis=axis)\n",
    "#     l2norm2 = tf.reduce_sum(tf.square(y_pred), axis=axis)\n",
    "    l2norm1 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_true)\n",
    "    l2norm2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_pred)\n",
    "    yy = tf.reduce_sum(tf.multiply(y_true, y_pred), axis=axis)\n",
    "        \n",
    "    return -tf.divide(yy, tf.multiply(tf.sqrt(l2norm1), tf.sqrt(l2norm2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YZKError(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None, \n",
    "                 penalized=None):\n",
    "    \n",
    "        \"\"\" Initializes `YZKError` instance.\n",
    "            Args:\n",
    "              reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "                loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "                option will be determined by the usage context. For almost all cases\n",
    "                this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
    "                `tf.distribute.Strategy`, outside of built-in training loops such as\n",
    "                `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n",
    "                will raise an error. Please see this custom training [tutorial](\n",
    "                  https://www.tensorflow.org/tutorials/distribute/custom_training)\n",
    "                for more details.\n",
    "              name: Optional name for the op. Defaults to 'mean_squared_error'.\n",
    "        \"\"\"\n",
    "\n",
    "        super(YZKError, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "        self.penalized = penalized\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        element_weight = self.element_weight\n",
    "\n",
    "#         logging.info(self.reduction, element_weight)\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        batchsize = y_pred.shape[0] \n",
    "        assert batchsize == y_true.shape[0]\n",
    "        \n",
    "#         logging.info(\"YZKError, shape of y_true = {}, y_pred = {}\".format(y_true.shape, y_pred.shape))\n",
    "        \n",
    "        PLoss = 0.    \n",
    "        penalized = self.penalized\n",
    "        if penalized is not None:\n",
    "            \n",
    "            y_pred_ = tf.convert_to_tensor(y_pred[:, penalized])\n",
    "            y_true_ = tf.convert_to_tensor(y_true[:, penalized])\n",
    "            \n",
    "            y_pred_ = tf.reshape(y_pred_, [-1, 1])\n",
    "            y_true_ = tf.reshape(y_true_, [-1, 1])\n",
    "#                 PLoss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true[:, penalized], y_pred[:, penalized])\n",
    "            PLoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(y_true_, y_pred_)\n",
    "        \n",
    "#         HuberLoss = 0\n",
    "#         MAELoss = 0\n",
    "#         if element_weight is not None:\n",
    "#             element_weight = tf.convert_to_tensor(element_weight)\n",
    "#             if element_weight.shape != []:  # is not a scale\n",
    "#                 nelement = y_pred.shape[1]\n",
    "#                 assert nelement == y_true.shape[1] \n",
    "#                 element_weight = tf.broadcast_to(element_weight, [batchsize, nelement])\n",
    "                \n",
    "#             y_pred_ = tf.math.multiply(y_pred, element_weight)\n",
    "#             y_true_ = tf.math.multiply(y_true, element_weight)\n",
    "        \n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true_, y_pred_)\n",
    "#         else:\n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true, y_pred)\n",
    "        \n",
    "        MAELoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "\n",
    "    #         CosSimLoss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "        CosSimLoss = tf.losses.CosineSimilarity(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "#         logging.info(\"YZKError, PLoss: {}\\n, HuberLoss: {}\\n, MAELoss: {}\\n, CosSimLoss: {}\\n\".format(PLoss, HuberLoss, MAELoss, CosSimLoss))\n",
    "        \n",
    "        if penalized is not None:\n",
    "            return tf.math.add(tf.math.add(tf.math.scalar_mul(3, PLoss), tf.math.scalar_mul(2, MAELoss)), tf.math.scalar_mul(1, CosSimLoss))\n",
    "        else:\n",
    "            return tf.math.add(tf.math.scalar_mul(2, MAELoss), tf.math.scalar_mul(1, CosSimLoss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inverse transform sigmoid(x)\n",
    "<center>\n",
    "<font size=4>\n",
    "$\n",
    "\\begin{align}\n",
    "\\sigma &=\\frac{1}{1+e^{-x}} \\\\\n",
    "\\frac{1}{\\sigma} &= 1+e^{-x} \\\\\n",
    "\\frac{1}{\\sigma}-1 &= e^{-x} \\\\\n",
    "\\log{\\frac{1-\\sigma}{\\sigma}} &= \\log{e^{-x}} \\\\\n",
    "x &= -\\log{\\frac{1-\\sigma}{\\sigma}} \\\\\n",
    "x &= \\log{\\frac{\\sigma}{1-\\sigma}}\n",
    "\\end{align}\n",
    "$\n",
    "</font size>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Crossentropy\n",
    "\n",
    "<center><font size=4>$BCE=y_{true}\\cdot\\log{y_{pred}}+\\left(1-y_{true}\\right)\\cdot\\log{\\left(1-y_{pred}\\right)}$</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBinaryCrossntropy(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None):\n",
    "    \n",
    "        \"\"\" Initializes `YZKError` instance.\n",
    "            Args:\n",
    "              reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "                loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "                option will be determined by the usage context. For almost all cases\n",
    "                this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
    "                `tf.distribute.Strategy`, outside of built-in training loops such as\n",
    "                `tf.keras` `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n",
    "                will raise an error. Please see this custom training [tutorial](\n",
    "                  https://www.tensorflow.org/tutorials/distribute/custom_training)\n",
    "                for more details.\n",
    "               element_weight: [w for y_true=1, w for y_true=0]\n",
    "            Ex.\n",
    "                WeightedBinaryCrossntropy(reduction=tf.losses.Reduction.NONE, element_weight=[5, 1])(y_true, y_pred)\n",
    "        \"\"\"\n",
    "\n",
    "        super(WeightedBinaryCrossntropy, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "    \n",
    "    def call(self, y_true, y_pred, from_logits=False):\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        \n",
    "        element_weight = self.element_weight\n",
    "        \n",
    "#         print(element_weight)\n",
    "        \n",
    "        if not from_logits:\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "            y_pred = tf.math.log(y_pred / (1 - y_pred))\n",
    "            y_pred = tf.math.sigmoid(y_pred)\n",
    "            \n",
    "        if element_weight is None:\n",
    "            bc = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        else:\n",
    "            bc = -(y_true * tf.math.log(y_pred) * element_weight[0] + (1 - y_true) * tf.math.log(1 - y_pred) * element_weight[1])\n",
    "\n",
    "        return tf.math.reduce_mean(bc, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPCNN1D(Layer):\n",
    "\n",
    "    '''\n",
    "        temporal pattern 1d-CNN\n",
    "        output shape = [None, features, filters] or [None, m, k]\n",
    "        \n",
    "        Layers are recursively composable\n",
    "            If you assign a Layer instance as an attribute of another Layer, \n",
    "            the outer layer will start tracking the weights of the inner layer.\n",
    "            We recommend creating such sublayers in the __init__() method (since the sublayers will typically have a build method, t\n",
    "            hey will be built when the outer layer gets built).\n",
    "    '''\n",
    "    def __init__(self, filters, name=\"TPCNN1D\", **kwargs):\n",
    "        super(TPCNN1D, self).__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        self.cnn1d = Conv1D(kernel_size=1, filters=self.filters, data_format='channels_first', name=self.name)\n",
    "\n",
    "        \n",
    "#     def build(self, input_shape):\n",
    "#         self.cnn1d = Conv1D(kernel_size=1, filters=self.filters, data_format='channels_first', name=self.name)\n",
    "#         self.cnn1d.build(input_shape)\n",
    "#         self._trainable_weights = self.cnn1d.trainable_weights\n",
    "        \n",
    "#         super(TPCNN1D, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "#         return self.cnn1d(x)\n",
    "        return tf.transpose(self.cnn1d(x), perm=[0, 2, 1])\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"filters\": self.filters}\n",
    "        base_config = super(TPCNN1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1], units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.W)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPAttention(Layer):\n",
    "    def __init__(self, timesteps=None, features=None,\n",
    "                 W_regularizer=None, W_constraint=None, name=\"TPAttention\", **kwargs):\n",
    "        '''     \n",
    "            Temporal Pattern Attention [https://arxiv.org/abs/1809.04206]\n",
    "                \n",
    "                \n",
    "                - k: filters or timesteps - 1 \n",
    "                - m: features\n",
    "            \n",
    "            \n",
    "            Keras Layer that implements an Attention mechanism for temporal data.\n",
    "            Supports Masking. \n",
    "            Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "            # Input shape\n",
    "                3D tensor with shape: `(samples, steps, features)`.\n",
    "            # Output shape\n",
    "                2D tensor with shape: `(samples, features)`.\n",
    "            :param kwargs:\n",
    "            Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "            The dimensions are inferred based on the output shape of the RNN.\n",
    "            Example:\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(Attention())\n",
    "\n",
    "            tensorflow & keras reference:\n",
    "                https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "                https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "                https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
    "            \n",
    "        '''\n",
    "        super(TPAttention, self).__init__(name=name, **kwargs)  # inherit Layer\n",
    "\n",
    "#         self.Linear()\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3, [batchsize, timestep, # of feature]\n",
    "        '''\n",
    "\n",
    "        if self.timesteps is None:  # k + 1\n",
    "            self.timesteps = input_shape[-1] + 1\n",
    "            \n",
    "        if self.features is None:  # m\n",
    "            self.features = input_shape[-2]   \n",
    "\n",
    "        # shape of W = [k, m]\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.features,),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "        # shape of W_h = [m, m]        \n",
    "        self.W_h = self.add_weight(shape=(self.features, self.features,),\n",
    "                                   initializer=self.init,\n",
    "                                   name='{}_Wh'.format(self.name))\n",
    "        \n",
    "        # shape of W_v = [m, k]\n",
    "        self.W_v = self.add_weight(shape=(self.features, input_shape[-1],),\n",
    "                                   initializer=self.init,\n",
    "                                   name='{}_Wv'.format(self.name))\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, h_t, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "            \n",
    "            Notations:\n",
    "                - H^C (= x): Convlutional operations on return sequence of LSTM\n",
    "                - h_t: hidden state of current time\n",
    "                - k: filters or timesteps - 1 \n",
    "                - m: features\n",
    "                \n",
    "            x.shape = [None, k, m]\n",
    "            h_t.shape = [None, m] \n",
    "        '''        \n",
    "\n",
    "        features = self.features\n",
    "        timesteps = self.timesteps\n",
    "        \n",
    "        logging.info(\"features = {}\".format(features))\n",
    "        logging.info(\"timesteps = {}\".format(timesteps))\n",
    "        logging.info(\"x.shape (batchsize, m, k) = {}\".format(x.shape))\n",
    "        logging.info(\"W.shape (k, m) = {}\".format(self.W.shape))\n",
    "        logging.info(\"h_t.shape (batchsize, m) (need to expand dim) = {}\".format(h_t.shape))\n",
    "        \n",
    "        scored = tf.matmul(x, self.W)\n",
    "        scored = tf.matmul(scored, tf.expand_dims(h_t, axis=-1))\n",
    "        alpha_i = tf.math.sigmoid(scored)  # [None, k, 1]        \n",
    "        context_vector = tf.matmul(tf.ones((1, features)), tf.multiply(alpha_i, x))  # row summation\n",
    "        context_vector = tf.transpose(context_vector, perm=[0, 2, 1])  # [None, k, 1] \n",
    "        \n",
    "        logging.info(\"scored.shape (batchsize, m, 1) = {}\".format(scored.shape))\n",
    "        logging.info(\"alpha_i.shape (batchsize, m, 1) = {}\".format(alpha_i.shape))        \n",
    "        logging.info(\"shape of W_h (m, m) = {}, h_t (batchsize, m, 1) = {}\".format(self.W_h.shape, tf.reshape(h_t, (-1, features, 1)).shape))\n",
    "        logging.info(\"shape of W_v (m, k) = {}, context_vector (batchsize, k, 1) = {}\".format(self.W_v.shape, context_vector.shape))\n",
    "        return tf.reshape(tf.matmul(self.W_h, tf.reshape(h_t, (-1, features, 1))) + tf.matmul(self.W_v, context_vector), (-1, features))\n",
    "      \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "#         if mask is not None:\n",
    "#             # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "#             a *= tf.cast(mask, K.floatx())\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         #return input_shape[0], input_shape[-1]\n",
    "#         return input_shape[0], self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\"timesteps\": self.timesteps, \n",
    "                  \"features\": self.features, \n",
    "                  \"W_regularizer\": self.W_regularizer, \n",
    "                  \"W_constraint\": self.W_constraint}\n",
    "        base_config = super(TPAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangeableLossw(Callback):\n",
    "    def __init__(self, lossw, wmultiplier):\n",
    "    \n",
    "        self.nlossw = 2\n",
    "        self.lossw = lossw\n",
    "        self.wmultiplier = wmultiplier\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch <= 10:\n",
    "#             logf = \"epoch {}, \".format(epoch)\n",
    "            for idx in range(self.nlossw):\n",
    "                K.set_value(self.lossw[idx], K.get_value(self.lossw[idx]) * self.wmultiplier[idx])\n",
    "#                 logf += \"lossw_{} = {}, \".format(idx, K.get_value(self.lossw[idx]))\n",
    "#             logf += \"\\n\"\n",
    "        \n",
    "#             logging.info(logf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBuilder():\n",
    "    def __init__(self, modeld=\"model\", ckptd=\"ckpt\", name=\"NNBuilder\", optimizer=\"SGD\"):\n",
    "        \n",
    "        if not os.path.exists(modeld):\n",
    "            os.makedirs(modeld)\n",
    "            \n",
    "        if not os.path.exists(ckptd):\n",
    "            os.makedirs(ckptd)\n",
    "        \n",
    "        self.name = name\n",
    "        self.modeld = modeld\n",
    "        self.ckptd = ckptd\n",
    "        self.callbacks = self._callbacks(modeld, ckptd, name=name)\n",
    "        self.optimizer = self._optimizer(name=optimizer)\n",
    "        \n",
    "            \n",
    "        \n",
    "    def CNN1D(self, filters, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None):\n",
    "        \n",
    "        _args       = NNBuilder._argreset(filters, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "                \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "\n",
    "        x = MaxPool1D(pool_size=2)(x)\n",
    "        x = Flatten()(x)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(x)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(x)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"CNN1D\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"CNN1D\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def TPALSTM(self):\n",
    "        embedding_layer = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "        sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        merged = concatenate([x1, y1])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        ########################################\n",
    "        ## add class weight\n",
    "        ########################################\n",
    "        if re_weight:\n",
    "            class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "        else:\n",
    "            class_weight = None\n",
    "\n",
    "        ########################################\n",
    "        ## train the model\n",
    "        ########################################\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "                outputs=preds)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer='nadam',\n",
    "                metrics=['acc'])\n",
    "        #model.summary()\n",
    "        print(STAMP)\n",
    "\n",
    "        early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "        bst_model_path = STAMP + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "        hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "                validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "                epochs=200, batch_size=2048, shuffle=True, \\\n",
    "                class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "        model.load_weights(bst_model_path)\n",
    "        bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "        ########################################\n",
    "        ## make the submission\n",
    "        ########################################\n",
    "        print('Start making the submission before fine-tuning')\n",
    "\n",
    "        preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "        preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "        preds /= 2\n",
    "\n",
    "        submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "        submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n",
    "    \n",
    "    def bidirectionalLSTM(self, cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0, merge_mode='concat'):    \n",
    "        \n",
    "        '''\n",
    "            merge_mode: one of {'sum', 'mul', 'concat', 'ave', None}.\n",
    "            if merge_mode='concat', then shape of LSTM output is [timesteps, # of cells * 2 (directions)] \n",
    "\n",
    "        '''\n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "           \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    x = Bidirectional(layer=LSTM(units[idx], return_sequences=True), \n",
    "                                      backward_layer=LSTM(units[idx], return_sequences=True, go_backwards=True), \n",
    "                                      merge_mode=merge_mode,\n",
    "                                      name=\"BLSTM_{}\".format(idx + 1))(inputs)\n",
    "#                 elif idx == nlayer - 1:  # the last hidden layer\n",
    "#                     x = LSTM(units[idx], name=\"LSTM_{}\".format(idx + 1))(x) \n",
    "                else:\n",
    "                    x = Bidirectional(layer=LSTM(units[idx], return_sequences=True), \n",
    "                                      backward_layer=LSTM(units[idx], return_sequences=True, go_backwards=True), \n",
    "                                      merge_mode=merge_mode,\n",
    "                                      name=\"BLSTM_{}\".format(idx + 1))(x)\n",
    "                logging.info(\"{0:02d}, x.shape = {1}\".format(idx, x.shape))\n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            x = Bidirectional(layer=LSTM(units[0], return_sequences=True), \n",
    "                              backward_layer=LSTM(units[0], return_sequences=True, go_backwards=True),\n",
    "                              merge_mode=merge_mode,\n",
    "                              name=\"BLSTM_1\")(inputs)\n",
    "#             x = Bidirectional(LSTM(units[0]), name=\"BLSTM_1\")(inputs)\n",
    "#             x = LSTM(units[0], name=\"BLSTM_1\")(inputs)\n",
    "#             x = LSTM(units[0], name=\"BLSTM_1\", return_sequences=True, return_state=True)(inputs)\n",
    "#             x = LSTM(units[0], name=\"BLSTM_1\", return_sequences=False, return_state=True)(inputs)\n",
    "\n",
    "\n",
    "            logging.info(\"01, x.shape = {}\".format(x.shape))\n",
    "\n",
    "#         x = tf.strided_slice(x, [0, 2, 0], [-1, 2, 4])\n",
    "\n",
    "        assert x.shape[1] == timesteps\n",
    "\n",
    "        H = Lambda(lambda x: x[:, 0:-1, :], name=\"H\")(x)\n",
    "        h_t = Lambda(lambda x: x[:, -1, :], name=\"h_t\")(x)\n",
    "        x = TPCNN1D(filters=units[-1], name=\"TPCNN1D\")(H)\n",
    "        x = TPAttention(name=\"TPAttention\")(x, h_t)\n",
    "\n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1\")(x)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2\")(x)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"bidirectionalLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"bidirectionalLSTM\")\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    \n",
    "    def _stackedLSTM(self, cells, inshape, outshape, target=\"regression\"):    \n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "   \n",
    "        model = Sequential()\n",
    "        \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    model.add(LSTM(units[idx], input_shape=(timesteps, nfeatures), return_sequences=True, name=\"lstm_{}\".format(idx))) \n",
    "                elif idx == nlayer - 1:  # the last hidden layer\n",
    "                    model.add(LSTM(units[idx], name=\"lstm_{}\".format(idx))) \n",
    "                else:\n",
    "                    model.add(LSTM(units[idx], return_sequences=True, name=\"lstm_{}\".format(idx))) \n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm_0\")) \n",
    "                \n",
    "        if target == \"regression\":\n",
    "            model.add(Dense(nfeatures, activation='sigmoid', name=\"dense\"))  # for regression\n",
    "        else:\n",
    "            model.add(Dense(nfeatures, activation='softmax', name=\"dense\"))  # for classification\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    def stackedLSTM(self, cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0):    \n",
    "        \n",
    "        '''\n",
    "            - dropout, applied to the first operation on the inputs\n",
    "            - recurrent_dropout, applied to the other operation on the recurrent inputs (previous output and/or states)\n",
    "        '''\n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "           \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    x = LSTM(units[idx], return_sequences=True, dropout=dropout, name=\"LSTM_{}\".format(idx + 1))(inputs) \n",
    "                elif idx == nlayer - 1:  # the last hidden layer\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, name=\"LSTM_{}\".format(idx + 1))(x) \n",
    "                else:\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, return_sequences=True, name=\"LSTM_{}\".format(idx + 1))(x)\n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            x = LSTM(units[0], dropout=dropout, name=\"LSTM_1\")(inputs)\n",
    "                \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"stackedLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"stackedLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def LSTMbasicAttention(self, shape, cells):\n",
    "        '''\n",
    "            shape = (timestep, feature)\n",
    "            return [model, optimizer, callbacks]\n",
    "        '''\n",
    "        \n",
    "        nfeatures = shape[1]\n",
    "        \n",
    "        inputs = Input(shape, name=\"input\")  # return a tensor\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "        else:\n",
    "            units = [cells]\n",
    "                \n",
    "        for idx, unit in enumerate(units):\n",
    "            if idx == 0:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(inputs)\n",
    "            else:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(x)\n",
    "            x = Attention(shape[0])(x)\n",
    "            \n",
    "        outputs = Dense(nfeatures)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    \n",
    "    def DNNLSTM(self, units, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None):\n",
    "        \n",
    "        '''\n",
    "            units: [units for Dense_1, ..., units for Dense_n, cells for LSTM_1], i.e. units[-1]: cells for LSTM_1\n",
    "            inshape: (timesteps, # of features)\n",
    "            outshape: an integer number for output layer (Dense), ex. 4\n",
    "            \n",
    "            NNB = NNBuilder()\n",
    "            model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 4], inshape=(10, 4), outshape=4, batchNormalization=None)\n",
    "            model.summary()\n",
    "            _________________________________________________________________\n",
    "            Model: \"DNNLSTM\"\n",
    "            _________________________________________________________________\n",
    "            Layer (type)                 Output Shape              Param #   \n",
    "            =================================================================\n",
    "            input (InputLayer)           [(None, 10, 4)]           0         \n",
    "            _________________________________________________________________\n",
    "            TDense_1 (TimeDistributed)   (None, 10, 10)            50         (4 * 10 + 10)\n",
    "            _________________________________________________________________\n",
    "            Activation_1 (TimeDistribute (None, 10, 10)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_2 (TimeDistributed)   (None, 10, 20)            220        (10 * 20 + 20)  \n",
    "            _________________________________________________________________\n",
    "            Activation_2 (TimeDistribute (None, 10, 20)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_3 (TimeDistributed)   (None, 10, 30)            630        (20 * 30 + 30)    \n",
    "            _________________________________________________________________\n",
    "            Activation_3 (TimeDistribute (None, 10, 30)            0         \n",
    "            _________________________________________________________________  ↓ (input gate, forget gate, output gate and neuron) \n",
    "            LSTM (LSTM)                  (None, 4)                 560        (4 * (30 * 4 + 4 + 4 * 4))\n",
    "            _________________________________________________________________                    ↑ (cell state pass to the other cells) \n",
    "            output (Dense)               (None, 4)                 20         (4 * 4 + 4)       \n",
    "            =================================================================\n",
    "            Total params: 1,480\n",
    "            Trainable params: 1,480\n",
    "            Non-trainable params: 0\n",
    "            \n",
    "            (4 * 10 + 10) + (10 * 20 + 20) + (20 * 30 + 30) + 4 * (30 * 4 + 4 + 4 * 4) + (4 * 4 + 4)   \n",
    "        '''\n",
    "        \n",
    "        assert len(units) >= 2\n",
    "        \n",
    "        inputs = Input(inshape, name=\"input\")  \n",
    "\n",
    "        nlayer = len(units) - 1\n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "        else:\n",
    "            activations = [\"relu\" for _ in range(nlayer)]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = TimeDistributed(BatchNormalization(), name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = TimeDistributed(Activation(activations[i]), name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = TimeDistributed(Dropout(dropouts[i]), name=\"Dropout_{}\".format(i + 1))(x)\n",
    "        x = LSTM(units[-1], name=\"LSTM\")(x)\n",
    "        \n",
    "        \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"DNNLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"DNNLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def DenseBuilder(units, inputs, batchNormalization=True, dropouts=None, activations=None):\n",
    "        \n",
    "        _args       = NNBuilder._argreset(units, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "    \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _argreset(units, dropouts=None, activations=None):\n",
    "        \n",
    "        _args = dict()\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(units, list):\n",
    "            nlayer = len(units)\n",
    "        else:\n",
    "            units = [units]\n",
    "        \n",
    "        _args[\"units\"] = units\n",
    "        _args[\"nlayer\"] = nlayer\n",
    "        \n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "            _args[\"dropouts\"] = dropouts\n",
    "        else:\n",
    "            _args[\"dropouts\"] = None\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "            _args[\"activations\"] = activations\n",
    "        else:\n",
    "            _args[\"activations\"] = None\n",
    "#             activations = [\"relu\" for _ in range(nlayer)]\n",
    "            \n",
    "        return _args\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _callbacks(modeld, ckptd, mmonitor=\"val_loss\", emonitor=\"loss\", lmonitor=\"val_loss\", name=\"ckpt\"):\n",
    "        \n",
    "        '''\n",
    "            mmonitor: monitor for model \n",
    "            emonitor: monitor for earlystopping\n",
    "            lmonitor: monitor for learning rate\n",
    "        '''\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "        \n",
    "        name_ = \"{epoch:04d}_{loss:.3f}_{val_loss:.3f}\"\n",
    "#         checkpointer = ModelCheckpoint(filepath=os.path.join(modeld, \"{0}_{1}_{2}.hdf5\".format(name, name_, timestamp)),\n",
    "        checkpointer = ModelCheckpoint(filepath=os.path.join(modeld, \"{0}.hdf5\".format(name)),\n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       monitor=mmonitor)\n",
    "        \n",
    "        earlystopper = EarlyStopping(monitor=emonitor, patience=10)\n",
    "\n",
    "        reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.5, patience=10, min_lr=0.0001)\n",
    "\n",
    "#         reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.9, patience=10, min_lr=0.0001)\n",
    "        \n",
    "        tb = TensorBoard(log_dir=ckptd)\n",
    "\n",
    "        csvlogger = CSVLogger(os.path.join(ckptd, \"{}_{}.log\".format(name, timestamp)), append=False, separator=\",\")\n",
    "\n",
    "        # Learning rate schedule.\n",
    "    #     lr_schedule = LearningRateScheduler(fixed_schedule, verbose=0)\n",
    "\n",
    "        return [checkpointer, earlystopper, reduceLR, tb, csvlogger]\n",
    "\n",
    "#         return [checkpointer, earlystopper, reduceLR, csvlogger]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _optimizer(lr=1e-2, name=\"Adam\"):\n",
    "        if name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "        else:\n",
    "            optimizer = Adam(learning_rate=lr)\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def mloader(filepath, custom_objects=None):\n",
    "        if custom_objects is not None:\n",
    "            return load_model(filepath, custom_objects=custom_objects)\n",
    "        else:\n",
    "            return load_model(filepath)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MArgs(object):\n",
    "    def __init__(self, mname, testf=None, train=None):\n",
    "        self.mname = mname\n",
    "        self.testf = testf\n",
    "        self.train = train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 100, batch_size = 100, nsize = 10000, nstep = 6, nfeature = 5, ntarget = 4, nclass = 5\n",
      "X_train.shape = (10000, 6, 5), y_train[0].shape = (10000, 4), y_train[1].shape = (10000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 11:00:41,429, bidirectionalLSTM-156-INFO: 00, x.shape = (None, 6, 20)\n",
      "2020-10-21 11:00:41,738, bidirectionalLSTM-156-INFO: 01, x.shape = (None, 6, 40)\n",
      "2020-10-21 11:00:41,738, bidirectionalLSTM-180-INFO: shape of x (LSTM) = (None, 6, 40)\n",
      "2020-10-21 11:00:41,752, bidirectionalLSTM-187-INFO: shape of x (TPCNN1D) = (None, 40, 20)\n",
      "2020-10-21 11:00:41,757, converted_call-603-INFO: features = 40\n",
      "2020-10-21 11:00:41,767, converted_call-603-INFO: timesteps = 21\n",
      "2020-10-21 11:00:41,776, converted_call-603-INFO: x.shape (batchsize, m, k) = (None, 40, 20)\n",
      "2020-10-21 11:00:41,786, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2020-10-21 11:00:41,796, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (None, 40)\n",
      "2020-10-21 11:00:41,810, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (None, 40, 1)\n",
      "2020-10-21 11:00:41,819, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (None, 40, 1)\n",
      "2020-10-21 11:00:41,829, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (None, 40, 1)\n",
      "2020-10-21 11:00:41,839, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (None, 20, 1)\n",
      "2020-10-21 11:00:41,851, bidirectionalLSTM-189-INFO: shape of x (TPAttention) = (None, 40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bidirectionalLSTM\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 6, 5)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BLSTM_1 (Bidirectional)         (None, 6, 20)        1280        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BLSTM_2 (Bidirectional)         (None, 6, 40)        6560        BLSTM_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "H (Lambda)                      (None, 5, 40)        0           BLSTM_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "TPCNN1D (TPCNN1D)               (None, 40, 20)       120         H[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "h_t (Lambda)                    (None, 40)           0           BLSTM_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "TPAttention (TPAttention)       (None, 40)           3200        TPCNN1D[0][0]                    \n",
      "                                                                 h_t[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "LDense1_1 (Dense)               (None, 20)           820         TPAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LDense2_1 (Dense)               (None, 20)           820         TPAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LDense1_2 (Dense)               (None, 12)           252         LDense1_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "LDense2_2 (Dense)               (None, 15)           315         LDense2_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Loss1 (Dense)                   (None, 4)            52          LDense1_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Loss2 (Dense)                   (None, 5)            80          LDense2_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,499\n",
      "Trainable params: 13,499\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "      0  1  2  3  4\n",
      "0     0  1  0  0  0\n",
      "1     0  0  0  0  1\n",
      "2     0  0  1  0  0\n",
      "3     0  1  0  0  0\n",
      "4     0  0  0  1  0\n",
      "...  .. .. .. .. ..\n",
      "9995  0  0  1  0  0\n",
      "9996  0  0  1  0  0\n",
      "9997  1  0  0  0  0\n",
      "9998  0  1  0  0  0\n",
      "9999  0  0  0  0  1\n",
      "\n",
      "[10000 rows x 5 columns]\n",
      "(10000, 6, 5) (10000, 4) (10000, 5)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 11:00:43,342, converted_call-603-INFO: features = 40\n",
      "2020-10-21 11:00:43,352, converted_call-603-INFO: timesteps = 21\n",
      "2020-10-21 11:00:43,362, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2020-10-21 11:00:43,371, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2020-10-21 11:00:43,381, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2020-10-21 11:00:43,396, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:43,405, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:43,416, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:43,425, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n",
      "2020-10-21 11:00:44,702, converted_call-603-INFO: features = 40\n",
      "2020-10-21 11:00:44,712, converted_call-603-INFO: timesteps = 21\n",
      "2020-10-21 11:00:44,722, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2020-10-21 11:00:44,731, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2020-10-21 11:00:44,740, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2020-10-21 11:00:44,754, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:44,764, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:44,774, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:44,784, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/70 [..............................] - ETA: 7s - loss: 1.8698 - Loss1_loss: 0.2599 - Loss2_loss: 1.6099WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_train_batch_end` time: 0.2092s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 11:00:46,247, _call_batch_end_hook-328-WARNING: Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0071s vs `on_train_batch_end` time: 0.2092s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - ETA: 0s - loss: 1.8616 - Loss1_loss: 0.2496 - Loss2_loss: 1.6120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-21 11:00:47,246, converted_call-603-INFO: features = 40\n",
      "2020-10-21 11:00:47,255, converted_call-603-INFO: timesteps = 21\n",
      "2020-10-21 11:00:47,265, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2020-10-21 11:00:47,274, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2020-10-21 11:00:47,284, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2020-10-21 11:00:47,298, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:47,307, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:47,317, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2020-10-21 11:00:47,327, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 2s 24ms/step - loss: 1.8616 - Loss1_loss: 0.2496 - Loss2_loss: 1.6120 - val_loss: 1.8640 - val_Loss1_loss: 0.2524 - val_Loss2_loss: 1.6116\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8601 - Loss1_loss: 0.2495 - Loss2_loss: 1.6106 - val_loss: 1.8615 - val_Loss1_loss: 0.2512 - val_Loss2_loss: 1.6102\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8590 - Loss1_loss: 0.2493 - Loss2_loss: 1.6097 - val_loss: 1.8612 - val_Loss1_loss: 0.2512 - val_Loss2_loss: 1.6100\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2492 - Loss2_loss: 1.6096 - val_loss: 1.8615 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6103\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2492 - Loss2_loss: 1.6096 - val_loss: 1.8611 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6100\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2492 - Loss2_loss: 1.6095 - val_loss: 1.8615 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6103\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2492 - Loss2_loss: 1.6097 - val_loss: 1.8614 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6102\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2492 - Loss2_loss: 1.6096 - val_loss: 1.8611 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6099\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2492 - Loss2_loss: 1.6095 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6100\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2492 - Loss2_loss: 1.6096 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8613 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2492 - Loss2_loss: 1.6096 - val_loss: 1.8610 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6099\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8610 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6099\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8616 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6105\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2492 - Loss2_loss: 1.6097 - val_loss: 1.8616 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6105\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2491 - Loss2_loss: 1.6096 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2492 - Loss2_loss: 1.6096 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8588 - Loss1_loss: 0.2492 - Loss2_loss: 1.6096 - val_loss: 1.8614 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6102\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8589 - Loss1_loss: 0.2491 - Loss2_loss: 1.6097 - val_loss: 1.8614 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6103\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2492 - Loss2_loss: 1.6095 - val_loss: 1.8610 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6099\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2492 - Loss2_loss: 1.6095 - val_loss: 1.8613 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6102\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6094 - val_loss: 1.8615 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6103\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8613 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6102\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8613 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6102\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8613 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8587 - Loss1_loss: 0.2491 - Loss2_loss: 1.6096 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8611 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6099\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6095 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6101\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6094 - val_loss: 1.8612 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6100\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.8586 - Loss1_loss: 0.2491 - Loss2_loss: 1.6094 - val_loss: 1.8611 - val_Loss1_loss: 0.2511 - val_Loss2_loss: 1.6100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "    mname = \"bidirectionalLSTM\"\n",
    "    args = MArgs(mname, train=[100, 100, 10000, 6, 5, 4, 5])\n",
    "    \n",
    "    import tensorflow as tf\n",
    "\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    \n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], enable=True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    if args.testf == \"_argreset\":\n",
    "        _args = NNBuilder._argreset([10, 20, 30], dropouts=0.5, activations=\"relu\")\n",
    "        print(_args)\n",
    "    \n",
    "    \n",
    "    if args.train is not None:\n",
    "        epochs, batch_size, nsize, nstep, nfeature, ntarget, nclass = args.train\n",
    "        X_train = np.random.random_sample((nsize, nstep, nfeature))\n",
    "        y_train = [np.random.random_sample((nsize, ntarget)), pd.get_dummies(pd.Series(np.random.randint(low=0, high=nclass, size=nsize)))]\n",
    "        print(\"epochs = {}, batch_size = {}, nsize = {}, nstep = {}, nfeature = {}, ntarget = {}, nclass = {}\".format(epochs, batch_size, nsize, nstep, nfeature, ntarget, nclass))\n",
    "\n",
    "        print(\"X_train.shape = {}, y_train[0].shape = {}, y_train[1].shape = {}\".format(X_train.shape, y_train[0].shape, y_train[1].shape))\n",
    "        \n",
    "    \n",
    "    if args.mname == \"DNNbuilder\":\n",
    "        inputs = Input(shape=(10), name=\"Input\")\n",
    "        x = NNBuilder.DenseBuilder([10, 30, 20, 40], inputs, dropouts=0.25, activations=\"relu\")\n",
    "        model = Model(inputs=inputs, outputs=x, name=\"DNN\")\n",
    "        model.summary()\n",
    "\n",
    "    elif args.mname == \"DNNLSTM\":\n",
    "        NNB = NNBuilder()\n",
    "        model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 10], inshape=(6, 4), outshape=[4, 1], outactfn=[\"tanh\", \"sigmoid\"], batchNormalization=None)\n",
    "        model.summary()\n",
    "        plot_model(model, to_file=\"DNNLSTM.png\", show_shapes=True)\n",
    "\n",
    "        model.compile(loss={\"regression_output\": \"mae\", \"classification_output\": \"binary_crossentropy\"},\n",
    "                      metrics={\"regression_output\": \"mae\", \"classification_output\": \"accuracy\"},\n",
    "                      optimizer=optimizer_)\n",
    "\n",
    "        epochs = 1000\n",
    "        batch_size = 10\n",
    "        n = 10000\n",
    "        X_train = np.random.random_sample((n, 6, 4))\n",
    "        y_train = [np.random.random_sample((n, 4)), np.random.randint(low=0, high=2, size=(n, 1))]\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "    elif args.mname == \"stackedLSTM\":\n",
    "        stackedLSTM, callbacks_, optimizer_ = NNBuilder().stackedLSTM(cells=[10, 20], inshape=[6, 4], outshape=[4, 4], outactfn=[\"sigmoid\", \"softmax\"])\n",
    "        stackedLSTM.compile(loss=\"mae\", optimizer=optimizer_)\n",
    "        stackedLSTM.summary()\n",
    "\n",
    "        epochs = 3\n",
    "        batch_size = 200\n",
    "        X_train = np.random.random_sample((1000, 6, 4))\n",
    "        y_train = [np.random.random_sample((1000, 4)), np.random.randint(low=0, high=2, size=1000)]\n",
    "\n",
    "\n",
    "    #     history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "    #     saved_model = \"/home/yuzhe/DataScience/QC/model/lstm1_0154_0.009_0.008_202008071814.hdf5\"\n",
    "    #     model = NNBuilder.mloader(saved_model)\n",
    "\n",
    "    elif args.mname == \"CNN1D\":\n",
    "        CNN1D, callbacks_, optimizer_ = NNBuilder().CNN1D(filters=[10, 20], inshape=[6, 4], outshape=[2], outactfn=[\"sigmoid\"], activations=\"relu\")\n",
    "        CNN1D.summary()\n",
    "\n",
    "    #     train(X_train, y_train, 30, 5000, loss=YZKError(element_weight=[1 / 6., 1 / 6., 1 / 6., 1 / 2.]), name=\"NNBuilderTest1\")\n",
    "    #     train(X_train, y_train, 30, 5000, loss=YZKError(), name=\"NNBuilderTest\")\n",
    "\n",
    "    elif args.mname == \"bidirectionalLSTM\":\n",
    "        bLSTM, callbacks_, optimizer_ = NNBuilder().bidirectionalLSTM(merge_mode=\"concat\", cells=[10, 20], inshape=[nstep, nfeature], outshape=[ntarget, nclass], outactfn=[\"sigmoid\", \"softmax\"])\n",
    "\n",
    "        bLSTM.compile(loss={\"Loss1\": \"mae\", \"Loss2\": \"categorical_crossentropy\"}, optimizer=optimizer_)\n",
    "        print(y_train[1])\n",
    "        print(X_train.shape, y_train[0].shape, y_train[1].shape)\n",
    "        history = bLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.3, verbose=1, shuffle=True)\n",
    "\n",
    "        \n",
    "#         (None, 4, 20) mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logcosh(a, t):\n",
    "    return (1 / a) * np.log(np.cosh(a * t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "#### check losses     \n",
    "    \n",
    "    from dgenerator import dgenerator\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    tperiod = [2016010101, 2016123124]\n",
    "    n_in = 6\n",
    "    n_out = 1\n",
    "    mode = \"test\"\n",
    "    vstack = True\n",
    "    fnpy = True\n",
    "    npyd = \"/home/yuzhe/DataScience/dataset\"\n",
    "    gif = \"/home/yuzhe/CODE/ProgramT1/GRDTools/SRC/RES/GI/1500_decode_stationlist_without_space.txt\"\n",
    "\n",
    "    dg = dgenerator(gif=gif, npyd=npyd)\n",
    "    vinfo = pd.DataFrame(dg.vrange)  \n",
    "    vinfo = pd.DataFrame(vinfo)\n",
    "    print(vinfo)\n",
    "#     vinfo = {\"Temp\": [-20.0, 50.0],\n",
    "#              \"RH\": [0.0, 100.0], \n",
    "#              \"Pres\": [600.0, 1100.0], \n",
    "#              \"Precp\": [0.0, 220.0]}\n",
    "    dataset = dg.hrfgenerator(tperiod, n_in=n_in, n_out=n_out, mode=mode, rescale=True, reformat=True, vstack=vstack, fnpy=fnpy, generator=False)\n",
    "    \n",
    "    datetimes = dataset[1]\n",
    "    nsize = len(datetimes)\n",
    "    print(dataset[0].shape)\n",
    "\n",
    "    \n",
    "    saved_model = \"../QC/model/lstm1_0055_0.008_0.011_202008111819_2.hdf5\"\n",
    "    model = NNBuilder.mloader(saved_model)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(vinfo.values)\n",
    "    \n",
    "    print(scaler.inverse_transform([[0.7, 0.6, 0.7, 0.2]]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    n = nsize\n",
    "    \n",
    "#     x = dataset[0][:, -4:]\n",
    "#     x = x[~np.isnan(x).any(axis=1)]\n",
    "#     idx = np.random.choice(np.arange(x.shape[0]), n, replace=False)\n",
    "#     x = x[idx, 0:]\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "\n",
    "    scaled = dataset[0]\n",
    "    scaled = scaled[~np.isnan(scaled).any(axis=1)]\n",
    "    X_test = np.reshape(scaled[:, :-4], (-1, 6, 4))\n",
    "    y_true = scaled[:, -4:]\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "#     xynorm = tf.norm(tf.subtract(y_pred, y_true), axis=1) \n",
    "\n",
    "    y_true = tf.reshape(y_true[:, -1], [-1, 1])\n",
    "    y_pred = tf.reshape(y_pred[:, -1], [-1, 1])\n",
    "    xynorm = tf.subtract(y_pred, y_true)\n",
    "\n",
    "    print(y_true.shape, y_true)\n",
    "    print(y_pred.shape, y_pred)\n",
    "    print(xynorm.shape)\n",
    "    \n",
    "    sample_weight = 1\n",
    "#     sample_weight = tf.broadcast_to(sample_weight, y_pred.shape)\n",
    "    \n",
    "    loss = YZKError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"YZK\")\n",
    "    print('yzk-loss: ', loss)\n",
    "    \n",
    "    loss = tf.losses.LogCosh(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "\n",
    "    print(\"shape of loss = {}, xynorm = {}\".format(loss, xynorm.shape))\n",
    "    ax.scatter(xynorm, loss, label=\"LogCosh\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MAE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Cos\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "\n",
    "#     loss = tf.keras.losses.KLDivergence(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "#     ax.scatter(xynorm, loss, label=\"KL\")\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"MSE\")\n",
    "\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MSLE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "    \n",
    "    loss = tf.keras.losses.Huber(reduction=tf.losses.Reduction.NONE, delta=0.25)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Huber\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_weights(training_data, class_weight_dictionary): \n",
    "    sample_weights = [class_weight_dictionary[np.where(one_hot_row==1)[0][0]] for one_hot_row in training_data]\n",
    "    return np.asarray(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
