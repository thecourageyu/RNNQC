{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model builder\n",
    "- date: 2020-08-07\n",
    "- maintainer: YZK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.720861Z",
     "iopub.status.busy": "2021-02-25T07:28:57.720612Z",
     "iopub.status.idle": "2021-02-25T07:28:57.723295Z",
     "shell.execute_reply": "2021-02-25T07:28:57.722790Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.720834Z"
    }
   },
   "outputs": [],
   "source": [
    "# jupyter nbconvert --to script mbuilder.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.730063Z",
     "iopub.status.busy": "2021-02-25T07:28:57.729869Z",
     "iopub.status.idle": "2021-02-25T07:28:57.751624Z",
     "shell.execute_reply": "2021-02-25T07:28:57.751107Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.730040Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from collections import deque, Counter\n",
    "from fbprophet import Prophet\n",
    "from functools import partial\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Sequential, Model, losses\n",
    "from tensorflow.keras import constraints, initializers, regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Bidirectional, Lambda, Layer, TimeDistributed\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Conv1D, Conv2D, Dense, Dropout, Flatten, Input, LSTM, MaxPool1D, MaxPool2D\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import msetup\n",
    "    msetup.setLogging(loglv=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.752743Z",
     "iopub.status.busy": "2021-02-25T07:28:57.752562Z",
     "iopub.status.idle": "2021-02-25T07:28:57.808860Z",
     "shell.execute_reply": "2021-02-25T07:28:57.808228Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.752720Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.810223Z",
     "iopub.status.busy": "2021-02-25T07:28:57.810034Z",
     "iopub.status.idle": "2021-02-25T07:28:57.826124Z",
     "shell.execute_reply": "2021-02-25T07:28:57.825617Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.810200Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstmbuilder(units, input_shape, loss, optimizer):\n",
    "    '''\n",
    "        input_shape: a tuple (timesteps, nfeatures)\n",
    "    '''\n",
    "    \n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(units, input_shape=input_shape))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.compile(loss=loss, optimizer=optimizer)\n",
    "             \n",
    "    return lstm\n",
    "\n",
    "\n",
    "# lstmbuilder(10, (10, 3), 'mae', SGD()).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.827261Z",
     "iopub.status.busy": "2021-02-25T07:28:57.827095Z",
     "iopub.status.idle": "2021-02-25T07:28:57.877040Z",
     "shell.execute_reply": "2021-02-25T07:28:57.876380Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.827241Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking. \n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \n",
    "        tensorflow & keras reference:\n",
    "            https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "            https://www.tensorflow.org/guide/keras/masking_and_padding\n",
    "            https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking\n",
    "            \n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)  # inherit Layer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3, [batchsize, timestep, # of feature]\n",
    "        '''\n",
    "        \n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "#         self.step_dim = input_shape[-2]\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "        '''\n",
    "        \n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.878079Z",
     "iopub.status.busy": "2021-02-25T07:28:57.877890Z",
     "iopub.status.idle": "2021-02-25T07:28:57.891791Z",
     "shell.execute_reply": "2021-02-25T07:28:57.891227Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.878055Z"
    }
   },
   "outputs": [],
   "source": [
    "def l2norm(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        l2 = np.sum(x**2)\n",
    "    else:\n",
    "        l2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=x)\n",
    "    \n",
    "    return l2\n",
    "\n",
    "def cosine_similarity(y_true, y_pred, axis=1):\n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "#     l2norm1 = tf.reduce_sum(tf.square(y_true), axis=axis)\n",
    "#     l2norm2 = tf.reduce_sum(tf.square(y_pred), axis=axis)\n",
    "    l2norm1 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_true)\n",
    "    l2norm2 = tf.map_fn(lambda t: tf.reduce_sum(tf.square(t)), elems=y_pred)\n",
    "    yy = tf.reduce_sum(tf.multiply(y_true, y_pred), axis=axis)\n",
    "        \n",
    "    return -tf.divide(yy, tf.multiply(tf.sqrt(l2norm1), tf.sqrt(l2norm2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Losses\n",
    "- Inherits From [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss)\n",
    "- Reference of Implement\n",
    "1. [BaseLossClass](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/losses.py#L47)\n",
    "2. [LossFunctionWrapper](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/losses.py#L213)\n",
    "3. [CategoricalCrossentropy](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/losses.py#L587-L662)\n",
    "- When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras`, `compile` and `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom training [tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) for more details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.892847Z",
     "iopub.status.busy": "2021-02-25T07:28:57.892657Z",
     "iopub.status.idle": "2021-02-25T07:28:57.910268Z",
     "shell.execute_reply": "2021-02-25T07:28:57.909729Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.892823Z"
    }
   },
   "outputs": [],
   "source": [
    "class YZKError(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None, \n",
    "                 penalized=None):\n",
    "    \n",
    "        \"\"\" \n",
    "            Initializes `YZKError` instance. get_config() need   \n",
    "\n",
    "            Args:\n",
    "              reduction: (Optional) Type of `tf.keras.losses.Reduction` to apply to\n",
    "                loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
    "                option will be determined by the usage context. For almost all cases\n",
    "                this defaults to `SUM_OVER_BATCH_SIZE`. \n",
    "              name: Optional name for the op. \n",
    "        \"\"\"\n",
    "\n",
    "        super(YZKError, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "        self.penalized = penalized\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        element_weight = self.element_weight\n",
    "\n",
    "#         logging.info(self.reduction, element_weight)\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        batchsize = y_pred.shape[0] \n",
    "        assert batchsize == y_true.shape[0]\n",
    "        \n",
    "#         logging.info(\"YZKError, shape of y_true = {}, y_pred = {}\".format(y_true.shape, y_pred.shape))\n",
    "        \n",
    "        PLoss = 0.    \n",
    "        penalized = self.penalized\n",
    "        if penalized is not None:\n",
    "            \n",
    "            _y_pred = tf.convert_to_tensor(y_pred[:, penalized])\n",
    "            _y_true = tf.convert_to_tensor(y_true[:, penalized])\n",
    "            \n",
    "            _y_pred = tf.reshape(_y_pred, [-1, 1])\n",
    "            _y_true = tf.reshape(_y_true, [-1, 1])\n",
    "#                 PLoss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true[:, penalized], y_pred[:, penalized])\n",
    "            PLoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(_y_true, _y_pred)\n",
    "        \n",
    "#         HuberLoss = 0\n",
    "#         MAELoss = 0\n",
    "#         if element_weight is not None:\n",
    "#             element_weight = tf.convert_to_tensor(element_weight)\n",
    "#             if element_weight.shape != []:  # is not a scale\n",
    "#                 nelement = y_pred.shape[1]\n",
    "#                 assert nelement == y_true.shape[1] \n",
    "#                 element_weight = tf.broadcast_to(element_weight, [batchsize, nelement])\n",
    "                \n",
    "#             y_pred_ = tf.math.multiply(y_pred, element_weight)\n",
    "#             y_true_ = tf.math.multiply(y_true, element_weight)\n",
    "        \n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true_, y_pred_)\n",
    "#         else:\n",
    "#             HuberLoss = tf.losses.Huber(reduction=self.reduction, delta=0.5)(y_true, y_pred)\n",
    "        \n",
    "        MAELoss = tf.losses.MeanAbsoluteError(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "\n",
    "    #         CosSimLoss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "        CosSimLoss = tf.losses.CosineSimilarity(reduction=self.reduction)(y_true, y_pred)\n",
    "\n",
    "#         logging.info(\"YZKError, PLoss: {}\\n, HuberLoss: {}\\n, MAELoss: {}\\n, CosSimLoss: {}\\n\".format(PLoss, HuberLoss, MAELoss, CosSimLoss))\n",
    "        \n",
    "        if penalized is not None:\n",
    "            return tf.math.add(tf.math.add(tf.math.scalar_mul(3, PLoss), tf.math.scalar_mul(2, MAELoss)), tf.math.scalar_mul(1, CosSimLoss))\n",
    "        else:\n",
    "            return tf.math.add(tf.math.scalar_mul(2, MAELoss), tf.math.scalar_mul(1, CosSimLoss))\n",
    "        \n",
    "    def get_config(self):\n",
    "        \"\"\"Returns the config dictionary for a `Loss` instance.\"\"\"\n",
    "        return {'reduction': self.reduction, 'name': self.name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inverse transform sigmoid(x)\n",
    "<center>\n",
    "<font size=4>\n",
    "$\n",
    "\\begin{align}\n",
    "\\sigma &=\\frac{1}{1+e^{-x}} \\\\\n",
    "\\frac{1}{\\sigma} &= 1+e^{-x} \\\\\n",
    "\\frac{1}{\\sigma}-1 &= e^{-x} \\\\\n",
    "\\log{\\frac{1-\\sigma}{\\sigma}} &= \\log{e^{-x}} \\\\\n",
    "x &= -\\log{\\frac{1-\\sigma}{\\sigma}} \\\\\n",
    "x &= \\log{\\frac{\\sigma}{1-\\sigma}}\n",
    "\\end{align}\n",
    "$\n",
    "</font size>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Crossentropy\n",
    "\n",
    "<center><font size=4>$BCE=y_{true}\\cdot\\log{y_{pred}}+\\left(1-y_{true}\\right)\\cdot\\log{\\left(1-y_{pred}\\right)}$</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.911263Z",
     "iopub.status.busy": "2021-02-25T07:28:57.911075Z",
     "iopub.status.idle": "2021-02-25T07:28:57.925073Z",
     "shell.execute_reply": "2021-02-25T07:28:57.924519Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.911239Z"
    }
   },
   "outputs": [],
   "source": [
    "class WeightedBinaryCrossntropy(Loss):\n",
    "    def __init__(self,\n",
    "                 reduction=losses_utils.ReductionV2.AUTO,\n",
    "                 name=None,\n",
    "                 element_weight=None):\n",
    "    \n",
    "        \"\"\" \n",
    "            Args:\n",
    "                element_weight: a weight list [weight for y_true=1, weight for y_true=0]\n",
    "            Ex.\n",
    "                WeightedBinaryCrossntropy(reduction=tf.losses.Reduction.NONE, element_weight=[5, 1])(y_true, y_pred)\n",
    "        \"\"\"\n",
    "\n",
    "        super(WeightedBinaryCrossntropy, self).__init__(name=name, reduction=reduction)\n",
    "        self.element_weight = element_weight\n",
    "    \n",
    "    def call(self, y_true, y_pred, from_logits=False):\n",
    "        \n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        \n",
    "        element_weight = self.element_weight\n",
    "        \n",
    "#         print(element_weight)\n",
    "        \n",
    "        if not from_logits:  # after activation\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "            y_pred = tf.math.log(y_pred / (1 - y_pred))\n",
    "            y_pred = tf.math.sigmoid(y_pred)\n",
    "            \n",
    "        if element_weight is None:\n",
    "            bc = -(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        else:\n",
    "            bc = -(y_true * tf.math.log(y_pred) * element_weight[0] + (1 - y_true) * tf.math.log(1 - y_pred) * element_weight[1])\n",
    "\n",
    "        return tf.math.reduce_mean(bc, axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T08:22:32.495039Z",
     "iopub.status.busy": "2021-02-08T08:22:32.494808Z",
     "iopub.status.idle": "2021-02-08T08:22:32.497438Z",
     "shell.execute_reply": "2021-02-08T08:22:32.496949Z",
     "shell.execute_reply.started": "2021-02-08T08:22:32.495013Z"
    }
   },
   "source": [
    "# Customized Layers\n",
    "- [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
    "- [masking_and_padding](https://www.tensorflow.org/guide/keras/masking_and_padding)\n",
    "- [Masking](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking)\n",
    "- Layers are recursively composable</br>\n",
    "If you assign a Layer instance as an attribute of another Layer, <font color=\"red\">the outer layer will start tracking the weights of the inner layer.</font></br>\n",
    "We recommend creating such sublayers in the <font color=\"gray\">\\_\\_init\\_\\_()</font> method (since the sublayers will typically have a build method, they will be built when the outer layer gets built).\n",
    "- You can optionally enable serialization on your layers  \n",
    "If you need your custom layers to be serializable as part of a Functional model, you can optionally implement a <font color=\"gray\">get_config()</font> method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T08:32:30.494670Z",
     "iopub.status.busy": "2021-02-25T08:32:30.494432Z",
     "iopub.status.idle": "2021-02-25T08:32:30.498284Z",
     "shell.execute_reply": "2021-02-25T08:32:30.497666Z",
     "shell.execute_reply.started": "2021-02-25T08:32:30.494645Z"
    }
   },
   "source": [
    "# Reference\n",
    "- [Weight initialization & Batch Normalization](https://reurl.cc/9ZRela)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:56:49.830728Z",
     "iopub.status.busy": "2021-02-25T07:56:49.830461Z",
     "iopub.status.idle": "2021-02-25T07:56:49.838045Z",
     "shell.execute_reply": "2021-02-25T07:56:49.837479Z",
     "shell.execute_reply.started": "2021-02-25T07:56:49.830699Z"
    }
   },
   "outputs": [],
   "source": [
    "class TPCNN1D(Layer):\n",
    "\n",
    "    '''\n",
    "        temporal pattern 1d-CNN\n",
    "        input shape = [None, timesteps, # of cells], channel is timesteps\n",
    "        output shape = [None, features, filters] or [None, m, k]\n",
    "        \n",
    "        Layers are recursively composable\n",
    "            If you assign a Layer instance as an attribute of another Layer, \n",
    "            the outer layer will start tracking the weights of the inner layer.\n",
    "            We recommend creating such sublayers in the __init__() method (since the sublayers will typically have a build method, t\n",
    "            hey will be built when the outer layer gets built).\n",
    "    '''\n",
    "    def __init__(self, filters, name=\"TPCNN1D\", **kwargs):\n",
    "        super(TPCNN1D, self).__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        self.cnn1d = Conv1D(kernel_size=1, filters=self.filters, data_format='channels_first', name=self.name)\n",
    "\n",
    "        \n",
    "#     def build(self, input_shape):\n",
    "#         self.cnn1d = Conv1D(kernel_size=1, filters=self.filters, data_format='channels_first', name=self.name)\n",
    "#         self.cnn1d.build(input_shape)\n",
    "#         self._trainable_weights = self.cnn1d.trainable_weights\n",
    "        \n",
    "#         super(TPCNN1D, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "#         print(self.cnn1d(x))\n",
    "#         return self.cnn1d(x)\n",
    "        # if channel first, then output shape = [None, filters, features] that need to transpose to [None, features, filters]\n",
    "        return tf.transpose(self.cnn1d(x), perm=[0, 2, 1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"filters\": self.filters}\n",
    "        base_config = super(TPCNN1D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.968467Z",
     "iopub.status.busy": "2021-02-25T07:28:57.968261Z",
     "iopub.status.idle": "2021-02-25T07:28:57.982919Z",
     "shell.execute_reply": "2021-02-25T07:28:57.982366Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.968442Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):  \n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1], units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.W)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:57.984184Z",
     "iopub.status.busy": "2021-02-25T07:28:57.983989Z",
     "iopub.status.idle": "2021-02-25T07:28:58.003853Z",
     "shell.execute_reply": "2021-02-25T07:28:58.003338Z",
     "shell.execute_reply.started": "2021-02-25T07:28:57.984160Z"
    }
   },
   "outputs": [],
   "source": [
    "class TPAttention(Layer):\n",
    "    def __init__(self, timesteps=None, features=None,\n",
    "                 W_regularizer=None, W_constraint=None, name=\"TPAttention\", **kwargs):\n",
    "        '''     \n",
    "            Temporal Pattern Attention [https://arxiv.org/abs/1809.04206]             \n",
    "                \n",
    "                - k: filters or timesteps - 1 \n",
    "                - m: features\n",
    "            \n",
    "            Keras Layer that implements an Attention mechanism for temporal data.\n",
    "            Supports Masking. \n",
    "            \n",
    "            Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "            # Input shape\n",
    "                3D tensor with shape: `(samples, steps, features)`.\n",
    "            # Output shape\n",
    "                2D tensor with shape: `(samples, features)`.\n",
    "                \n",
    "            :param kwargs:\n",
    "            Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "            The dimensions are inferred based on the output shape of the RNN.\n",
    "            Example:\n",
    "                model.add(LSTM(64, return_sequences=True))\n",
    "                model.add(Attention())\n",
    "            \n",
    "        '''\n",
    "        super(TPAttention, self).__init__(name=name, **kwargs)  # inherits from Layer\n",
    "\n",
    "#         self.Linear()\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "\n",
    "        self.timesteps = timesteps\n",
    "        self.features = features\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''\n",
    "            deferring weight creation until the shape of the inputs is known\n",
    "            input_shape[-1] is the number of features if len(input_shape) == 3, [batchsize, timestep, # of feature]\n",
    "        '''\n",
    "\n",
    "        if self.timesteps is None:  # k + 1\n",
    "            self.timesteps = input_shape[-1] + 1\n",
    "            \n",
    "        if self.features is None:  # m\n",
    "            self.features = input_shape[-2]   \n",
    "\n",
    "        # shape of W = [k, m]\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], self.features,),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        \n",
    "        # shape of W_h = [m, m]        \n",
    "        self.W_h = self.add_weight(shape=(self.features, self.features,),\n",
    "                                   initializer=self.init,\n",
    "                                   name='{}_Wh'.format(self.name))\n",
    "        \n",
    "        # shape of W_v = [m, k]\n",
    "        self.W_v = self.add_weight(shape=(self.features, input_shape[-1],),\n",
    "                                   initializer=self.init,\n",
    "                                   name='{}_Wv'.format(self.name))\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, h_t, mask=None):\n",
    "        '''\n",
    "            The __call__() method of your layer will automatically run build the first time it is called. \n",
    "            You now have a layer that's lazy and thus easier to use\n",
    "            \n",
    "            Notations:\n",
    "                - H^C (= x): Convlutional operations on return sequence of LSTM\n",
    "                - h_t: hidden state of current time\n",
    "                - k: filters or timesteps - 1 \n",
    "                - m: features\n",
    "                \n",
    "            x.shape = [None, k, m]\n",
    "            h_t.shape = [None, m] \n",
    "        '''        \n",
    "\n",
    "        features = self.features\n",
    "        timesteps = self.timesteps\n",
    "        \n",
    "        logging.info(\"features = {}\".format(features))\n",
    "        logging.info(\"timesteps = {}\".format(timesteps))\n",
    "        logging.info(\"x.shape (batchsize, m, k) = {}\".format(x.shape))\n",
    "        logging.info(\"W.shape (k, m) = {}\".format(self.W.shape))\n",
    "        logging.info(\"h_t.shape (batchsize, m) (need to expand dim) = {}\".format(h_t.shape))\n",
    "        \n",
    "        scored = tf.matmul(x, self.W)\n",
    "        scored = tf.matmul(scored, tf.expand_dims(h_t, axis=-1))\n",
    "        alpha_i = tf.math.sigmoid(scored)  # [None, k, 1]        \n",
    "        context_vector = tf.matmul(tf.ones((1, features)), tf.multiply(alpha_i, x))  # row summation\n",
    "        context_vector = tf.transpose(context_vector, perm=[0, 2, 1])  # [None, k, 1] \n",
    "        \n",
    "        logging.info(\"scored.shape (batchsize, m, 1) = {}\".format(scored.shape))\n",
    "        logging.info(\"alpha_i.shape (batchsize, m, 1) = {}\".format(alpha_i.shape))        \n",
    "        logging.info(\"shape of W_h (m, m) = {}, h_t (batchsize, m, 1) = {}\".format(self.W_h.shape, tf.reshape(h_t, (-1, features, 1)).shape))\n",
    "        logging.info(\"shape of W_v (m, k) = {}, context_vector (batchsize, k, 1) = {}\".format(self.W_v.shape, context_vector.shape))\n",
    "        return tf.reshape(tf.matmul(self.W_h, tf.reshape(h_t, (-1, features, 1))) + tf.matmul(self.W_v, context_vector), (-1, features))\n",
    "      \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "#         if mask is not None:\n",
    "#             # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "#             a *= tf.cast(mask, K.floatx())\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         #return input_shape[0], input_shape[-1]\n",
    "#         return input_shape[0], self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\"timesteps\": self.timesteps, \n",
    "                  \"features\": self.features, \n",
    "                  \"W_regularizer\": self.W_regularizer, \n",
    "                  \"W_constraint\": self.W_constraint}\n",
    "        base_config = super(TPAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:58.004884Z",
     "iopub.status.busy": "2021-02-25T07:28:58.004700Z",
     "iopub.status.idle": "2021-02-25T07:28:58.016217Z",
     "shell.execute_reply": "2021-02-25T07:28:58.015637Z",
     "shell.execute_reply.started": "2021-02-25T07:28:58.004860Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChangeableLossw(Callback):\n",
    "    def __init__(self, lossw, wmultiplier):\n",
    "    \n",
    "        self.nlossw = 2\n",
    "        self.lossw = lossw\n",
    "        self.wmultiplier = wmultiplier\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch <= 10:\n",
    "#             logf = \"epoch {}, \".format(epoch)\n",
    "            for idx in range(self.nlossw):\n",
    "                K.set_value(self.lossw[idx], K.get_value(self.lossw[idx]) * self.wmultiplier[idx])\n",
    "#                 logf += \"lossw_{} = {}, \".format(idx, K.get_value(self.lossw[idx]))\n",
    "#             logf += \"\\n\"\n",
    "        \n",
    "#             logging.info(logf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-08T07:20:58.949923Z",
     "iopub.status.busy": "2021-02-08T07:20:58.949700Z",
     "iopub.status.idle": "2021-02-08T07:20:58.952436Z",
     "shell.execute_reply": "2021-02-08T07:20:58.951909Z",
     "shell.execute_reply.started": "2021-02-08T07:20:58.949896Z"
    }
   },
   "source": [
    "# Neuron Network Builder\n",
    "- [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda)\n",
    "- [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) \n",
    "- [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional)\n",
    "- [Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:58.017523Z",
     "iopub.status.busy": "2021-02-25T07:28:58.017317Z",
     "iopub.status.idle": "2021-02-25T07:28:58.101178Z",
     "shell.execute_reply": "2021-02-25T07:28:58.100633Z",
     "shell.execute_reply.started": "2021-02-25T07:28:58.017497Z"
    }
   },
   "outputs": [],
   "source": [
    "class NNBuilder():\n",
    "    def __init__(self, modeld=\"model\", ckptd=\"ckpt\", name=\"NNBuilder\", optimizer=\"SGD\"):\n",
    "        \n",
    "        if not os.path.exists(modeld):\n",
    "            os.makedirs(modeld)\n",
    "            \n",
    "        if not os.path.exists(ckptd):\n",
    "            os.makedirs(ckptd)\n",
    "        \n",
    "        self.name = name\n",
    "        self.modeld = modeld\n",
    "        self.ckptd = ckptd\n",
    "        \n",
    "#         self.callbacks = self._callbacks(modeld, ckptd, name=name)\n",
    "#         self.optimizer = self._optimizer(name=optimizer)\n",
    "        \n",
    "    def setObjV(self, optimizer=None, callbacks=None):\n",
    "        if optimizer is None:\n",
    "            self.optimizer = self._optimizer()\n",
    "        else:\n",
    "            self.optimizer = self._optimizer(name=optimizer[\"name\"], lr=optimizer[\"lr\"])\n",
    "\n",
    "        if callbacks is None:\n",
    "            self.callbacks = self._callbacks()\n",
    "        else:\n",
    "            self.callbacks = self._callbacks(mmonitor=callbacks[\"mmonitor\"], \n",
    "                                             emonitor=callbacks[\"emonitor\"], \n",
    "                                             lmonitor=callbacks[\"lmonitor\"])    \n",
    "        \n",
    "    def CNN1D(self, filters, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None, optimizer=None, callbacks=None):\n",
    "        \n",
    "        self.name = \"CNN1D\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        _args       = NNBuilder._argreset(filters, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "                \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Conv1D(filters=units[i], kernel_size=3, strides=1, name=\"Conv1D_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "\n",
    "        x = MaxPool1D(pool_size=2)(x)\n",
    "        x = Flatten()(x)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(x)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(x)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"CNN1D\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"CNN1D\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def bidirectionalLSTM(self, cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0, merge_mode='concat', optimizer=None, callbacks=None):    \n",
    "        \n",
    "        '''\n",
    "            input: [batchsize, timesteps, nfeatures]\n",
    "            merge_mode: one of {'sum', 'mul', 'concat', 'ave', None}.\n",
    "            if merge_mode='concat', then shape of LSTM output is [timesteps, # of cells * 2 (directions)] \n",
    "        '''\n",
    "        \n",
    "        self.name = \"bidirectionalLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]          \n",
    "       \n",
    "        for idx in range(nlayer):\n",
    "            if idx == 0:  # the first hidden layer\n",
    "                x = Bidirectional(layer=LSTM(units[idx], return_sequences=True), \n",
    "                                  backward_layer=LSTM(units[idx], return_sequences=True, go_backwards=True), \n",
    "                                  merge_mode=merge_mode,\n",
    "                                  name=\"BLSTM_{}\".format(idx + 1))(inputs)\n",
    "            else:\n",
    "                x = Bidirectional(layer=LSTM(units[idx], return_sequences=True), \n",
    "                                  backward_layer=LSTM(units[idx], return_sequences=True, go_backwards=True), \n",
    "                                  merge_mode=merge_mode,\n",
    "                                  name=\"BLSTM_{}\".format(idx + 1))(x)\n",
    "\n",
    "            logging.info(\"{0:02d}, x.shape = {1}\".format(idx + 1, x.shape))\n",
    "\n",
    "#         x = tf.strided_slice(x, [0, 2, 0], [-1, 2, 4])\n",
    "\n",
    "        assert x.shape[1] == timesteps\n",
    "\n",
    "        H = Lambda(lambda x: x[:, 0:-1, :], name=\"H\")(x)\n",
    "        h_t = Lambda(lambda x: x[:, -1, :], name=\"h_t\")(x)\n",
    "        x = TPCNN1D(filters=units[-1], name=\"TPCNN1D\")(H)\n",
    "        x = TPAttention(name=\"TPAttention\")(x, h_t)\n",
    "\n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1\")(x)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2\")(x)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"bidirectionalLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"bidirectionalLSTM\")\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    def stackedLSTM(self, cells, inshape, outshape, outactfn=[\"sigmoid\"], dropout=0, recurrent_dropout=0, optimizer=None, callbacks=None):    \n",
    "        \n",
    "        '''\n",
    "            - dropout, applied to the first operation on the inputs\n",
    "            - recurrent_dropout, applied to the other operation on the recurrent inputs (previous output and/or states)\n",
    "        '''\n",
    "        \n",
    "        self.name = \"stackedLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        timesteps = inshape[0]\n",
    "        nfeatures = inshape[1]\n",
    "                \n",
    "        inputs = Input(inshape, name=\"input\")     \n",
    "            \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "            nlayer = len(cells)\n",
    "        else:\n",
    "            units = [cells]\n",
    "           \n",
    "        if nlayer > 1:\n",
    "            for idx in range(nlayer):\n",
    "                if idx == 0:  # the first hidden layer\n",
    "                    x = LSTM(units[idx], return_sequences=True, dropout=dropout, name=\"LSTM_{}\".format(idx + 1))(inputs) \n",
    "                elif idx == nlayer - 1:  # the last hidden layer\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, name=\"LSTM_{}\".format(idx + 1))(x) \n",
    "                else:\n",
    "                    x = LSTM(units[idx], recurrent_dropout=recurrent_dropout, return_sequences=True, name=\"LSTM_{}\".format(idx + 1))(x)\n",
    "        else:\n",
    "#             model.add(LSTM(units[0], input_shape=(timesteps, nfeatures), name=\"lstm\"))\n",
    "            x = LSTM(units[0], dropout=dropout, name=\"LSTM_1\")(inputs)\n",
    "                \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"stackedLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"stackedLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    def LSTMbasicAttention(self, shape, cells, optimizer=None, callbacks=None):\n",
    "        '''\n",
    "            shape = (timestep, feature)\n",
    "            return [model, optimizer, callbacks]\n",
    "        '''\n",
    "        \n",
    "        self.name = \"stackedLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        nfeatures = shape[1]\n",
    "        \n",
    "        inputs = Input(shape, name=\"input\")  # return a tensor\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(cells, list):\n",
    "            units = cells\n",
    "        else:\n",
    "            units = [cells]\n",
    "                \n",
    "        for idx, unit in enumerate(units):\n",
    "            if idx == 0:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(inputs)\n",
    "            else:\n",
    "                x = LSTM(unit, return_sequences=True, name=\"LSTM_{}\".format(idx))(x)\n",
    "            x = Attention(shape[0])(x)\n",
    "            \n",
    "        outputs = Dense(nfeatures)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "    \n",
    "    \n",
    "    def DNNLSTM(self, units, inshape, outshape, outactfn=[\"sigmoid\"], batchNormalization=True, dropouts=None, activations=None, optimizer=None, callbacks=None):\n",
    "        \n",
    "        '''\n",
    "            units: [units for Dense_1, ..., units for Dense_n, cells for LSTM_1], i.e. units[-1]: cells for LSTM_1\n",
    "            inshape: (timesteps, # of features)\n",
    "            outshape: an integer number for output layer (Dense), ex. 4\n",
    "            \n",
    "            NNB = NNBuilder()\n",
    "            model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 4], inshape=(10, 4), outshape=4, batchNormalization=None)\n",
    "            model.summary()\n",
    "            _________________________________________________________________\n",
    "            Model: \"DNNLSTM\"\n",
    "            _________________________________________________________________\n",
    "            Layer (type)                 Output Shape              Param #   \n",
    "            =================================================================\n",
    "            input (InputLayer)           [(None, 10, 4)]           0         \n",
    "            _________________________________________________________________\n",
    "            TDense_1 (TimeDistributed)   (None, 10, 10)            50         (4 * 10 + 10)\n",
    "            _________________________________________________________________\n",
    "            Activation_1 (TimeDistribute (None, 10, 10)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_2 (TimeDistributed)   (None, 10, 20)            220        (10 * 20 + 20)  \n",
    "            _________________________________________________________________\n",
    "            Activation_2 (TimeDistribute (None, 10, 20)            0         \n",
    "            _________________________________________________________________\n",
    "            TDense_3 (TimeDistributed)   (None, 10, 30)            630        (20 * 30 + 30)    \n",
    "            _________________________________________________________________\n",
    "            Activation_3 (TimeDistribute (None, 10, 30)            0         \n",
    "            _________________________________________________________________  ↓ (input gate, forget gate, output gate and neuron) \n",
    "            LSTM (LSTM)                  (None, 4)                 560        (4 * (30 * 4 + 4 + 4 * 4))\n",
    "            _________________________________________________________________                    ↑ (cell state pass to the other cells) \n",
    "            output (Dense)               (None, 4)                 20         (4 * 4 + 4)       \n",
    "            =================================================================\n",
    "            Total params: 1,480\n",
    "            Trainable params: 1,480\n",
    "            Non-trainable params: 0\n",
    "            \n",
    "            (4 * 10 + 10) + (10 * 20 + 20) + (20 * 30 + 30) + 4 * (30 * 4 + 4 + 4 * 4) + (4 * 4 + 4)   \n",
    "        '''\n",
    "        \n",
    "        self.name = \"DNNLSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        assert len(units) >= 2\n",
    "        \n",
    "        inputs = Input(inshape, name=\"input\")  \n",
    "\n",
    "        nlayer = len(units) - 1\n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "        else:\n",
    "            activations = [\"relu\" for _ in range(nlayer)]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = TimeDistributed(Dense(units[i]), name=\"TDense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = TimeDistributed(BatchNormalization(), name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = TimeDistributed(Activation(activations[i]), name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = TimeDistributed(Dropout(dropouts[i]), name=\"Dropout_{}\".format(i + 1))(x)\n",
    "        x = LSTM(units[-1], name=\"LSTM\")(x)\n",
    "        \n",
    "        \n",
    "        loss1 = Dense(units[-1], activation=\"relu\", name=\"LDense1_1\")(x)\n",
    "        loss1 = Dense(outshape[0] * 3, activation=\"relu\", name=\"LDense1_2\")(loss1)\n",
    "        loss1 = Dense(outshape[0], activation=outactfn[0], name=\"Loss1\")(loss1)\n",
    "        \n",
    "        if len(outactfn) == len(outshape) == 2:\n",
    "            loss2 = Dense(units[-1], activation=\"relu\", name=\"LDense2_1\")(x)\n",
    "            loss2 = Dense(outshape[1] * 3, activation=\"relu\", name=\"LDense2_2\")(loss2)\n",
    "            loss2 = Dense(outshape[1], activation=outactfn[1], name=\"Loss2\")(loss2)\n",
    "            model = Model(inputs=inputs, outputs=[loss1, loss2], name=\"DNNLSTM\")\n",
    "        else:    \n",
    "            model = Model(inputs=inputs, outputs=loss1, name=\"DNNLSTM\")\n",
    "        \n",
    "        return [model, self.callbacks, self.optimizer]\n",
    "\n",
    "    \n",
    "    def TPALSTM(self, optimizer=None, callbacks=None):\n",
    "        \n",
    "        self.name = \"TPALSTM\"\n",
    "        self.setObjV(optimizer, callbacks)\n",
    "        \n",
    "        embedding_layer = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        \n",
    "        lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "        sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "        x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "        sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "        y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "        merged = concatenate([x1, y1])\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        merged = Dense(num_dense, activation=act)(merged)\n",
    "        merged = Dropout(rate_drop_dense)(merged)\n",
    "        merged = BatchNormalization()(merged)\n",
    "\n",
    "        preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "        ########################################\n",
    "        ## add class weight\n",
    "        ########################################\n",
    "        if re_weight:\n",
    "            class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "        else:\n",
    "            class_weight = None\n",
    "\n",
    "        ########################################\n",
    "        ## train the model\n",
    "        ########################################\n",
    "        model = Model(inputs=[sequence_1_input, sequence_2_input], \\\n",
    "                outputs=preds)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                optimizer='nadam',\n",
    "                metrics=['acc'])\n",
    "        #model.summary()\n",
    "        print(STAMP)\n",
    "\n",
    "        early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "        bst_model_path = STAMP + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "        hist = model.fit([data_1_train, data_2_train], labels_train, \\\n",
    "                validation_data=([data_1_val, data_2_val], labels_val, weight_val), \\\n",
    "                epochs=200, batch_size=2048, shuffle=True, \\\n",
    "                class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "        model.load_weights(bst_model_path)\n",
    "        bst_val_score = min(hist.history['val_loss'])\n",
    "\n",
    "        ########################################\n",
    "        ## make the submission\n",
    "        ########################################\n",
    "        print('Start making the submission before fine-tuning')\n",
    "\n",
    "        preds = model.predict([test_data_1, test_data_2], batch_size=8192, verbose=1)\n",
    "        preds += model.predict([test_data_2, test_data_1], batch_size=8192, verbose=1)\n",
    "        preds /= 2\n",
    "\n",
    "        submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "        submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def DenseBuilder(units, inputs, batchNormalization=True, dropouts=None, activations=None, optimizer=None, callbacks=None):\n",
    "        \n",
    "        _args       = NNBuilder._argreset(units, dropouts=dropouts, activations=activations)\n",
    "        units       = _args[\"units\"]\n",
    "        nlayer      = _args[\"nlayer\"]\n",
    "        dropouts    = _args[\"dropouts\"]\n",
    "        activations = _args[\"activations\"]\n",
    "        \n",
    "        for i in range(nlayer):\n",
    "            if i == 0:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(inputs)\n",
    "            else:\n",
    "                x = Dense(units[i], name=\"Dense_{}\".format(i + 1))(x)\n",
    "            if batchNormalization:\n",
    "                x = BatchNormalization(name=\"BatchNormalization_{}\".format(i + 1))(x)\n",
    "            x = Activation(activations[i], name=\"Activation_{}\".format(i + 1))(x)\n",
    "            if dropouts is not None:\n",
    "                x = Dropout(dropouts[i], name=\"Dropout_{}\".format(i + 1))(x)\n",
    "    \n",
    "        return x\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _argreset(units, dropouts=None, activations=None):\n",
    "        \n",
    "        _args = dict()\n",
    "        \n",
    "        nlayer = 1\n",
    "        if isinstance(units, list):\n",
    "            nlayer = len(units)\n",
    "        else:\n",
    "            units = [units]\n",
    "        \n",
    "        _args[\"units\"] = units\n",
    "        _args[\"nlayer\"] = nlayer\n",
    "        \n",
    "        if dropouts is not None:\n",
    "            if isinstance(dropouts, list):\n",
    "                assert nlayer == len(dropouts)\n",
    "            else:\n",
    "                dropouts = [dropouts for _ in range(nlayer)]\n",
    "            _args[\"dropouts\"] = dropouts\n",
    "        else:\n",
    "            _args[\"dropouts\"] = None\n",
    "        \n",
    "        if activations is not None:\n",
    "            if isinstance(activations, list):\n",
    "                assert nlayer == len(activations)\n",
    "            else:\n",
    "                activations = [activations for _ in range(nlayer)]\n",
    "            _args[\"activations\"] = activations\n",
    "        else:\n",
    "            _args[\"activations\"] = None\n",
    "#             activations = [\"relu\" for _ in range(nlayer)]\n",
    "            \n",
    "        return _args\n",
    "\n",
    "    \n",
    "    def _callbacks(self, mmonitor=\"val_loss\", emonitor=\"loss\", lmonitor=\"val_loss\"):\n",
    "\n",
    "#     def _callbacks(modeld, ckptd, mmonitor=\"val_loss\", emonitor=\"loss\", lmonitor=\"val_loss\", name=\"ckpt\"):\n",
    "        \n",
    "        '''\n",
    "            mmonitor: monitor for model \n",
    "            emonitor: monitor for earlystopping\n",
    "            lmonitor: monitor for learning rate\n",
    "        '''\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "        \n",
    "        _name = \"{epoch:04d}_{loss:.3f}_{val_loss:.3f}\"\n",
    "#         checkpointer = ModelCheckpoint(filepath=os.path.join(modeld, \"{0}_{1}_{2}.hdf5\".format(name, _name, timestamp)),\n",
    "        checkpointer = ModelCheckpoint(filepath=os.path.join(self.modeld, \"{0}.hdf5\".format(self.name)),\n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True, \n",
    "                                       monitor=mmonitor)\n",
    "        \n",
    "        earlystopper = EarlyStopping(monitor=emonitor, patience=10)\n",
    "\n",
    "        reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.5, patience=10, min_lr=0.0001)\n",
    "\n",
    "#         reduceLR = ReduceLROnPlateau(monitor=lmonitor, factor=0.9, patience=10, min_lr=0.0001)\n",
    "        \n",
    "        tb = TensorBoard(log_dir=self.ckptd)\n",
    "\n",
    "        csvlogger = CSVLogger(os.path.join(self.ckptd, \"{}_{}.log\".format(self.name, timestamp)), append=False, separator=\",\")\n",
    "\n",
    "        # Learning rate schedule.\n",
    "    #     lr_schedule = LearningRateScheduler(fixed_schedule, verbose=0)\n",
    "\n",
    "        return [checkpointer, earlystopper, reduceLR, tb, csvlogger]\n",
    "\n",
    "#         return [checkpointer, earlystopper, reduceLR, csvlogger]\n",
    "    \n",
    "    def _optimizer(self, lr=1e-3, name=\"Adam\"):\n",
    "#     def _optimizer(lr=1e-2, name=\"Adam\"):\n",
    "        if name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "        else:\n",
    "            optimizer = Adam(learning_rate=lr)\n",
    "        return optimizer\n",
    "    \n",
    "    @staticmethod\n",
    "    def mloader(filepath, custom_objects=None):\n",
    "        if custom_objects is not None:\n",
    "            return load_model(filepath, custom_objects=custom_objects)\n",
    "        else:\n",
    "            return load_model(filepath)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:28:58.102137Z",
     "iopub.status.busy": "2021-02-25T07:28:58.101956Z",
     "iopub.status.idle": "2021-02-25T07:28:58.130076Z",
     "shell.execute_reply": "2021-02-25T07:28:58.129583Z",
     "shell.execute_reply.started": "2021-02-25T07:28:58.102114Z"
    }
   },
   "outputs": [],
   "source": [
    "class MArgs(object):\n",
    "    def __init__(self, mname, testf=None, train=None):\n",
    "        self.mname = mname\n",
    "        self.testf = testf\n",
    "        self.train = train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:56:56.605669Z",
     "iopub.status.busy": "2021-02-25T07:56:56.605397Z",
     "iopub.status.idle": "2021-02-25T07:58:15.821094Z",
     "shell.execute_reply": "2021-02-25T07:58:15.820346Z",
     "shell.execute_reply.started": "2021-02-25T07:56:56.605637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "****** epochs = 100, batch_size = 100, nsize = 10000, nstep = 6, nfeature = 5, ntarget = 4, nclass = 5\n",
      "****** X_train.shape = (10000, 6, 5), y_train[0].shape = (10000, 4), y_train[1].shape = (10000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:56:57,047, bidirectionalLSTM-103-INFO: 01, x.shape = (None, 6, 20)\n",
      "2021-02-25 15:56:57,429, bidirectionalLSTM-103-INFO: 02, x.shape = (None, 6, 40)\n",
      "2021-02-25 15:56:57,498, converted_call-603-INFO: features = 40\n",
      "2021-02-25 15:56:57,512, converted_call-603-INFO: timesteps = 21\n",
      "2021-02-25 15:56:57,526, converted_call-603-INFO: x.shape (batchsize, m, k) = (None, 40, 20)\n",
      "2021-02-25 15:56:57,539, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-02-25 15:56:57,554, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (None, 40)\n",
      "2021-02-25 15:56:57,573, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (None, 40, 1)\n",
      "2021-02-25 15:56:57,587, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (None, 40, 1)\n",
      "2021-02-25 15:56:57,601, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (None, 40, 1)\n",
      "2021-02-25 15:56:57,615, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (None, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"TPCNN1D/TPCNN1D/add_2:0\", shape=(None, 20, 40), dtype=float32)\n",
      "Model: \"bidirectionalLSTM\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 6, 5)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "BLSTM_1 (Bidirectional)         (None, 6, 20)        1280        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "BLSTM_2 (Bidirectional)         (None, 6, 40)        6560        BLSTM_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "H (Lambda)                      (None, 5, 40)        0           BLSTM_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "TPCNN1D (TPCNN1D)               (None, 40, 20)       120         H[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "h_t (Lambda)                    (None, 40)           0           BLSTM_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "TPAttention (TPAttention)       (None, 40)           3200        TPCNN1D[0][0]                    \n",
      "                                                                 h_t[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "LDense1 (Dense)                 (None, 20)           820         TPAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LDense2 (Dense)                 (None, 20)           820         TPAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Loss1 (Dense)                   (None, 4)            84          LDense1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Loss2 (Dense)                   (None, 5)            105         LDense2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,989\n",
      "Trainable params: 12,989\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "      0  1  2  3  4\n",
      "0     0  0  0  0  1\n",
      "1     0  0  1  0  0\n",
      "2     0  0  0  0  1\n",
      "3     0  0  0  1  0\n",
      "4     0  1  0  0  0\n",
      "...  .. .. .. .. ..\n",
      "9995  0  1  0  0  0\n",
      "9996  1  0  0  0  0\n",
      "9997  0  0  0  1  0\n",
      "9998  0  0  0  0  1\n",
      "9999  0  0  0  1  0\n",
      "\n",
      "[10000 rows x 5 columns]\n",
      "(10000, 6, 5) (10000, 4) (10000, 5)\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:56:58,850, converted_call-603-INFO: features = 40\n",
      "2021-02-25 15:56:58,864, converted_call-603-INFO: timesteps = 21\n",
      "2021-02-25 15:56:58,879, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2021-02-25 15:56:58,893, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-02-25 15:56:58,906, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2021-02-25 15:56:58,927, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:56:58,943, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:56:58,959, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:56:58,974, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bidirectionalLSTM/TPCNN1D/TPCNN1D/add:0\", shape=(100, 20, 40), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:57:00,969, converted_call-603-INFO: features = 40\n",
      "2021-02-25 15:57:00,985, converted_call-603-INFO: timesteps = 21\n",
      "2021-02-25 15:57:00,999, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2021-02-25 15:57:01,013, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-02-25 15:57:01,027, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2021-02-25 15:57:01,048, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:57:01,063, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:57:01,079, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:57:01,093, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bidirectionalLSTM/TPCNN1D/TPCNN1D/add:0\", shape=(100, 20, 40), dtype=float32)\n",
      " 2/70 [..............................] - ETA: 8s - loss: 1.8618 - Loss1_loss: 0.2458 - Loss2_loss: 1.6161WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.2349s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:57:03,294, _call_batch_end_hook-328-WARNING: Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.2349s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/70 [===========================>..] - ETA: 0s - loss: 1.8631 - Loss1_loss: 0.2511 - Loss2_loss: 1.6120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:57:04,729, converted_call-603-INFO: features = 40\n",
      "2021-02-25 15:57:04,745, converted_call-603-INFO: timesteps = 21\n",
      "2021-02-25 15:57:04,761, converted_call-603-INFO: x.shape (batchsize, m, k) = (100, 40, 20)\n",
      "2021-02-25 15:57:04,776, converted_call-603-INFO: W.shape (k, m) = (20, 40)\n",
      "2021-02-25 15:57:04,791, converted_call-603-INFO: h_t.shape (batchsize, m) (need to expand dim) = (100, 40)\n",
      "2021-02-25 15:57:04,812, converted_call-603-INFO: scored.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:57:04,828, converted_call-603-INFO: alpha_i.shape (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:57:04,843, converted_call-603-INFO: shape of W_h (m, m) = (40, 40), h_t (batchsize, m, 1) = (100, 40, 1)\n",
      "2021-02-25 15:57:04,858, converted_call-603-INFO: shape of W_v (m, k) = (40, 20), context_vector (batchsize, k, 1) = (100, 20, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bidirectionalLSTM/TPCNN1D/TPCNN1D/add:0\", shape=(100, 20, 40), dtype=float32)\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 1.8628 - Loss1_loss: 0.2509 - Loss2_loss: 1.6119 - val_loss: 1.8596 - val_Loss1_loss: 0.2506 - val_Loss2_loss: 1.6090\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8606 - Loss1_loss: 0.2508 - Loss2_loss: 1.6098 - val_loss: 1.8599 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6093\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8603 - Loss1_loss: 0.2507 - Loss2_loss: 1.6097 - val_loss: 1.8598 - val_Loss1_loss: 0.2506 - val_Loss2_loss: 1.6092\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8601 - Loss1_loss: 0.2507 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6091\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8602 - Loss1_loss: 0.2508 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6091\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8600 - Loss1_loss: 0.2506 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6091\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8600 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8599 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6091\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8601 - Loss1_loss: 0.2506 - Loss2_loss: 1.6095 - val_loss: 1.8598 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6090\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8601 - Loss1_loss: 0.2506 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6091\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8600 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6091\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8600 - Loss1_loss: 0.2506 - Loss2_loss: 1.6094 - val_loss: 1.8597 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6090\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8598 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8599 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8599 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6090\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8599 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8599 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8599 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8599 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8597 - val_Loss1_loss: 0.2507 - val_Loss2_loss: 1.6090\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8599 - Loss1_loss: 0.2505 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8600 - Loss1_loss: 0.2505 - Loss2_loss: 1.6095 - val_loss: 1.8600 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6090\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8597 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6089\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6089\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8597 - Loss1_loss: 0.2504 - Loss2_loss: 1.6093 - val_loss: 1.8597 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6089\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8598 - Loss1_loss: 0.2504 - Loss2_loss: 1.6094 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8596 - Loss1_loss: 0.2504 - Loss2_loss: 1.6092 - val_loss: 1.8597 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6088\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8596 - Loss1_loss: 0.2504 - Loss2_loss: 1.6093 - val_loss: 1.8597 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6089\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8595 - Loss1_loss: 0.2504 - Loss2_loss: 1.6091 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6089\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8595 - Loss1_loss: 0.2504 - Loss2_loss: 1.6092 - val_loss: 1.8597 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6088\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8595 - Loss1_loss: 0.2504 - Loss2_loss: 1.6091 - val_loss: 1.8598 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6089\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8594 - Loss1_loss: 0.2503 - Loss2_loss: 1.6091 - val_loss: 1.8597 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6088\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8593 - Loss1_loss: 0.2503 - Loss2_loss: 1.6090 - val_loss: 1.8596 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6087\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8596 - Loss1_loss: 0.2503 - Loss2_loss: 1.6092 - val_loss: 1.8597 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6088\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8594 - Loss1_loss: 0.2503 - Loss2_loss: 1.6091 - val_loss: 1.8598 - val_Loss1_loss: 0.2508 - val_Loss2_loss: 1.6090\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8593 - Loss1_loss: 0.2504 - Loss2_loss: 1.6089 - val_loss: 1.8596 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6088\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8592 - Loss1_loss: 0.2504 - Loss2_loss: 1.6088 - val_loss: 1.8598 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6090\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8591 - Loss1_loss: 0.2504 - Loss2_loss: 1.6088 - val_loss: 1.8594 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6086\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8589 - Loss1_loss: 0.2503 - Loss2_loss: 1.6086 - val_loss: 1.8594 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6086\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8588 - Loss1_loss: 0.2503 - Loss2_loss: 1.6085 - val_loss: 1.8597 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6088\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8588 - Loss1_loss: 0.2503 - Loss2_loss: 1.6084 - val_loss: 1.8597 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6088\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8587 - Loss1_loss: 0.2503 - Loss2_loss: 1.6084 - val_loss: 1.8596 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6087\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8588 - Loss1_loss: 0.2503 - Loss2_loss: 1.6084 - val_loss: 1.8594 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6086\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8585 - Loss1_loss: 0.2503 - Loss2_loss: 1.6082 - val_loss: 1.8597 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6087\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8584 - Loss1_loss: 0.2503 - Loss2_loss: 1.6081 - val_loss: 1.8596 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6087\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8583 - Loss1_loss: 0.2503 - Loss2_loss: 1.6080 - val_loss: 1.8598 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6088\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8582 - Loss1_loss: 0.2503 - Loss2_loss: 1.6079 - val_loss: 1.8597 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6087\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8583 - Loss1_loss: 0.2503 - Loss2_loss: 1.6080 - val_loss: 1.8598 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6089\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8582 - Loss1_loss: 0.2503 - Loss2_loss: 1.6079 - val_loss: 1.8597 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6088\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8582 - Loss1_loss: 0.2503 - Loss2_loss: 1.6078 - val_loss: 1.8599 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6090\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8580 - Loss1_loss: 0.2503 - Loss2_loss: 1.6076 - val_loss: 1.8599 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6090\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8580 - Loss1_loss: 0.2503 - Loss2_loss: 1.6077 - val_loss: 1.8598 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6089\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8579 - Loss1_loss: 0.2503 - Loss2_loss: 1.6076 - val_loss: 1.8601 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6091\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8579 - Loss1_loss: 0.2503 - Loss2_loss: 1.6076 - val_loss: 1.8598 - val_Loss1_loss: 0.2509 - val_Loss2_loss: 1.6088\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8578 - Loss1_loss: 0.2503 - Loss2_loss: 1.6075 - val_loss: 1.8601 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6091\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8577 - Loss1_loss: 0.2503 - Loss2_loss: 1.6074 - val_loss: 1.8599 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6089\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8577 - Loss1_loss: 0.2503 - Loss2_loss: 1.6074 - val_loss: 1.8600 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6090\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8577 - Loss1_loss: 0.2503 - Loss2_loss: 1.6074 - val_loss: 1.8597 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6087\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8577 - Loss1_loss: 0.2503 - Loss2_loss: 1.6073 - val_loss: 1.8599 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6089\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8575 - Loss1_loss: 0.2503 - Loss2_loss: 1.6072 - val_loss: 1.8600 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6090\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8575 - Loss1_loss: 0.2503 - Loss2_loss: 1.6072 - val_loss: 1.8600 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6090\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8574 - Loss1_loss: 0.2503 - Loss2_loss: 1.6071 - val_loss: 1.8600 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6091\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8573 - Loss1_loss: 0.2503 - Loss2_loss: 1.6070 - val_loss: 1.8601 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6091\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8573 - Loss1_loss: 0.2503 - Loss2_loss: 1.6070 - val_loss: 1.8601 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6091\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8572 - Loss1_loss: 0.2503 - Loss2_loss: 1.6069 - val_loss: 1.8600 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6090\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8576 - Loss1_loss: 0.2502 - Loss2_loss: 1.6074 - val_loss: 1.8604 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6094\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8571 - Loss1_loss: 0.2503 - Loss2_loss: 1.6068 - val_loss: 1.8604 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6094\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8569 - Loss1_loss: 0.2503 - Loss2_loss: 1.6066 - val_loss: 1.8603 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6093\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8570 - Loss1_loss: 0.2503 - Loss2_loss: 1.6067 - val_loss: 1.8602 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6092\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8568 - Loss1_loss: 0.2503 - Loss2_loss: 1.6065 - val_loss: 1.8602 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6092\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8568 - Loss1_loss: 0.2503 - Loss2_loss: 1.6066 - val_loss: 1.8603 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6093\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 1.8567 - Loss1_loss: 0.2503 - Loss2_loss: 1.6064 - val_loss: 1.8603 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6093\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8566 - Loss1_loss: 0.2503 - Loss2_loss: 1.6064 - val_loss: 1.8604 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6094\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8567 - Loss1_loss: 0.2503 - Loss2_loss: 1.6065 - val_loss: 1.8604 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6093\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8565 - Loss1_loss: 0.2503 - Loss2_loss: 1.6063 - val_loss: 1.8605 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6095\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8565 - Loss1_loss: 0.2503 - Loss2_loss: 1.6063 - val_loss: 1.8604 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6094\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8564 - Loss1_loss: 0.2502 - Loss2_loss: 1.6061 - val_loss: 1.8603 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6092\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8563 - Loss1_loss: 0.2502 - Loss2_loss: 1.6060 - val_loss: 1.8602 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6092\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8564 - Loss1_loss: 0.2503 - Loss2_loss: 1.6061 - val_loss: 1.8603 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6093\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8563 - Loss1_loss: 0.2502 - Loss2_loss: 1.6060 - val_loss: 1.8604 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6093\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8562 - Loss1_loss: 0.2502 - Loss2_loss: 1.6060 - val_loss: 1.8606 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6095\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8560 - Loss1_loss: 0.2502 - Loss2_loss: 1.6058 - val_loss: 1.8605 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6095\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8561 - Loss1_loss: 0.2502 - Loss2_loss: 1.6059 - val_loss: 1.8603 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6093\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8561 - Loss1_loss: 0.2502 - Loss2_loss: 1.6059 - val_loss: 1.8604 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6094\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8560 - Loss1_loss: 0.2502 - Loss2_loss: 1.6058 - val_loss: 1.8606 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6095\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8559 - Loss1_loss: 0.2502 - Loss2_loss: 1.6057 - val_loss: 1.8605 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6095\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8557 - Loss1_loss: 0.2502 - Loss2_loss: 1.6055 - val_loss: 1.8607 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6097\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 1.8556 - Loss1_loss: 0.2502 - Loss2_loss: 1.6054 - val_loss: 1.8606 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6096\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 1.8555 - Loss1_loss: 0.2502 - Loss2_loss: 1.6054 - val_loss: 1.8607 - val_Loss1_loss: 0.2510 - val_Loss2_loss: 1.6096\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    gpuId = 1\n",
    "    mname = \"bidirectionalLSTM\"\n",
    "    args = MArgs(mname, train=[100, 100, 10000, 6, 5, 4, 5])\n",
    "    \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(gpuId)  # only device you want to use can visible\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(gpus)\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], enable=True)\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    \n",
    "    if args.testf == \"_argreset\":\n",
    "        _args = NNBuilder._argreset([10, 20, 30], dropouts=0.5, activations=\"relu\")\n",
    "        print(_args)\n",
    "    \n",
    "    \n",
    "    if args.train is not None:\n",
    "        epochs, batch_size, nsize, nstep, nfeature, ntarget, nclass = args.train\n",
    "        X_train = np.random.random_sample((nsize, nstep, nfeature))\n",
    "        y_train = [np.random.random_sample((nsize, ntarget)), pd.get_dummies(pd.Series(np.random.randint(low=0, high=nclass, size=nsize)))]\n",
    "        print(\"****** epochs = {}, batch_size = {}, nsize = {}, nstep = {}, nfeature = {}, ntarget = {}, nclass = {}\".format(epochs, batch_size, nsize, nstep, nfeature, ntarget, nclass))\n",
    "        print(\"****** X_train.shape = {}, y_train[0].shape = {}, y_train[1].shape = {}\".format(X_train.shape, y_train[0].shape, y_train[1].shape))\n",
    "        \n",
    "    \n",
    "    if args.mname == \"DNNbuilder\":\n",
    "        inputs = Input(shape=(10), name=\"Input\")\n",
    "        x = NNBuilder.DenseBuilder([10, 30, 20, 40], inputs, dropouts=0.25, activations=\"relu\")\n",
    "        model = Model(inputs=inputs, outputs=x, name=\"DNN\")\n",
    "        model.summary()\n",
    "\n",
    "    elif args.mname == \"DNNLSTM\":\n",
    "        NNB = NNBuilder()\n",
    "        model, callbacks_, optimizer_ = NNB.DNNLSTM([10, 20, 30, 10], inshape=(6, 4), outshape=[4, 1], outactfn=[\"tanh\", \"sigmoid\"], batchNormalization=None)\n",
    "        model.summary()\n",
    "        plot_model(model, to_file=\"DNNLSTM.png\", show_shapes=True)\n",
    "\n",
    "        model.compile(loss={\"regression_output\": \"mae\", \"classification_output\": \"binary_crossentropy\"},\n",
    "                      metrics={\"regression_output\": \"mae\", \"classification_output\": \"accuracy\"},\n",
    "                      optimizer=optimizer_)\n",
    "\n",
    "        epochs = 1000\n",
    "        batch_size = 10\n",
    "        n = 10000\n",
    "        X_train = np.random.random_sample((n, 6, 4))\n",
    "        y_train = [np.random.random_sample((n, 4)), np.random.randint(low=0, high=2, size=(n, 1))]\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "    elif args.mname == \"stackedLSTM\":\n",
    "        stackedLSTM, callbacks_, optimizer_ = NNBuilder().stackedLSTM(cells=[10, 20], inshape=[6, 4], outshape=[4, 4], outactfn=[\"sigmoid\", \"softmax\"])\n",
    "        stackedLSTM.compile(loss=\"mae\", optimizer=optimizer_)\n",
    "        stackedLSTM.summary()\n",
    "\n",
    "        epochs = 3\n",
    "        batch_size = 200\n",
    "        X_train = np.random.random_sample((1000, 6, 4))\n",
    "        y_train = [np.random.random_sample((1000, 4)), np.random.randint(low=0, high=2, size=1000)]\n",
    "\n",
    "\n",
    "    #     history = stackedLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.1, verbose=2, shuffle=True)\n",
    "\n",
    "    #     saved_model = \"/home/yuzhe/DataScience/QC/model/lstm1_0154_0.009_0.008_202008071814.hdf5\"\n",
    "    #     model = NNBuilder.mloader(saved_model)\n",
    "\n",
    "    elif args.mname == \"CNN1D\":\n",
    "        CNN1D, callbacks_, optimizer_ = NNBuilder().CNN1D(filters=[10, 20], inshape=[6, 4], outshape=[2], outactfn=[\"sigmoid\"], activations=\"relu\")\n",
    "        CNN1D.summary()\n",
    "\n",
    "    #     train(X_train, y_train, 30, 5000, loss=YZKError(element_weight=[1 / 6., 1 / 6., 1 / 6., 1 / 2.]), name=\"NNBuilderTest1\")\n",
    "    #     train(X_train, y_train, 30, 5000, loss=YZKError(), name=\"NNBuilderTest\")\n",
    "\n",
    "    elif args.mname == \"bidirectionalLSTM\":\n",
    "        bLSTM, callbacks_, optimizer_ = NNBuilder().bidirectionalLSTM(merge_mode=\"concat\", cells=[10, 20], inshape=[nstep, nfeature], outshape=[ntarget, nclass], outactfn=[\"sigmoid\", \"softmax\"])\n",
    "\n",
    "        bLSTM.compile(loss={\"Loss1\": \"mae\", \"Loss2\": \"categorical_crossentropy\"}, optimizer=optimizer_)\n",
    "        print(y_train[1])\n",
    "        print(X_train.shape, y_train[0].shape, y_train[1].shape)\n",
    "        history = bLSTM.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks_, validation_split=0.3, verbose=1, shuffle=True)\n",
    "\n",
    "        \n",
    "#         (None, 4, 20) mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:29:31.870226Z",
     "iopub.status.busy": "2021-02-25T07:29:31.869997Z",
     "iopub.status.idle": "2021-02-25T07:29:31.873967Z",
     "shell.execute_reply": "2021-02-25T07:29:31.873393Z",
     "shell.execute_reply.started": "2021-02-25T07:29:31.870198Z"
    }
   },
   "outputs": [],
   "source": [
    "def logcosh(a, t):\n",
    "    return (1 / a) * np.log(np.cosh(a * t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-25T07:29:31.875019Z",
     "iopub.status.busy": "2021-02-25T07:29:31.874830Z",
     "iopub.status.idle": "2021-02-25T07:29:32.510968Z",
     "shell.execute_reply": "2021-02-25T07:29:32.510167Z",
     "shell.execute_reply.started": "2021-02-25T07:29:31.874996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 15:29:32,216, hrfgenerator-367-INFO: hrfgenerator-vnames-199: ['Temp', 'RH', 'Pres', 'Precp']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temp     RH    Pres  Precp\n",
      "0 -20.0    0.0   600.0    0.0\n",
      "1  50.0  100.0  1100.0  220.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/yuzhe/DataScience/dataset/hrf_2016010101_2016123124_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b712c943eefe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#              \"Pres\": [600.0, 1100.0],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#              \"Precp\": [0.0, 220.0]}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhrfgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtperiod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfnpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdatetimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DataScience/lib/dgenerator.py\u001b[0m in \u001b[0;36mhrfgenerator\u001b[0;34m(self, tperiod, n_in, n_out, t2last, mode, fnpy, rescale, reformat, vstack, dropnan, classify, generator, batchsize)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mdarray\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/hrf_{}_{}_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpyd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mqcdtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/qcdtime_{}_{}_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpyd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mstnids_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/stnid_{}_{}_{}.npy\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpyd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtperiod\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/yuzhe/DataScience/dataset/hrf_2016010101_2016123124_test.npy'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "#### check losses     \n",
    "    \n",
    "    from dgenerator import dgenerator\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    tperiod = [2016010101, 2016123124]\n",
    "    n_in = 6\n",
    "    n_out = 1\n",
    "    mode = \"test\"\n",
    "    vstack = True\n",
    "    fnpy = True\n",
    "    npyd = \"/home/yuzhe/DataScience/dataset\"\n",
    "    gif = \"/home/yuzhe/CODE/ProgramT1/GRDTools/SRC/RES/GI/1500_decode_stationlist_without_space.txt\"\n",
    "\n",
    "    dg = dgenerator(gif=gif, npyd=npyd)\n",
    "    vinfo = pd.DataFrame(dg.vrange)  \n",
    "    vinfo = pd.DataFrame(vinfo)\n",
    "    print(vinfo)\n",
    "#     vinfo = {\"Temp\": [-20.0, 50.0],\n",
    "#              \"RH\": [0.0, 100.0], \n",
    "#              \"Pres\": [600.0, 1100.0], \n",
    "#              \"Precp\": [0.0, 220.0]}\n",
    "    dataset = dg.hrfgenerator(tperiod, n_in=n_in, n_out=n_out, mode=mode, rescale=True, reformat=True, vstack=vstack, fnpy=fnpy, generator=False)\n",
    "    \n",
    "    datetimes = dataset[1]\n",
    "    nsize = len(datetimes)\n",
    "    print(dataset[0].shape)\n",
    "\n",
    "    \n",
    "    saved_model = \"../QC/model/lstm1_0055_0.008_0.011_202008111819_2.hdf5\"\n",
    "    model = NNBuilder.mloader(saved_model)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(vinfo.values)\n",
    "    \n",
    "    print(scaler.inverse_transform([[0.7, 0.6, 0.7, 0.2]]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    n = nsize\n",
    "    \n",
    "#     x = dataset[0][:, -4:]\n",
    "#     x = x[~np.isnan(x).any(axis=1)]\n",
    "#     idx = np.random.choice(np.arange(x.shape[0]), n, replace=False)\n",
    "#     x = x[idx, 0:]\n",
    "#     x = tf.convert_to_tensor(x)\n",
    "\n",
    "    scaled = dataset[0]\n",
    "    scaled = scaled[~np.isnan(scaled).any(axis=1)]\n",
    "    X_test = np.reshape(scaled[:, :-4], (-1, 6, 4))\n",
    "    y_true = scaled[:, -4:]\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "#     xynorm = tf.norm(tf.subtract(y_pred, y_true), axis=1) \n",
    "\n",
    "    y_true = tf.reshape(y_true[:, -1], [-1, 1])\n",
    "    y_pred = tf.reshape(y_pred[:, -1], [-1, 1])\n",
    "    xynorm = tf.subtract(y_pred, y_true)\n",
    "\n",
    "    print(y_true.shape, y_true)\n",
    "    print(y_pred.shape, y_pred)\n",
    "    print(xynorm.shape)\n",
    "    \n",
    "    sample_weight = 1\n",
    "#     sample_weight = tf.broadcast_to(sample_weight, y_pred.shape)\n",
    "    \n",
    "    loss = YZKError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"YZK\")\n",
    "    print('yzk-loss: ', loss)\n",
    "    \n",
    "    loss = tf.losses.LogCosh(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "\n",
    "    print(\"shape of loss = {}, xynorm = {}\".format(loss, xynorm.shape))\n",
    "    ax.scatter(xynorm, loss, label=\"LogCosh\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.MeanAbsoluteError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MAE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    loss = tf.losses.CosineSimilarity(reduction=tf.losses.Reduction.NONE)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Cos\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "\n",
    "#     loss = tf.keras.losses.KLDivergence(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "#     ax.scatter(xynorm, loss, label=\"KL\")\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    ax.scatter(xynorm, loss, label=\"MSE\")\n",
    "\n",
    "\n",
    "    loss = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.losses.Reduction.NONE)(y_true, y_pred)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"MSLE\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "    \n",
    "    loss = tf.keras.losses.Huber(reduction=tf.losses.Reduction.NONE, delta=0.25)(y_true, y_pred, sample_weight=sample_weight)\n",
    "#     loss = tf.sort(loss)\n",
    "    ax.scatter(xynorm, loss, label=\"Huber\")\n",
    "#     mposi1 = y_true[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     mposi2 = y_pred[tf.where(loss == tf.math.reduce_max(loss)).numpy()[0, 0], :]\n",
    "#     print(mposi1, mposi2)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-02-25T07:29:32.511697Z",
     "iopub.status.idle": "2021-02-25T07:29:32.511970Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample_weights(training_data, class_weight_dictionary): \n",
    "    sample_weights = [class_weight_dictionary[np.where(one_hot_row==1)[0][0]] for one_hot_row in training_data]\n",
    "    return np.asarray(sample_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
